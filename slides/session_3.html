<!DOCTYPE html>
<html lang="en"><head>
<script src="session_3_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_3_files/libs/quarto-html/tabby.min.js"></script>
<script src="session_3_files/libs/quarto-html/popper.min.js"></script>
<script src="session_3_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session_3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_3_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session_3_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session_3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session_3_files/libs/quarto-contrib/videojs/video.min.js"></script>
<link href="session_3_files/libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.538">

  <meta name="author" content="Baltais">
  <title>Reading</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session_3_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session_3_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="session_3_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session_3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session_3_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session_3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session_3_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .center-xy {
    margin: 0;
    position: absolute;
    top: 40%;
    left: 40%;
    -ms-transform: translateY(-50%), translateX(-50%);
    transform: translateY(-50%), translateX(-50%);
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Reading</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Baltais 
</div>
</div>
</div>

</section>
<section>
<section id="reading" class="title-slide slide level1 center">
<h1>Reading</h1>

</section>
<section id="reading-research" class="slide level2">
<h2>Reading research</h2>
<p>We have a wealth of paradigms to explore how individuals comprehend a piece of written text (from words to several lines, either created stimuli or real texts), considering both low-level processes for word identification (e.g., lexical access) and higher-level processes for sentence and discourse comprehension (e.g., inferences). !!!</p>
</section>
<section id="whats-so-special-about-reading" class="slide level2">
<h2>What’s so special about reading?</h2>
<ul>
<li>Reading is relatively new.
<ul>
<li>Homo Sapiens <span class="math inline">\(\pm\)</span> 300.000 BC</li>
<li>Sumerian pictographic writing <span class="math inline">\(\pm\)</span> 3.300 BC</li>
<li>Phoenician alphabetic language <span class="math inline">\(\pm\)</span> 2.000 BC</li>
</ul></li>
<li>Reading is a skill (exposure, formal instruction, practice)
<ul>
<li>We are good at it (250 WPM; Brysbaert, 2019)</li>
<li>Linked to academic achievement, late L2 acquisition.</li>
</ul></li>
</ul>
</section>
<section id="history" class="slide level2">
<h2>History</h2>
<ul>
<li><p>Javal (1879)</p>
<ul>
<li>Reading is not smooth.</li>
</ul></li>
<li><p>Mid-70s: Technological advances</p></li>
<li><p>Rayner’s work.</p>
<ul>
<li>Clifton et al., 2016: “Eye movements in reading and information processing: Keith Rayner’s 40 year legacy”.</li>
</ul></li>
</ul>
</section>
<section id="history-1" class="slide level2">
<h2>History</h2>
<p>Main findings:</p>
<ul>
<li>Individuals are good at reading, but there are some individual characteristics (e.g., proficiency, L1 v L2).</li>
<li>There seems to be some immediacy<sup>1</sup> when comprehending a text, but we also get ahead of ourselves.</li>
<li>In contrast to spoken language, individuals can go back when they encounter a difficulty, and indeed they do.</li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Serial versus paralllel debate; Just and Carpenter’s eye-mind hypothesis</p></li></ol></aside></section></section>
<section>
<section id="how-to-research-reading-abilities" class="title-slide slide level1 center">
<h1>How to research reading abilities</h1>

</section>
<section id="reading-research-1" class="slide level2">
<h2>Reading research</h2>
<p>We need to take a little detour:</p>
<ul>
<li>Eye movements
<ul>
<li>Visual human system</li>
<li>Foveal and parafoveal processing</li>
</ul></li>
</ul>
</section>
<section id="eye-movements" class="slide level2">
<h2>Eye movements</h2>
<p>Visual Human System</p>
<ul>
<li>Fovea: Highest visual acuity</li>
<li>Parafovea: Still partial recognition of objects.</li>
<li>Periphery: Blurred image.</li>
</ul>
<p>Fixations versus saccades - Movements (saccades) to place objects on the fovea for processing (fixations)</p>
</section>
<section id="eye-movements-in-reading" class="slide level2">
<h2>Eye movements in reading</h2>
<p>What does this mean in the case of reading?</p>
<ul>
<li>A priori: We move our eyes to place objects (words?) on the fovea so we can process them (e.g., information extraction).</li>
</ul>
<p>But this is not true in reality.</p>
</section>
<section id="eye-movements-in-reading-1" class="slide level2">
<h2>Eye movements in reading</h2>

<img data-src="images/session_4/perceptual_span_reading.JPG" class="quarto-figure quarto-figure-center r-stretch" width="590"><p class="caption">From Conklin et al., 2018</p></section>
<section id="eye-movements-in-reading-2" class="slide level2">
<h2>Eye movements in reading</h2>
<ul>
<li>Fovea does not equate the perceptual span (= effective visual field).</li>
<li>Reading is asymmetric = asymmetric extraction of information.
<ul>
<li>Bias towards the direction of reading in the language.</li>
<li>In English: L: 3/4 ls, R: 14-15 letter space (3-4 ls/ 1 visual angle).</li>
</ul></li>
</ul>
<aside class="notes">
<p>Fovea ~ 1 visual angle It would appear that we have little to none cognitive resources to process N+1 when we are dealing with N (fixating on N entails using visual, orthographics, phonological… to accessing a lexeme, its morphological information, semantics…)</p>
<p>So why do we need this somewhat technical discussion about the distinction between words in the fovea and parafovea? The next few paragraphs will look at why these distinctions are important in reading, as well as their implications for designing research studies.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-in-reading-3" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Fovea = N; N - 1, 2, … towards the left; N + 1, 2, … towards the right.</p>
<ul>
<li>What kinds of information can be extracted from either.
<ul>
<li>See table 4.1. Conklin et al.&nbsp;(2018)</li>
</ul></li>
<li>cf.&nbsp;Confounds and design
<ul>
<li>Parafoveal-on-foveal effects</li>
</ul></li>
</ul>
</section>
<section id="eye-movements-in-reading-4" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Fixations</p>
<ul>
<li>Mean duration (silent reading in English): 250 - 250 ms.</li>
<li>Lexical access initiated at about 100 ms. (Sereno &amp; Rayner, 2003).</li>
<li>Optimal Viewing Position</li>
<li>Affected by word length, frequency, part of speech, etc.</li>
</ul>

<img data-src="images/session_4/mariia_fix.JPG" class="quarto-figure quarto-figure-center r-stretch" width="500"><p class="caption">Data from Mariia Baltais</p></section>
<section id="eye-movements-in-reading-5" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Saccades</p>
<ul>
<li>Saccadic supression</li>
<li>Mean size (silent reading in English): 7 to 9 letter spaces (<span class="math inline">\(\pm\)</span> 2 words before or after the fixated word)
<ul>
<li>Size: distance travelled (amplitude).</li>
</ul></li>
</ul>

<img data-src="images/session_4/mariia_sac.JPG" class="quarto-figure quarto-figure-center r-stretch" width="500"><p class="caption">Data from Mariia Baltais</p></section>
<section id="eye-movements-in-reading-6" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Regressions</p>
<ul>
<li>10-15% of the saccades are regressions to preceding areas.
<ul>
<li>Correct ‘overshooting’ (short regressions within a word)</li>
<li>Processing difficulty</li>
</ul></li>
</ul>

<img data-src="images/session_4/mariia_sac.JPG" class="quarto-figure quarto-figure-center r-stretch" width="500"><p class="caption">Data from Mariia Baltais</p></section>
<section id="eye-movements-in-reading-7" class="slide level2">
<h2>Eye movements in reading</h2>
<ul>
<li>Skipping: When a reader does not fixate on a region when reading.</li>
</ul>

<img data-src="images/session_4/skipping.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="eye-movements-in-reading-8" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Fovea and parafovea.</p>
<p>Most words are fixated, many are skipped.</p>
<ul>
<li>Content (85%) versus function (35%).</li>
<li>Predictor: length, predictability, frequency.
<ul>
<li>Parafoveal processing.</li>
</ul></li>
<li>Overshooting versus processing ease.</li>
</ul>
</section>
<section id="eye-movements-in-reading-9" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Blinking</p>
<ul>
<li>Noise
<ul>
<li>Data pre-processing</li>
</ul></li>
</ul>

<img data-src="images/session_4/mariia_blink.JPG" class="quarto-figure quarto-figure-center r-stretch" width="500"><p class="caption">Data from Mariia Baltais</p></section>
<section id="eye-movements-in-reading-10" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Fixations, saccades, skipping -&gt; wide range of operationalised measures ~ process of interest.</p>
<p>Reading (during a fixation) entails the use of visual, orthographic, phonological and morphological information to:</p>
<ul>
<li>Identify a word.</li>
<li>Activate its lexical representation.</li>
<li>Integrate with the preceding context.</li>
</ul>
</section>
<section id="linking-hypothesis" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Recognition (lexical access) versus integration (syntactic processing).</p>
<p>Just and Carpenter’s eye-mind hypothesis (1980).</p>
<ul>
<li>Where we look at = what we are processing.</li>
<li>For how long we look at it = how difficult it is to process it.</li>
</ul>
<aside class="notes">
<p>The Just and Carpenter reading model proposes that the gaze duration is related to the time to execute comprehension processes.</p>
<p>Longer fixations indicate longer processing caused by the word’s infrequency and its thematic importance. The link between eye-fixation data and the Just and Carpenter theory is established based on two crucial assumptions:</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-1" class="slide level2">
<h2>Linking hypothesis</h2>
<p>The more difficult the text…</p>
<ul>
<li>The ________________ the fixation durations</li>
<li>The ________________ the saccade sizes</li>
<li>The ________________ the regressions</li>
<li>The ________________ the skipping of words</li>
</ul>
</section>
<section id="linking-hypothesis-2" class="slide level2">
<h2>Linking hypothesis</h2>
<p>The more difficult the text…</p>
<ul>
<li>The longer the fixation durations</li>
<li>The smaller the saccade sizes</li>
<li>The more frequent the regressions</li>
<li>The less frequent the skipping of words</li>
</ul>
</section>
<section id="linking-hypothesis-3" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Assumptions of the mind-eye hypothesis:</p>
<ul>
<li>Immediacy: One word at a time, as it is encountered.
<ul>
<li>Serial versus parallel processing.</li>
</ul></li>
<li>Strong eye-mind hypothesis: Eye movements only when processing is done.</li>
</ul>
<p>Too strong of an assumption? (e.g., Mitchell et al., 2008; Boland, 2004; ).</p>
<ul>
<li><p>Series of underlying assumptions (e.g., serial processing)</p></li>
<li><p>Pickering et al., 2004: Immediacy assumption.</p></li>
</ul>
<aside class="notes">
<ol type="1">
<li>Immediacy assumption: A reader attempts to interpret each content word as it is seen, even if he / she must make guesses that may turn out to be incorrect later. 2. The strong eye-mind hypothesis: As stated by Just and Carpenter, the strong eye-mind hypothesis is as follows: The eye remains fixated on a word until its processing is done.</li>
</ol>
<p>Boland, 2004: Eye remains on a word until its structural integration is completed.</p>
<p>First-pass reading shows syntactic constrains.</p>
<p>Other measures (e.g., regression-path duration) reflect higher cognitive processes such as semantic integration or discourse processing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="measures" class="title-slide slide level1 center">
<h1>Measures</h1>

</section>
<section id="measures-1" class="slide level2">
<h2>Measures</h2>
<p>Fixations, saccades, and skipping are groupped into different measures that arguably tap onto different cognitive processes.</p>
<ul>
<li>Global versus local measures.
<ul>
<li>Overall reading behaviour versus smaller units of text.</li>
</ul></li>
<li>Local measures: Early versus intermediate versus late (see also Clifton et al., 2007).</li>
</ul>
</section>
<section id="measures-2" class="slide level2">
<h2>Measures</h2>
<p>Reflect different stages of processing.</p>
<ul>
<li>Early v Intermediary v Late measures
<ul>
<li>Early: Highly automatic word recognition and lexical access processes.</li>
<li>Late: More conscious, controlled, strategic processes.</li>
</ul></li>
<li>Mapped onto first pass and total measures.</li>
</ul>
<p>Again: No transparent link.</p>
<ul>
<li>Different types of linguistic manipulations (e.g.&nbsp;a lexical manipulation compared with a syntactic manipulation) can produce effects in the same eye movement measure (e.g.&nbsp;first fixation duration, or gaze duration).</li>
</ul>
</section>
<section id="early-measures" class="slide level2">
<h2>Early measures</h2>
<p>First pass measures.</p>
<ul>
<li>1st fixation duration (or single fixation duration, related to gaze duration, 1st pass time reading): Time on an AI.</li>
<li>Skipping rate: % words that receive no fixation during 1st pass.
<ul>
<li>Visual and linguistic factors e.g., parafoveal processing.</li>
</ul></li>
<li>Factors known to affect: Frequency, familiarity, ambiguity, predictability, semantic association.</li>
</ul>
<aside class="notes">
<p>1st fix: duration of the 1st fix on a word provided it isn’t skipped. single fix dur: when there is only one fix gaze duration: sum of all fixations on a word prior to moving to another one (forward or backwards) skipping more likely due to linguistic factors but also processing it in the parafovea</p>
<p>earliest point for us to see an effect of a variable being manipulated</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="early-measures-1" class="slide level2">
<h2>Early measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_4/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration: 3</li>
<li>Single fixation duration: NA (there are two fixations in the first pass)</li>
<li>Gaze duration: 3 + 4</li>
<li>Skipping rate: % trials where AI is skipped in first pass.</li>
</ul>
</section>
<section id="intermediate-measures" class="slide level2">
<h2>Intermediate measures</h2>
<ul>
<li>Regression-path duration (or go-past time): time on N, N -1, etc. before N +1,
<ul>
<li>Includes regressions out (1st pass regressions) and regressions in.</li>
</ul></li>
</ul>
<aside class="notes">
<p>hard to tell whether they show difficulty of encountering an item and the subsequent time taken to overcome the difficulty</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="intermediate-measures-1" class="slide level2">
<h2>Intermediate measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_4/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>Regression-path duration/go-past time: 3+4+5+6</li>
<li>Regressions-out (on first pass): %</li>
<li>Regressions-in: %</li>
</ul>
</section>
<section id="late-measures" class="slide level2">
<h2>Late measures</h2>
<ul>
<li>Total reading time: <span class="math inline">\(\Sigma\)</span> fix on N.
<ul>
<li>Contextual and discourse-level factors + lexical (i.e., retrieval and integration).</li>
</ul></li>
<li>Re-reading time (2nd pass reading time): Different definitions across the literature.
<ul>
<li>General comprehension difficulties</li>
</ul></li>
</ul>
</section>
<section id="late-measures-1" class="slide level2">
<h2>Late measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_4/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>Total reading time: 3+4+6+9</li>
<li>Re-reading time: 6+9 (total reading time - gaze duration)</li>
<li>Fixation count: 4</li>
</ul>
</section>
<section id="measures-3" class="slide level2">
<h2>Measures</h2>
<p>Measures are dependent:</p>
<ul>
<li>1st fixation duration - Gaze duration - Total reading time.</li>
<li>Late measures are cumulative.</li>
<li>Not total transparent mapping!</li>
</ul>
<p>To reiterate…</p>

<img data-src="images/session_4/reiterate.JPG" class="r-stretch quarto-figure-center"><p class="caption">Carroll, 2017</p></section>
<section id="measures-exercise" class="slide level2">
<h2>Measures: Exercise</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_4/exercise.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration:</li>
<li>Gaze duration:</li>
<li>Regressions-out:</li>
<li>Regression-path duration:</li>
<li>Re-reading time:</li>
<li>Total reading time:</li>
</ul>
<aside class="notes">
<p>3 3 2 3+4+5+6+7 5+7 3+5+7</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="paradigms-in-reading-research" class="title-slide slide level1 center">
<h1>Paradigms in reading research</h1>

</section>
<section id="paradigms" class="slide level2">
<h2>Paradigms</h2>
<p>Ecological validity.</p>
<ul>
<li>Gaze-contigent paradigms: Eye gaze determines changes to the text display.</li>
<li>Reading of experimental stimuli.</li>
<li>Reading of natural text.</li>
</ul>
</section>
<section id="disappearing-text-paradigm" class="slide level2">
<h2>Disappearing text paradigm</h2>
<p>Liversedge et al., 2004</p>
<ul>
<li>ROI: Disappears after an amount of time of fixation.</li>
<li>Shows: Fovea processing.</li>
<li>IV: Time to disappear ~ time for normal reading.
<ul>
<li>Visual exposure for word recognition.</li>
</ul></li>
</ul>
</section>
<section id="disappearing-text-paradigm-1" class="slide level2">
<h2>Disappearing text paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_4/disappearing_text_animation.mp4"></video>
</section>
<section id="fast-priming-paradigm" class="slide level2">
<h2>Fast priming paradigm</h2>
<p>Sereno &amp; Rayder, 1992</p>
<ul>
<li>ROI: Target word is first a prime, then target word is presented after a fixation on it has occured for some time.</li>
<li>Shows: Prime facilitation</li>
<li>IV: Relationship prime and target.</li>
</ul>
</section>
<section id="fast-priming-paradigm-1" class="slide level2">
<h2>Fast priming paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_4/fast_priming_paradigm.mp4"></video>
</section>
<section id="moving-window-paradigm" class="slide level2">
<h2>Moving window paradigm</h2>
<p>McConkie &amp; Rayder, 1975</p>
<ul>
<li>ROI: Mask around a fixation.</li>
<li>Shows: Parafoveal processing and perceptual span (extraction and use of information).</li>
<li>IV: Size of the window.</li>
</ul>
<aside class="notes">
<p>By varying the size of the window and the type of mask (e.g., X’s, visually similar or dissimilar characters) and comparing the reading times in the window and normal reading conditions, it is possible to define the size of the area from which a reader can efficiently extract and utilize information.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="moving-window-paradigm-1" class="slide level2">
<h2>Moving window paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_4/moving_window.mp4"></video>
</section>
<section id="foveal-mask" class="slide level2">
<h2>Foveal mask</h2>
<p>Rayner &amp; Bertera, 1979</p>
<ul>
<li>ROI: Mask within the fixation</li>
<li>Shows: Parafoveal processing and perceptual span.</li>
<li>IV: Size of the window.</li>
</ul>
</section>
<section id="foveal-mask-1" class="slide level2">
<h2>Foveal mask</h2>
<p><br></p>
<video id="video_shortcode_videojs_video4" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_4/foveal_mask.mp4"></video>
</section>
<section id="boundary-paradigm" class="slide level2">
<h2>Boundary paradigm</h2>
<p>Rayder, 1975</p>
<p>Saccadic suppression.</p>
<ul>
<li>ROI: Masked and then changed after fixations cross an invisible boundary.</li>
<li>Shows: Parafoveal processing and perceptual span (information extracted).
<ul>
<li>Preview window effect.</li>
<li>Parafoveal-on-foveal effect (e.g., Drieghe, 2011).</li>
</ul></li>
<li>IV: What information is extracted e.g., visual similarity.</li>
</ul>
<aside class="notes">
<p>The boundary paradigm (Rayner, 1975) makes use of the saccadic suppression. Saccadic suppression means that during a saccade the intake of visual information is suspended and the reader is practically blind. If a change in the visual environment is made during a saccade or very soon after the eyes have landed (&lt; 6 ms after the end of a saccade, McConkie &amp; Loschky, 2002), the reader does not become consciously aware of it. The target word (“sentence” in the example of Table 7.1) is initially masked with a character string (“somkasoc”), and when the reader’s eyes cross an invisible boundary in the text, the mask is replaced with the actual target word.</p>
<p>If the reader has extracted information from the target word preview prior to its change to the correct form, one should observe increased fixation time on the target word, even though the reader is not consciously aware of this. The size of the slowdown in eye fixation time, i.e.&nbsp;the difference between normal condition inwhich no change was made and a change condition is called the preview effect.</p>
<p>The preview benefit is simply computed as the difference in fixation time between a full preview condition, in which the target word was presented normally, and the preview condition.</p>
<p>Another measure to assess parafoveal processing is the so-called parafoveal-on-foveal effect (Kennedy, 2000). It measures the extent to which parafoveally available information affects fixation time on the previous word (Drieghe, 2011).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="boundary-paradigm-1" class="slide level2">
<h2>Boundary paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video5" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_4/boundary.mp4"></video>
</section>
<section id="natural-sentence-reading-experimental-stimuli" class="slide level2">
<h2>Natural sentence reading (experimental stimuli)</h2>
<p>Linguistic and non-linguistic factors affecting eye patterns in reading.</p>
<p>Usually, exploration of how movement patterns over a IA differ as a function of the properties of it.</p>
<p>Frazier &amp; Rayner (1982)</p>

<img data-src="images/session_4/fraz.JPG" class="r-stretch"></section>
<section id="natural-text-reading-constructed-texts" class="slide level2">
<h2>Natural text reading (constructed texts)</h2>
<p>Pellicer-Sánchez (2016)</p>

<img data-src="images/session_4/texts_example.JPG" class="quarto-figure quarto-figure-center r-stretch"><p class="caption">From Conklin et al., 2016</p></section>
<section id="natural-text-reading" class="slide level2">
<h2>Natural text reading</h2>
<p>GECO corpus (Cop et al., 2017)</p>
<ul>
<li>The Mysterious Affair At Styles by Agatha Christie (1920)</li>
</ul>

<img data-src="images/session_4/geco.JPG" class="r-stretch"><p>Lack of control over text versus ecological validity.</p>
<ul>
<li>Participants’ characteristics + matching samples
<ul>
<li>Additional questionnaires</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="single-sentence-reading-experiment" class="title-slide slide level1 center">
<h1>Single sentence reading experiment</h1>

</section>
<section id="single-sentence-reading-experiment-1" class="slide level2">
<h2>Single sentence reading experiment</h2>
<p>Elements:</p>
<ul>
<li>A sentence.</li>
</ul>
<p>Trial sequence: Drift &gt; Visual Presentation</p>
</section>
<section id="drift-correction" class="slide level2">
<h2>Drift correction</h2>
<p>Where the first word appears</p>

<img data-src="images/lab_setup/drift_reading.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="visual-presentation" class="slide level2">
<h2>Visual Presentation</h2>
<p>Visual presentation.</p>
<ul>
<li>Time window of interest is from start to end (so no triggers in between).
<ul>
<li>cf.&nbsp;gaze-contingent paradigms.</li>
</ul></li>
</ul>
</section>
<section id="visual-presentation-1" class="slide level2">
<h2>Visual Presentation</h2>
<p>Visual presentation: Areas of Interest</p>

<img data-src="images/session_2/ia_reading.png" class="r-stretch"></section>
<section id="areas-of-interest" class="slide level2">
<h2>Areas of Interest</h2>
<ul>
<li>Target region but also
<ul>
<li>Pre-critical region</li>
<li>Spillover region</li>
</ul></li>
</ul>
<p>Research question.</p>
<ul>
<li>Single words v phrases v clauses …</li>
</ul>
</section>
<section id="areas-of-interest-1" class="slide level2">
<h2>Areas of Interest</h2>
<ul>
<li>Never beginning/end of the line.
<ul>
<li>1st and last fixations on a line are 5-7 ls from the edges.</li>
<li>1st word likely to be skipped.</li>
<li>1st fix longer, last fix shorter.</li>
</ul></li>
<li>AI never clause-final.
<ul>
<li>Wrap-up effect.</li>
</ul></li>
</ul>
</section>
<section id="task" class="slide level2">
<h2>Task</h2>
<ul>
<li>Move to the next trial</li>
<li>Comprehension (every now and then/every trial)</li>
</ul>
</section>
<section id="confounds" class="slide level2">
<h2>Confounds</h2>
<ul>
<li>Characteristics of the visual presentation (non-linguistic).</li>
<li>Characteristics of the materials (linguistic)</li>
<li>Characteristics of the human visual system</li>
</ul>
</section>
<section id="non-linguistic-confounds" class="slide level2">
<h2>Non-linguistic confounds</h2>
<p>Text layout:</p>
<ul>
<li>Font style and size
<ul>
<li>B&amp;W for dyslexic participants is not good.</li>
<li>Horizontal space.</li>
<li>14-18 pt</li>
<li>Monospace font (e.g., Courier)</li>
</ul></li>
</ul>
</section>
<section id="non-linguistic-confounds-1" class="slide level2">
<h2>Non-linguistic confounds</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Line spacing
<ul>
<li>Less accuracy for vertical movements</li>
<li>Double/triple spacing.</li>
</ul></li>
<li>Line breaks</li>
<li>Margins
<ul>
<li>Accuracy is low at screen borders.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br></p>
<p><img data-src="images/session_4/example_spacing.JPG"></p>
</div>
</div>
</section>
<section id="linguistic-confounds" class="slide level2">
<h2>Linguistic confounds</h2>
<p>Same we have discussed in previous weeks (see Clifton et al., 2016):</p>
<ul>
<li>The Big Three (Frequency, Length, Predictability)</li>
<li>Lexical ambiguity</li>
<li>Orthographic neighbourhood size (differ by 1 letter)</li>
<li>Prevalence</li>
<li>Inter alia</li>
</ul>
</section>
<section id="hvs-confounds" class="slide level2">
<h2>HVS confounds</h2>
<p>Foveal versus parafoveal processing.</p>
<ul>
<li><p>Sequential nature of reading.</p></li>
<li><p>How information from one area can influence eye movements on the other.</p>
<ul>
<li>Predictability, length, etc.</li>
<li>Parafoveal-on-foveal effects (see Hyönä, 2011).</li>
<li>Serial versus parallel processing.
<ul>
<li>Preview benefit, spillover effects.</li>
</ul></li>
</ul></li>
<li><p>Spillover: Effects of processing N are seen on N + 1, + 2.</p></li>
<li><p>Preview: Effects of perceiving N on N - 1.</p>
<ul>
<li>Match contexts!</li>
</ul></li>
</ul>
</section>
<section id="exercise" class="slide level2">
<h2>Exercise</h2>
<p>Howard et al.&nbsp;(2017)</p>
<ol type="1">
<li>How did they define the Areas of Interest?</li>
<li>What factors should be considered for the antecedent? (i.e., hammer/plunger in the example)</li>
<li>What information do you need from procedure and apparatus to replicate this study?</li>
</ol>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise</h2>
<ol type="1">
<li>How did they define the Areas of Interest?</li>
</ol>
<p>7 AIs, two regions of interest.</p>

<img data-src="images/session_4/ias_paper.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="exercise-2" class="slide level2">
<h2>Exercise</h2>
<ol start="2" type="1">
<li>What factors should be considered for the antecedent?</li>
</ol>
<ul>
<li>Typicality (IV)</li>
<li>No matching!</li>
</ul>

<img data-src="images/session_4/matching_exercise.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="exercise-3" class="slide level2">
<h2>Exercise</h2>
<ol start="3" type="1">
<li>What information do you need from procedure and apparatus to replicate this study?</li>
</ol>
<p><img data-src="images/session_4/3a.JPG" class="quarto-figure quarto-figure-left" width="500"> <img data-src="images/session_4/3b.JPG" class="quarto-figure quarto-figure-right" width="500"></p>
</section>
<section id="analysis" class="slide level2">
<h2>Analysis</h2>
<p>More on W9 &amp; W11 but:</p>
<ul>
<li>Multiple dependent variables (stages of processing).</li>
<li>Type I error</li>
<li>Tables!</li>
</ul>
</section>
<section id="uses" class="slide level2">
<h2>Uses</h2>
<ul>
<li>Word recognition
<ul>
<li>Frequency effect, Rayner &amp; Duffy (1986)</li>
</ul></li>
<li>Syntactic processing
<ul>
<li>Structural ambiguities, Frazier &amp; Rayner (1982)</li>
</ul></li>
<li>Semantic integration
<ul>
<li>Plausibility effect, Staub et al.&nbsp;(2007)</li>
</ul></li>
<li>Predictive processing (Frisson et al., 2017)</li>
<li>Parafoveal processing (Juhasz et al., 2009)</li>
<li>Text comprehension (Dirix et al., 2019)</li>
<li>Multiword units (Carrol et al., 2016)</li>
</ul>
</section>
<section id="uses-1" class="slide level2">
<h2>Uses</h2>
<ul>
<li>L2 speakers
<ul>
<li>Gender agreement, Keating (2009)</li>
</ul></li>
<li>Children (Blythe et al., 2011)</li>
<li>Older adults (Solan et al., 1995)</li>
<li>Clinical populations
<ul>
<li>Children with dyslexia, Hyönä et al.&nbsp;(1995)</li>
<li>Adults with ASD, Howard et al.&nbsp;(2017)</li>
</ul></li>
</ul>
</section>
<section id="pros-cons" class="slide level2">
<h2>Pros &amp; cons</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Pros</strong></p>
<ul>
<li>Naturalistic reading.</li>
<li>Non-invasive method.</li>
<li>Different stages of processing.</li>
<li>Combination with offline measures.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Cons</strong></p>
<ul>
<li>Literacy required.</li>
<li>Dependent variables.</li>
<li>Resources.</li>
</ul>
</div>
</div>
</section>
<section class="slide level2">

<p>Let’s take a break!</p>
</section>
<section id="building-an-eye-tracking-experiment" class="slide level2">
<h2>Building an eye-tracking experiment</h2>
<p>We are going to code a reading eye-tracking experiment on OS.</p>
<ul>
<li>Many of the things we learnt yesterday can be used today!
<ul>
<li>Especially, we will use the same eye-tracking plugins</li>
</ul></li>
</ul>
</section>
<section id="building-a-reading-experiment" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Example: high/low frequency words (Rayner &amp; Duffy,1986))</p>
<ul>
<li>DV: Fixations on an area of interest</li>
<li>IV: Word frequency</li>
</ul>
<p>5 areas of interest</p>
</section>
<section id="building-a-reading-experiment-1" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The constraints of design:</p>
<ul>
<li>We want to counterbalance particants across lists</li>
<li>We want participants to progress in the experiment by pressing the spacebar</li>
<li>We want to show participants the instructions at the beginning of the experiment</li>
</ul>
</section>
<section id="building-a-reading-experiment-2" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The constraints of the eye tracker:</p>
<ul>
<li>We want five areas of interest</li>
<li>We want a drift correction at the beginning of every trial
<ul>
<li>Drift should appear where the sentence starts</li>
</ul></li>
</ul>
</section>
<section id="building-a-reading-experiment-3" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The loop</p>
<ul>
<li>Each row: a trial</li>
<li>What does the first row represent?</li>
</ul>

<img data-src="images/session_3/OS/datasource.jpg" class="r-stretch"></section>
<section id="building-a-reading-experiment-4" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The trial sequence</p>

<img data-src="images/session_3/OS/trialsequence.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-5" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Send backdrop to .edf (before trial sequence)</p>

<img data-src="images/session_3/OS/examplebackdrop.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-6" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Send backdrop to .edf (before trial sequence)</p>

<img data-src="images/session_3/OS/psychoend.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-7" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Send backdrop to .edf (before trial sequence)</p>

<img data-src="images/session_3/OS/importmodules.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-8" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Drift correction</p>

<img data-src="images/session_3/OS/drift.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-9" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Present stimuli and sending areas of interest done via inline python script</p>

<img data-src="images/session_3/OS/prepare.JPG" class="r-stretch quarto-figure-center"><p class="caption">The prepare tab</p></section>
<section id="building-a-reading-experiment-10" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Present stimuli: text drawn on an area of interest basis</p>
<ul>
<li>Get where one area ends so the next one starts there
<ul>
<li>“Draw” rectangles</li>
</ul></li>
</ul>

<img data-src="images/session_3/OS/presentstimuli1.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-11" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Calculate the edges of the areas of interest</p>

<img data-src="images/session_3/OS/sendIAs.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-12" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Present the text</p>

<img data-src="images/session_3/OS/showtext.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-13" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Record the areas of interest</p>

<img data-src="images/session_3/OS/recordIAs.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-14" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Take a screenshot of the screen + send it to the tracker</p>

<img data-src="images/session_3/OS/sendIAs.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-15" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Your turn now!</p>
<ul>
<li>Try:
<ul>
<li>This example.</li>
<li>A mock experiment</li>
<li>Your own research question</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<p>Blythe, H. I., Häikiö, T., Bertam, R., Liversedge, S. P., &amp; Hyönä, J. (2011). Reading disappearing text: Why do children refixate words? <em>Vision Research, 51</em>(1), 84–92. https://doi.org/10.1016/j.visres.2010.10.003</p>
<p>Boland, J. E. (2004). Linking eye movements to sentence comprehension in reading and listening. <em>The on-line study of sentence comprehension: Eyetracking, ERP, and beyond</em>, 51-76.</p>
<p>Brysbaert, M. (2019). How many words do we read per minute? A review and meta-analysis of reading rate. <em>Journal of Memory and Language, 109</em>, 104047. https://doi.org/10.1016/j.jml.2019.104047</p>
<p>Carrol, G., Conklin, K., &amp; Gyllstad, H. (2016). FOUND IN TRANSLATION: The Influence of the L1 on the Reading of Idioms in a L2. <em>Studies in Second Language Acquisition, 38</em>(3), 403–443. https://doi.org/10.1017/S0272263115000492</p>
<p>Carroll, T. (2017). Eye Behavior While Reading Words of Sanskrit and Urdu Origin in Hindi. Brigham Young University.</p>
<p>Clifton, C., Staub, A., &amp; Rayner, K. (2007). Eye movements in reading words and sentences. <em>Eye movements</em>, 341-371.</p>
</section>
<section id="references-1" class="slide level2 smaller">
<h2>References</h2>
<p>Clifton, C., Ferreira, F., Henderson, J. M., Inhoff, A. W., Liversedge, S. P., Reichle, E. D., &amp; Schotter, E. R. (2016). Eye movements in reading and information processing: Keith Rayner’s 40year legacy. <em>Journal of Memory and Language, 86</em>, 1–19. https://doi.org/10.1016/j.jml.2015.07.004</p>
<p>Conklin, K., Pellicer-Sánchez, A., &amp; Carrol, G. (2018). Eye-Tracking: A Guide for Applied Linguistics Research. <em>Cambridge University Press</em>. https://doi.org/10.1017/9781108233279</p>
<p>Cop, U., Dirix, N., Drieghe, D., &amp; Duyck, W. (2017). Presenting GECO: An eyetracking corpus of monolingual and bilingual sentence reading. <em>Behavior Research Methods, 49</em>(2), 602–615. https://doi.org/10.3758/s13428-016-0734-0</p>
<p>Dirix, N., Vander Beken, H., De Bruyne, E., Brysbaert, M., &amp; Duyck, W. (2019). Reading Text When Studying in a Second Language: An Eye-Tracking Study. <em>Reading Research Quarterly, 55</em>(3), 371–397. https://doi.org/10.1002/rrq.277</p>
<p>Drieghe, D. (2011). Parafoveal-on-foveal effects on eye movements during reading. In S. P. Liversedge, I. Gilchrist, &amp; S. Everling (Eds.), <em>The Oxford handbook of eye movements</em> (pp.&nbsp;840–855). Oxford: Oxford University Press.</p>
</section>
<section id="references-2" class="slide level2 smaller">
<h2>References</h2>
<p>Frazier, L., &amp; Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. <em>Cognitive Psychology, 14</em>(2), 178–210. https://doi.org/10.1016/0010-0285(82)90008-1</p>
<p>Frisson, S., Harvey, D. R., &amp; Staub, A. (2017). No prediction error cost in reading: Evidence from eye movements. <em>Journal of Memory and Language, 95</em>, 200–214. https://doi.org/10.1016/j.jml.2017.04.007</p>
<p>Hyönä, J. (2011). Foveal and parafoveal processing during reading. In S. P. Liversedge, I. Gilchrist, &amp; S. Everling (Eds.), <em>The Oxford handbook of eye movements</em> (pp.&nbsp;820–838). Oxford: Oxford University Press.</p>
<p>Hyönä, J., Olson, R., Defries, J., Fulker, D., Pennington, B., &amp; Smith, S. (1995). Eye Fixation Patterns Among Dyslexic and Normal Readers: Effects of Word Length and Word Frequency. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 21</em>, 1430–1440. https://doi.org/10.1037/0278-7393.21.6.1430</p>
<p>Howard, P. L., Liversedge, S. P., &amp; Benson, V. (2017). Processing of co-reference in autism spectrum disorder. <em>Autism Research, 10</em>(12), 1968–1980. https://doi.org/10.1002/aur.1845</p>
<p>Juhasz, B. J., Pollatsek, A., Hyönä, J., Drieghe, D., &amp; Rayner, K. (2009). Parafoveal processing within and between words. <em>Quarterly Journal of Experimental Psychology, 62</em>(7), 1356–1376. https://doi.org/10.1080/17470210802400010</p>
</section>
<section id="references-3" class="slide level2 smaller">
<h2>References</h2>
<p>Just, M. A., &amp; Carpenter, P. A. (1980). A theory of reading: from eye fixations to comprehension. <em>Psychological review, 87</em>(4), 329.</p>
<p>Keating, G. D. (2009). Sensitivity to Violations of Gender Agreement in Native and Nonnative Spanish: An Eye-Movement Investigation. <em>Language Learning, 59</em>(3), 503–535. https://doi.org/10.1111/j.1467-9922.2009.00516.x</p>
<p>Liversedge, S. P., Rayner, K., White, S. J., Vergilino-Perez, D., Findlay, J. M., &amp; Kentridge, R. W. (2004). Eye movements when reading disappearing text: is there a gap effect in reading?. <em>Vision research, 44</em>(10), 1013-1024.</p>
<p>McConkie, G. W., &amp; Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. <em>Perception &amp; Psychophysics, 17</em>, 578–586.</p>
<p>Mitchell, D. C., Shen, X., Green, M. J., &amp; Hodgson, T. L. (2008). Accounting for regressive eye-movements in models of sentence processing: A reappraisal of the Selective Reanalysis hypothesis. <em>Journal of Memory and Language, 59</em>(3), 266-293.</p>
<p>Pellicer-Sánchez, A. (2016). INCIDENTAL L2 VOCABULARY ACQUISITION FROM AND WHILE READING: An Eye-Tracking Study. <em>Studies in Second Language Acquisition, 38</em>(1), 97–130. https://doi.org/10.1017/S0272263115000224</p>
</section>
<section id="references-4" class="slide level2 smaller">
<h2>References</h2>
<p>Pickering, M. J., Frisson, S., McElree, B., &amp; Traxler, M. J. (2004). Eye Movements and Semantic Composition. In M. Carreiras &amp; C. Clifton Jr.&nbsp;(Eds.), <em>The On-line Study of Sentence Comprehension</em>. Psychology Press.</p>
<p>Rayner, K. (1975). The perceptual span and peripheral cues in reading. <em>Cognitive Psychology, 7</em>, 65–81.</p>
<p>Rayner, K., &amp; Bertera, J. H. (1979). Reading without a fovea. <em>Science, 206</em>, 468–469.</p>
<p>Rayner, K., &amp; Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. <em>Memory &amp; Cognition, 14</em>(3), 191–201. https://doi.org/10.3758/BF03197692</p>
<p>Sereno, S. C., &amp; Rayner, K. (1992). Fast priming during eye fixations in reading. <em>Journal of Experimental Psychology: Human Perception and Performance, 18</em>(1), 173.</p>
<p>Sereno, S. C., &amp; Rayner, K. (2003). Measuring word recognition in reading: Eye movements and event-related potentials. <em>Trends in Cognitive Sciences, 7</em>(11), 489–493. https://doi.org/10.1016/j.tics.2003.09.010</p>
<p>Solan, H. A., Feldman, J., &amp; Tujak, L. (1995). Developing Visual and Reading Efficiency in Older Adults. <em>Optometry and Vision Science, 72</em>(2), 139.</p>
</section>
<section id="references-5" class="slide level2 smaller">
<h2>References</h2>
<p>Staub, A., Rayner, K., Pollatsek, A., Hyönä, J., &amp; Majewski, H. (2007). The time course of plausibility effects on eye movements in reading: Evidence from noun-noun compounds. <em>Journal of Experimental Psychology. Learning, Memory, and Cognition, 33</em>(6), 1162–1169. https://doi.org/10.1037/0278-7393.33.6.1162</p>
<div class="quarto-auto-generated-content">
<p><img src="images/logo_mils.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Reading</p>
</div>
</div>
</section></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session_3_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session_3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session_3_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    <script>videojs(video_shortcode_videojs_video2);</script>
    <script>videojs(video_shortcode_videojs_video3);</script>
    <script>videojs(video_shortcode_videojs_video4);</script>
    <script>videojs(video_shortcode_videojs_video5);</script>
    

</body></html>