<!DOCTYPE html>
<html lang="en"><head>
<script src="session_1_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_1_files/libs/quarto-html/tabby.min.js"></script>
<script src="session_1_files/libs/quarto-html/popper.min.js"></script>
<script src="session_1_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session_1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_1_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session_1_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session_1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session_1_files/libs/quarto-contrib/videojs/video.min.js"></script>
<link href="session_1_files/libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.555">

  <meta name="author" content="Badaya &amp; Baltais">
  <title>Introduction to eye-tracking</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session_1_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session_1_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="session_1_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session_1_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session_1_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session_1_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session_1_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .center-xy {
    margin: 0;
    position: absolute;
    top: 40%;
    left: 40%;
    -ms-transform: translateY(-50%), translateX(-50%);
    transform: translateY(-50%), translateX(-50%);
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Introduction to eye-tracking</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Badaya &amp; Baltais 
</div>
</div>
</div>

</section>
<section>
<section id="welcome-to-the-course" class="title-slide slide level1 center">
<h1>Welcome to the course</h1>

</section>
<section id="welcome-to-the-course-1" class="slide level2">
<h2>Welcome to the course</h2>
<p>Teaching team</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Esperanza Badaya
<ul>
<li>Research focus: Speech comprehension (esp.&nbsp;paralinguistic cues &amp; social cognition)</li>
<li>esperanza.badaya@ugent.be</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>Mariia Baltais
<ul>
<li>Research focus: Reading (esp.&nbsp;influence of frequency &amp; semantics on sentence processing)</li>
<li>mariia.baltais@ugent.be</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="welcome-to-the-course-2" class="slide level2">
<h2>Welcome to the course</h2>
<p>What about yourselves?</p>
<ul>
<li>Name &amp; uni/department</li>
<li>Research interest</li>
<li>Goal for this course?</li>
</ul>
</section>
<section id="overview-of-the-course" class="slide level2">
<h2>Overview of the course</h2>
<p>Learning objectives</p>
<ul>
<li>Basic knowledge of how eye-trackers work and how to ensure high quality data
<ul>
<li>Practice with EyeLink 1000+</li>
</ul></li>
<li>Understanding of eye-tracking measurements and their meaning in psycholinguistics</li>
<li>Introduction to the Visual World Paradigm and reading.
<ul>
<li>Set up, confounds, measures of interest.</li>
</ul></li>
<li>Introduction to eye-tracking data from raw data to analysis.
<ul>
<li>Data pre-processing, wrangling, models and visualization.</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-1" class="slide level2">
<h2>Overview of the course</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Materials (all available on Ufora and GitHub)</p>
<ul>
<li>Slides</li>
<li>Template scripts in PsychoPy/OpenSesame/Experiment Builder
<ul>
<li>Focus in class: PsychoPy</li>
</ul></li>
<li>Eye-tracking datasets</li>
<li>Template scripts for pre-processing and analysis</li>
<li>Further resources (papers &amp; book chapters)</li>
</ul>
</div><div class="column" style="width:50%;">
<p>Requirements</p>
<ul>
<li>Install PsychoPy
<ul>
<li>version</li>
</ul></li>
<li>Install R, RStudio
<ul>
<li>version</li>
<li>packages</li>
</ul></li>
<li>Install DataViewer</li>
<li>Readings
<ul>
<li>Experimental design .pdf</li>
<li>Recommended readings per day</li>
</ul></li>
</ul>
</div>
</div>
<aside class="notes">
<p>change to OS &amp; update</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="overview-of-the-course-2" class="slide level2">
<h2>Overview of the course</h2>
<table class="caption-top">
<colgroup>
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Content</th>
<th style="text-align: center;">Materials</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Day 1</td>
<td style="text-align: center;">Introduction to eye-tracking</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: center;">Day 2</td>
<td style="text-align: center;">Visual World Paradigm</td>
<td style="text-align: center;">Experimental design .pdf, RR_1, PsychoPy</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Day 3</td>
<td style="text-align: center;">Reading</td>
<td style="text-align: center;">RR_2</td>
</tr>
<tr class="even">
<td style="text-align: center;">Day 4</td>
<td style="text-align: center;">Lab session &amp; Data pre-processing</td>
<td style="text-align: center;">DataViewer / R, datasets</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Day 5</td>
<td style="text-align: center;">Data visualization and analysis</td>
<td style="text-align: center;">R, datasets</td>
</tr>
</tbody>
</table>
<p>Any issues with installation, come to Mariia or myself!</p>
<aside class="notes">
<p>change to OS &amp; update</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="introduction-to-eye-tracking" class="title-slide slide level1 center">
<h1>Introduction to eye-tracking</h1>

</section>
<section id="introduction-to-eye-tracking-1" class="slide level2">
<h2>Introduction to eye-tracking</h2>
<ol type="1">
<li>What is eye-tracking?</li>
<li>Why do we track eyes?
<ol type="i">
<li>The human visual system</li>
<li>Visual attention</li>
</ol></li>
<li>Eye movements</li>
<li>How do eye-trackers work?</li>
<li>An eye-tracking experiment
<ol type="i">
<li>The eye-tracking lab</li>
<li>Basic structure of the experiment</li>
</ol></li>
<li>Eye-tracking in language research
<ol type="i">
<li>Visual World Paradigm</li>
<li>Reading</li>
</ol></li>
</ol>
</section>
<section id="what-is-eye-tracking" class="slide level2">
<h2>What is eye-tracking?</h2>
<p><a href="https://www.youtube.com/watch?v=TDG6j2LkVqU">Flying object</a></p>
</section>
<section id="what-is-eye-tracking-1" class="slide level2">
<h2>What is eye-tracking?</h2>
<div style="text-align: center">
<p>Eye-tracking is a <span class="fg" style="--col: #e64173">non-invasive technique</span> to explore cognitive processes as they unfold (i.e., <span class="fg" style="--col: #e64173">online processing</span>). It has temporal resolution.</p>
</div>
<ul>
<li>As opposed to EEG, fMRI</li>
<li>As opposed to offline measures (e.g., questionnaires)
<ul>
<li>Converging evidence</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="why-do-we-track-eyes" class="title-slide slide level1 center">
<h1>Why do we track eyes?</h1>

</section>
<section id="the-human-visual-system" class="slide level2">
<h2>The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br> Important parts of the eye’s anatomy:</p>
<ul>
<li>Cornea</li>
<li>Pupil</li>
<li>Retina
<ul>
<li>Fovea</li>
<li>Parafovea</li>
<li>Periphery</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/eye_anatomy.png"></p>
</div>
</div>
</section>
<section id="the-human-visual-system-1" class="slide level2">
<h2>The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<ul>
<li>Light hits the cornea.
<ul>
<li>Some light is reflected (Purkinje reflections)</li>
</ul></li>
<li>Light enters the eye via the pupil.</li>
<li>The lens reflects the light onto the retina.
<ul>
<li>Photosensitive layer with <span class="fg" style="--col: #e64173">cones</span> and <span class="fg" style="--col: #e64173">rods</span>.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/eye_anatomy.png"></p>
</div>
</div>
</section>
<section id="the-human-visual-system-2" class="slide level2">
<h2>The Human Visual System</h2>
<p>Photoreceptors with different properties (e.g., spectral sensitivity, photopigments).</p>
<ul>
<li>Cones
<ul>
<li><span class="fg" style="--col: #e64173">Color vision and spatial frequency</span> (“visual details”).</li>
<li>Well-illuminated conditions (photopic vision).</li>
</ul></li>
<li>Rods
<ul>
<li><span class="fg" style="--col: #e64173">Black and white</span>.</li>
<li>Low-light conditions (scotopic vision).</li>
</ul></li>
</ul>
</section>
<section id="the-human-visual-system-3" class="slide level2">
<h2>The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>They also differ in where they are located in the fovea.</p>
<ul>
<li>Cones: Highest density in the <span class="fg" style="--col: #e64173">fovea</span>.</li>
<li>Rods: Higher density in the <span class="fg" style="--col: #e64173">fovea’s periphery</span>.</li>
</ul>
<p><span class="fg" style="--col: #e64173">Consequence</span>: Vision is sharpest in the fovea.</p>
<ul>
<li>25% of visual cortex devoted to processing 2.5° of the visual scene.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/rod_distribution.png"></p>
</div>
</div>
</section>
<section id="the-human-visual-system-4" class="slide level2">
<h2>The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<p><strong>But</strong> the fovea is a small section of our <span class="fg" style="--col: #e64173">visual field</span>.</p>
<ul>
<li>Space respect to our eyes that we can perceive.</li>
<li>Measured in degrees of visual angle.
<ul>
<li><span class="fg" style="--col: #e64173">Visual angle</span> = object size / distance.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/central_vision.png"></p>
</div>
</div>
<aside class="notes">
<p>“Space respect to our eyes” - ? Is it the definition of visual field or fovea?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-human-visual-system-5" class="slide level2">
<h2>The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Retina:</p>
<ul>
<li>Fovea: 1-2 degrees of visual angle.</li>
<li>Parafovea: 10 degrees of visual angle to either side.</li>
<li>Periphery: Remaining space beyond the parafovea.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-layout-panel" data-layout="[[-1], [1], [-1]]">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/vision_angles.png"></p>
<figcaption>Central vision != foveal vision.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="the-human-visual-system-6" class="slide level2">
<h2>The Human Visual System</h2>
<div class="quarto-layout-panel" data-layout="[[-1], [1], [-1]]">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/foveaparafovea_illustrationreading.JPG" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="the-human-visual-system-7" class="slide level2">
<h2>The Human Visual System</h2>
<p><br></p>
<div style="text-align: center">
<p>Therefore, we move our eyes to place visual stimuli in the fovea to process it with the highest acuity. <span class="fg" style="--col: #e64173">Eye movements are a consequence of the eyes’ anatomy</span>.</p>
</div>
<ul>
<li>Parafoveal processing: No acute image, words still partially recognizable.</li>
<li>Periphery: Blurred image, no word/letter recognition.</li>
</ul>
</section>
<section id="visual-attention" class="slide level2">
<h2>Visual attention</h2>
<p>What do you notice about the eye movements here? What do you infer from them?</p>
<p><a href="https://www.youtube.com/watch?v=jzeBKRjWVwE">Playing a video game</a></p>
</section>
<section id="visual-attention-1" class="slide level2">
<h2>Visual attention</h2>
<p><span class="fg" style="--col: #e64173">Attention</span> (i.e., <span class="fg" style="--col: #e64173">linking hypothesis</span>)</p>
<ul>
<li>Tracking eye movements can tell us what viewers are paying attention to.</li>
</ul>
</section>
<section id="visual-attention-2" class="slide level2">
<h2>Visual attention</h2>
<p>Attention determines what we process and the detail of the representation built.</p>
<p><a href="https://www.youtube.com/watch?v=U1saQoMRD8A">Attention test</a></p>
</section>
<section id="visual-attention-3" class="slide level2">
<h2>Visual attention</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="fg" style="--col: #e64173">Attention</span> (i.e., <span class="fg" style="--col: #e64173">linking hypothesis</span>)</p>
<ul>
<li><span class="fg" style="--col: #e64173">Bottom-up</span> and <span class="fg" style="--col: #e64173">top-down</span> processes.
<ul>
<li>Details that attract individuals’ attention (exogenous) v. Individuals’ strategies (endogenous).</li>
</ul></li>
<li>Individuals as active viewers.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/endogenous.jpg"></p>
<figcaption>Open University</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="visual-attention-4" class="slide level2">
<h2>Visual attention</h2>
<p>Yarbus (1960): Scan paths guided by attention.</p>

<img data-src="images/session_1/unexpected_visitor.png" class="r-stretch quarto-figure-center"><p class="caption">They Did Not Expect Him, Iliá Repin</p></section>
<section id="visual-attention-5" class="slide level2">
<h2>Visual attention</h2>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="images/session_1/unexpected_visitor.png"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/yarbus_freeviewing.png"></p>
<figcaption>Free viewing</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="visual-attention-6" class="slide level2">
<h2>Visual attention</h2>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="images/session_1/unexpected_visitor.png"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/yarbus_ages.png"></p>
<figcaption>Estimate individuals’ ages</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="visual-attention-7" class="slide level2">
<h2>Visual attention</h2>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="images/session_1/unexpected_visitor.png"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/yarbus_guessactivity.png"></p>
<figcaption>Estimate what they were doing when the visitor arrived</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="visual-attention-8" class="slide level2">
<h2>Visual attention</h2>
<p>Attention is a bridge between our minds and eye movements.</p>
<p><span class="fg" style="--col: #e64173">Potential caveat</span></p>
<ul>
<li>Covert versus overt attention:
<ul>
<li>Covert: Mental shift without physical evidence (e.g., looking at the slides while thinking about lunch).</li>
<li>Overt: Moving your eyes to check what time it is.</li>
</ul></li>
</ul>
<p>We only have access to overt attention.</p>
<ul>
<li>But they are highly interlinked.</li>
</ul>
</section>
<section id="visual-attention-9" class="slide level2">
<h2>Visual attention</h2>
<ul>
<li>In language<sup>1</sup>: The <span class="fg" style="--col: #e64173">eye-mind hypothesis</span> (Just &amp; Carpenter, 1980).
<ul>
<li><span class="fg" style="--col: #e64173">Where</span> we look indicates <span class="fg" style="--col: #e64173">what we are processing</span>, <span class="fg" style="--col: #e64173">for how long we look</span> indicates the <span class="fg" style="--col: #e64173">cognitive effort it takes to process it</span>.</li>
<li>This is an <strong>assumption</strong> (cf.&nbsp;Pickering et al., 2004; Magnuson, 2019).</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>NB later in the course this will be challenged, especially wrt speech comprehension.</p></li></ol></aside></section>
<section id="why-do-we-do-it" class="slide level2">
<h2>Why do we do it?</h2>
<ul>
<li>Visual acuity is highest in the fovea, but the fovea is a rather small section of the retina.</li>
<li>Moving our eyes helps us to ‘place’ objects on the fovea.</li>
<li>Why do we want to place something there? Arguably, because we are interested in it.</li>
<li>We move our eyes to what captures our attention to process it.</li>
</ul>
</section></section>
<section>
<section id="eye-movements" class="title-slide slide level1 center">
<h1>Eye movements</h1>

</section>
<section id="nature-of-eye-movements" class="slide level2">
<h2>Nature of eye-movements</h2>
<ul>
<li>One dominant eye<sup>1</sup>.</li>
<li>Binocular disparity:
<ul>
<li>Relatively small in healthy subjects.</li>
<li>Decreases over the time of a fixation.</li>
<li>No complete temporal synchrony in eye movements.</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p>More on this when we cover properly the lab set up.</p></li></ol></aside></section>
<section id="eye-movements-1" class="slide level2">
<h2>Eye movements</h2>
<div style="text-align: center">
<p><strong>What eye movements can you think of?</strong></p>
<div>
<ul>
<li class="fragment">Think of how we talk about things: we <span class="fg" style="--col: #e64173">fixate</span> on things, we <span class="fg" style="--col: #e64173">move</span> our eyes.</li>
<li class="fragment">We also <span class="fg" style="--col: #e64173">blink</span>.</li>
<li class="fragment">We can also measure <span class="fg" style="--col: #e64173">pupil size</span>.</li>
</ul>
</div>
</div>
</section>
<section id="eye-movements-2" class="slide level2">
<h2>Eye movements</h2>
<p><strong>What eye movements can you think of?</strong></p>
<div>
<ul>
<li class="fragment">Fixations.</li>
<li class="fragment">Saccades.</li>
<li class="fragment">Blinks.</li>
<li class="fragment">Smooth pursuit.</li>
<li class="fragment">Pupil size changes.</li>
</ul>
</div>
</section>
<section id="fixations" class="slide level2">
<h2>Fixations</h2>
<p>When our eye ‘stops’, i.e., multiple gaze points close in time and/or space.</p>
<ul>
<li><p>Automatic, physiological response.</p></li>
<li><p>Eye is <em>relatively</em> stable.</p></li>
<li><p>Average duration: 200 - 300 ms.</p></li>
<li><p>Minimal duration: 20 - 50 ms (not standard).</p></li>
</ul>
</section>
<section id="fixations-1" class="slide level2">
<h2>Fixations</h2>
<p>When our eye ‘stops’.</p>
<ul>
<li>Eye is <em>relatively</em> stable.
<ul>
<li>Tremor, drifts, microsaccades.</li>
</ul></li>
</ul>

<img data-src="images/session_1/fixation_tremor.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="saccades" class="slide level2">
<h2>Saccades</h2>

<img data-src="images/session_1/fixation_saccades.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="saccades-1" class="slide level2">
<h2>Saccades</h2>
<p>“Jerky” movement: Fast movement of the eye, usually from one fixation to another.</p>
<ul>
<li>Temporary blindness (i.e., <span class="fg" style="--col: #e64173">saccadic suppression</span>).</li>
<li>Reactive saccades versus Voluntary saccades.
<ul>
<li>Sudden appearance of an object versus Exploration.</li>
</ul></li>
</ul>
</section>
<section id="saccades-2" class="slide level2">
<h2>Saccades</h2>
<ul>
<li>Amplitude: distance travelled.
<ul>
<li>Average: 15°.</li>
</ul></li>
<li>Average duration: 30 - 80 ms.</li>
<li>Velocity, acceleration and deceleration.</li>
<li>Direction: Forwards and backwards (i.e., regressions).</li>
<li>Accuracy: Over- and under-shooting.
<ul>
<li>In simple lab conditions, fall slightly short of the target, thus followed by a small corrective saccade.</li>
</ul></li>
</ul>
</section>
<section id="saccades-3" class="slide level2">
<h2>Saccades</h2>
<p>Forwards and backwards (i.e., regressions).</p>
<ul>
<li>Short and long regressions.</li>
</ul>

<img data-src="images/session_1/example_reading.JPG" class="r-stretch quarto-figure-center"><p class="caption">From Conklin et al., 2018</p></section>
<section id="blinks" class="slide level2">
<h2>Blinks</h2>
<p>By necessity, people blink.</p>
<ul>
<li>Usually, surrounded by saccades.</li>
<li>Pupil changes when eyelids open/close.
<ul>
<li>Mostly important for data processing.</li>
</ul></li>
<li>Lab conditions.</li>
</ul>
</section>
<section id="smooth-pursuit" class="slide level2">
<h2>Smooth pursuit</h2>
<p>A “moving fixation” ~ following a target.</p>
<ul>
<li>Slower than a saccade, but bounded by the velocity of the target being followed.</li>
<li>Asymmetrical: Horizontal &gt; vertical.</li>
</ul>
</section>
<section id="pupil-size" class="slide level2">
<h2>Pupil size</h2>
<p>Pupil dilates for reasons other than light, e.g., <span class="fg" style="--col: #e64173">cognitive effort</span>.</p>
<ul>
<li><p>Linking hypothesis: Pupil size reflects effort exerted.</p>
<ul>
<li>Harder tasks = increase in pupil size.</li>
</ul></li>
<li><p>Increasing interest in language research, e.g., accented-speech comprehension.</p></li>
<li><p>Changes can take up to 3 s.</p>
<ul>
<li>Far longer than other eye events.</li>
</ul></li>
</ul>
</section>
<section id="eye-movements-3" class="slide level2">
<h2>Eye movements</h2>
<ul>
<li>There are five major eye movements (or events) that an eye-tracker can capture.</li>
<li>Fixations refer to ‘stable’ gazes on a space for a sustained period of time. We usually describe them in terms of how many they are, when they start, and how long they are.</li>
<li>Saccades are fast movements, commonly from one fixation to another. We describe them in terms of onset, offset, angle, velocity, latency, and acceleration.</li>
<li>Smooth pursuits are fixations that move.</li>
<li>Pupil size can change due to cognitive processing, whereby its size increases when effort is exerted.</li>
</ul>
</section>
<section class="slide level2">

<p>Questions so far? Thoughts?</p>
</section></section>
<section>
<section id="how-do-eye-trackers-work" class="title-slide slide level1 center">
<h1>How do eye-trackers work?</h1>

</section>
<section id="how-do-eye-trackers-work-1" class="slide level2">
<h2>How do eye-trackers work?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Old, rudimentary eye-trackers.</p>
<ul>
<li>Louis Émile Javal (1879)
<ul>
<li>‘Naked eye’ observations.</li>
<li>Stop-start pattern in reading.</li>
</ul></li>
<li>Edmund Huey (1898)
<ul>
<li>Primitive ‘eye-tracking’ device.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br> <br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/huey.JPG"></p>
<figcaption>Huey, 1898; from Hutton, 2019</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="how-do-eye-trackers-work-2" class="slide level2">
<h2>How do eye-trackers work?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br> <br> <br></p>
<ul>
<li>Alfred Yarbus
<ul>
<li>Suction cups reflecting onto a photosensitive surface.</li>
<li>Scan paths.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br></p>
<p><img data-src="images/session_1/oldtrackers.png"></p>
</div>
</div>
</section>
<section id="how-do-eye-trackers-work-3" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>Now you can see why nowadays eye-tracking is non-invasive.</p>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="images/session_1/trackers_today.jpg"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: center;">
<p><img data-src="images/session_1/headmounted_tracker.png"></p>
</div>
</div>
</div>
</section>
<section id="how-do-eye-trackers-work-4" class="slide level2">
<h2>How do eye-trackers work?</h2>
<ul>
<li>Detects gaze and records its x and y coordinates on the screen.</li>
<li>Parse gaze position into eye events, e.g., any gaze points nearby close in time are grouped into a <em>fixation</em>.</li>
</ul>
</section>
<section id="how-do-eye-trackers-work-5" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p><span class="fg" style="--col: #e64173">Detects gaze</span> and records its x and y coordinates on the screen.</p>
<ul>
<li>Video-based recording of the location of either one point, the pupil, or two points, the pupil and the corneal reflection.</li>
<li>P-CR (<span class="fg" style="--col: #e64173">the pupil and the corneal reflection</span>) is the most common.
<ul>
<li>Two hardware components: A camera and an infrared illuminator (which are fixed in space).</li>
<li><span class="fg" style="--col: #e64173">1<span class="math inline">\(^{st}\)</span> corneal reflection (Purkinje reflection 1)</span>.</li>
</ul></li>
</ul>

<img data-src="images/session_1/tracking_eyes2.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="how-do-eye-trackers-work-6" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p><span class="fg" style="--col: #e64173">Detects gaze</span> and records its x and y coordinates on the screen.</p>
<ul>
<li>Video-based recording of the location of two points: <span class="fg" style="--col: #e64173">the pupil and the corneal reflection</span>.
<ul>
<li>Infrared light is reflected on the participant’s eyes.</li>
<li>Camera picks up the corneal reflection.</li>
<li>Algorithm-based image processing to identify these two locations</li>
</ul></li>
</ul>

<img data-src="images/session_1/tracking_eyes3.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="how-do-eye-trackers-work-7" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>Why pupil and corneal reflection?</p>
<ul>
<li>A ‘two-points of reference’ system.
<ul>
<li>The pupil moves when we move our eyes (and so does its location on the camera).</li>
<li>But corneal reflection does not (because the light source does not move).</li>
</ul></li>
<li><span class="fg" style="--col: #e64173">Compensate for small head movements</span>.</li>
</ul>

<img data-src="images/session_1/tracking_eyes1.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="how-do-eye-trackers-work-8" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>Why pupil and corneal reflection?</p>
<ul>
<li>A ‘two-points of reference’ system.
<ul>
<li>Distance between pupil and corneal reflection changes with eye rotation, but is relatively <span class="fg" style="--col: #e64173">constant</span> with head movements.</li>
<li>Pupil position - CR position.</li>
</ul></li>
</ul>

<img data-src="images/session_1/tracking_eyes4.png" class="r-stretch quarto-figure-center"><p class="caption">Example of view of the pupil when the head moves.</p></section>
<section id="how-do-eye-trackers-work-9" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>Detects gaze and <span class="fg" style="--col: #e64173">records</span> its x and y coordinates on the screen.</p>
<ul>
<li>How many times? <span class="fg" style="--col: #e64173">Sampling frequency</span></li>
</ul>
</section>
<section id="how-do-eye-trackers-work-sampling-frequency" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p><span class="fg" style="--col: #e64173">Sampling frequency</span> (or rate): Number of times the tracker measures gaze position per second. Measured in hertz (Hz).</p>
<ul>
<li>300 Hz: 1 data point every ~3 ms, 300 samples per second.
<ul>
<li>1 s is 1000 ms.</li>
<li>1000/300 = 3.33</li>
</ul></li>
<li>Higher sampling frequency = more data points, and closer in time.</li>
</ul>
</section>
<section id="how-do-eye-trackers-work-sampling-frequency-1" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p><strong>Question</strong></p>
<div>
<ul>
<li class="fragment">If our sampling frequency is 500 Hz, what is the time elapsed between each data point?
<ul>
<li class="fragment">2 ms.</li>
</ul></li>
<li class="fragment">And at 60 Hz?
<ul>
<li class="fragment">~16 ms.</li>
</ul></li>
<li class="fragment">And 2000 Hz?
<ul>
<li class="fragment">0.5 ms.</li>
</ul></li>
</ul>
</div>
</section>
<section id="how-do-eye-trackers-work-sampling-frequency-2" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p>Relationship between sampling frequency and measure of interest.</p>
<ul>
<li>The <span class="fg" style="--col: #e64173">faster</span> it is, the <span class="fg" style="--col: #e64173">higher</span> the sampling frequency needed to detect it.
<ul>
<li>Onset, offset, duration.</li>
</ul></li>
</ul>
<p>Example: 50 Hz sampling frequency.</p>
<ul>
<li>50 samples in a second, one every 20 ms.
<ul>
<li>A saccade starts in these 20 ms, we miss it.</li>
<li>A fixation begins in these 20 ms, we get it, but misrepresent its onset.</li>
</ul></li>
</ul>
</section>
<section id="how-do-eye-trackers-work-sampling-frequency-3" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p><br> <br></p>

<img data-src="images/session_1/sample_rate.jpg" class="r-stretch quarto-figure-center"><p class="caption">Measurement error represented by dashed line; picture taken when the vertical lines cross the horizontal lines.</p></section>
<section id="how-do-eye-trackers-work-sampling-frequency-4" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p><span class="fg" style="--col: #e64173">This is why sampling frequency matters</span>.</p>
<ul>
<li><p>Depends on your research question, e.g., reading versus observing a scene.</p></li>
<li><p><span class="fg" style="--col: #e64173">Reading</span> usually requires <span class="fg" style="--col: #e64173">higher</span> sampling frequency.</p></li>
<li><p>Higher sample frequency = <span class="fg" style="--col: #e64173">higher precision</span>.</p></li>
<li><p>Considering as well the size of the object we are interested in.</p>
<ul>
<li>Big –&gt; can afford low sampling rate, precision and accuracy might be ok.</li>
<li>Small (e.g., reading) –&gt; high sampling rate (&gt;250 Hz).</li>
</ul></li>
</ul>
</section>
<section id="how-do-eye-trackers-work-sampling-frequency-5" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p>How small (or slow) can you go with the sampling frequency?</p>
<ul>
<li>Nyquist-Shannon sampling theorem
<ul>
<li>Twice as large as the event you want to measure.</li>
</ul></li>
</ul>
<p>In reality, more like ‘rule-of-thumb’:</p>
<ul>
<li>250 Hz</li>
</ul>
</section>
<section id="how-do-eye-trackers-work-sampling-frequency-6" class="slide level2">
<h2>How do eye-trackers work?: Sampling frequency</h2>
<p>How small (or slow) can you go with the sampling frequency?</p>
<p>Relationship between sampling error, sampling frequency, and <span class="fg" style="--col: #e64173">data points</span>.</p>
<ul>
<li>Lower sampling frequencies = more data points (power).
<ul>
<li>NB: More data points might not be the solution.</li>
</ul></li>
</ul>
<p>Conventional: min. 500 Hz for picture viewing, min. 1000 Hz for reading.</p>
</section>
<section id="how-do-eye-trackers-work-10" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>Detects gaze and records its <span class="fg" style="--col: #e64173">x and y coordinates</span> on the screen.</p>
<ul>
<li>How do we know that the x and y detected are good? Accuracy and precision</li>
<li><span class="fg" style="--col: #e64173">Accuracy</span>: Difference between the measurement and its true value.</li>
<li><span class="fg" style="--col: #e64173">Precision</span>: Ability to reproduce a reliable measurement.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/precision_accuracy.JPG" height="300"></p>
<figcaption>From Holmqvist et al., 2011</figcaption>
</figure>
</div>
</section>
<section id="how-do-eye-trackers-work-11" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>Detects gaze and records its <span class="fg" style="--col: #e64173">x and y coordinates</span> on the screen.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Accuracy</p>
<ul>
<li>Recorded gaze position versus the true gaze position.</li>
<li>Methods to increase accuracy in the experimental session.
<ul>
<li>Calibration, validation, drift correction.</li>
<li>Screen corners.</li>
</ul></li>
<li>Data loss.</li>
</ul>
</div><div class="column" style="width:50%;">
<p>Precision</p>
<ul>
<li>Spatial precision: Eye fixating on a stationary target.</li>
<li>Temporal precision: Standard deviation of delays from actual movements until they are marked (eye-tracker latencies).</li>
</ul>
</div>
</div>
</section>
<section id="how-do-eye-trackers-work-12" class="slide level2">
<h2>How do eye-trackers work?</h2>
<p>These properties impact data quality.</p>
<ul>
<li>The <span class="fg" style="--col: #e64173">validity of results</span> based on eye movement analysis are clearly dependent on the quality of eye movement data (Holmqvist et al., 2012)</li>
</ul>
<aside class="notes">
<p>did you mean Holmqvist et al., 2011?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="how-do-eye-trackers-work-types" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Different eye-tracking systems on the market.</p>
<ul>
<li>SR Research, Tobii, SMI.</li>
</ul>
<p>Differences in software, e.g., parsing of events and subsequent measures.</p>
<ul>
<li>Research question</li>
</ul>
</section>
<section id="how-do-eye-trackers-work-types-1" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Different types of trackers on the market as a function of:</p>
<ul>
<li><p>Position camera wrt eye.</p></li>
<li><p>Lens zoom.</p></li>
<li><p>Software for parsing events.</p></li>
<li><p>Pros and cons of each.</p>
<ul>
<li>E.g., they differ in sampling frequency (head-mounted tend to have lower sampling frequencies).</li>
</ul></li>
</ul>
</section>
<section id="how-do-eye-trackers-work-types-2" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p><br></p>

<img data-src="images/session_1/tracker_types.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="how-do-eye-trackers-work-types-3" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Table-mounted, head fixed:</p>
<ul>
<li>Assumed constant distance.
<ul>
<li>Camera and infrared light are above the participant’s head or fixed position near the monitor (desktop mounted).</li>
</ul></li>
<li>Head movement is restricted.</li>
<li>Pros: High precision.</li>
<li>Cons: Not suitable for all populations.</li>
</ul>
</section>
<section id="how-do-eye-trackers-work-types-4" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Table-mounted, head free:</p>
<ul>
<li><p>Distance changes.</p>
<ul>
<li>Camera and infrared light can be on the table or monitor.</li>
<li>Range of head movement.
<ul>
<li>Head distance.</li>
</ul></li>
</ul></li>
<li><p>Pros: Generally, high precision; suitable for more populations.</p></li>
<li><p>Cons: The lack of head restriction can lower data precision.</p></li>
<li><p>Portable trackers.</p></li>
<li><p>Convertible trackers.</p></li>
</ul>
</section>
<section id="how-do-eye-trackers-work-types-5" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Head-mounted:</p>
<ul>
<li>Camera and infrared light are mounted on the participant’s head (e.g., with glasses)</li>
<li>Pros: Ecological validaity.</li>
<li>Cons: Usually, lower sampling frequencies, lower accuracy.</li>
</ul>
</section>
<section id="how-do-eye-trackers-work-types-6" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Eye-tracking without an eye-tracker.</p>

<img data-src="images/session_1/alternative_trackers.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="how-do-eye-trackers-work-types-7" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Eye-tracking without an eye-tracker.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/mt_1.png" height="500"></p>
<figcaption>King et al., 2018</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/mt_2.png" class="quarto-figure quarto-figure-center" height="500"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="how-do-eye-trackers-work-types-8" class="slide level2">
<h2>How do eye-trackers work?: Types</h2>
<p>Combination with other techniques.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/combining_tracking.png" class="quarto-figure quarto-figure-center" height="300"></p>
</figure>
</div>
</section>
<section id="how-do-eye-trackers-work-13" class="slide level2">
<h2>How do eye-trackers work?</h2>
<ul>
<li>We track the x and y coordinates of gaze on a screen by figuring out the position of either the pupil or the pupil and the corneal reflection.</li>
<li>How many times we record the x and y coordinates is determined by the sampling frequency.</li>
<li>Our sampling frequency is determined by our measure of interest.</li>
<li>We distinguish eye-trackers depending on the distance between the eyes and the camera.</li>
</ul>
</section>
<section id="eye-tracking-pros" class="slide level2">
<h2>Eye-tracking: Pros</h2>
<div class="fragment">
<ul>
<li>Widely applicable technique.</li>
<li>Relatively easy to interpret.</li>
<li>Can provide a huge range of online measures.</li>
<li>Temporal precision.</li>
<li>Relatively high ecological validity.</li>
</ul>
</div>
</section>
<section id="eye-tracking-cons" class="slide level2">
<h2>Eye-tracking: Cons</h2>
<div class="fragment">
<ul>
<li>Relatively expensive.
<ul>
<li>In terms of money: up to 30,000 e (but in comparison to EEG?).</li>
<li>In terms of time: one participant at a time<sup>1</sup>.</li>
</ul></li>
<li>Trade-off between accuracy and ecological validity.</li>
<li>Participants’ criteria.</li>
</ul>
</div>
<aside><ol class="aside-footnotes"><li id="fn3"><p>Dual eye-tracking experiments are being done!</p></li></ol></aside></section>
<section class="slide level2">

<p>Let’s take a break!</p>
</section></section>
<section>
<section id="an-eye-tracking-experiment" class="title-slide slide level1 center">
<h1>An eye-tracking experiment</h1>

</section>
<section id="the-eye-tracking-lab" class="slide level2">
<h2>The eye-tracking lab</h2>
<ul>
<li>Most eye-trackers nowadays are video-based recordings of pupil and corneal reflection.</li>
<li>Two hardware components: A camera and an infrared illuminator.</li>
<li>Gaze position is captured every X ms (depends on sampling rate).</li>
</ul>
<p>Question is: How does an eye-tracking lab look like? What elements do you think the lab has?</p>
<div class="fragment">
<p>A camera, a screen, etc.</p>
</div>
</section>
<section id="the-eye-tracking-lab-1" class="slide level2">
<h2>The eye-tracking lab</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<p>Hardware</p>
<ul>
<li>2 PCs (Host and Presentation)</li>
<li>2 Screens (Experimenter and Participants)</li>
<li>Camera and infrared</li>
<li>Peripheral hardware</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/lab_setup2.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-eye-tracking-lab-2" class="slide level2">
<h2>The eye-tracking lab</h2>

<img data-src="images/lab_setup/lab_setup.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="the-eye-tracking-lab-3" class="slide level2">
<h2>The eye-tracking lab</h2>
<p>Hardware set up.</p>
<ul>
<li>Distance between participant, camera, and participant’s screen are measured.
<ul>
<li>Distance participant - monitor: 40-70 cm</li>
<li>Distance participant - camera: ideal 50-55 cm</li>
<li>Interplay with screen size.</li>
<li><span class="bg" style="--col: #e64173">/!\</span> Trackable range of the tracker</li>
<li>Constant across participants</li>
</ul></li>
</ul>
<p>All in a sound-isolated, no-sunlight room.</p>
</section>
<section id="the-eye-tracking-lab-4" class="slide level2">
<h2>The eye-tracking lab</h2>
<p>Camera position:</p>
<ul>
<li>Slightly below when recording .</li>
<li>Not too low (problems in upper areas)/high (problems in lower areas) or you may lose CR.</li>
<li>In desktop-mounted: Alignment with lower part of the monitor.</li>
<li>In tower-mounted: Above participants’ head.</li>
</ul>
</section>
<section id="the-eye-tracking-lab-5" class="slide level2">
<h2>The eye-tracking lab</h2>
<p>Host PC and camera are connected and synchronized. Host PC and Presentation PC are connected and synchronized.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Recording/Host PC
<ul>
<li>Camera set up.</li>
<li>Records triggers.</li>
<li>Processes and records eye-tracking data.</li>
<li>Observe eye movements in real time.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>Presentation PC
<ul>
<li>Presentation of stimuli.</li>
<li>Sends triggers and trial information.</li>
<li>Control when in time gaze is actually recorded.</li>
<li>Gathers behavioural data.</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="the-eye-tracking-lab-6" class="slide level2">
<h2>The eye-tracking lab</h2>
<p>What information do you think is sent between the two computers?</p>
<div class="fragment">
<ul>
<li>Areas of Interest</li>
<li>Triggers</li>
<li>Experimental condition</li>
<li>Stimuli per trial</li>
<li>…</li>
</ul>
<p><span class="bg" style="--col: #e64173">/!\</span> Keep this in mind when coding the experiment.</p>
</div>
</section>
<section id="structure-of-an-eye-tracking-experiment" class="slide level2">
<h2>Structure of an eye-tracking experiment</h2>
<ol type="1">
<li><p>Set up</p>
<ul>
<li>Welcome participant, instructions
<ul>
<li>Including introduction to the eye-tracker</li>
</ul></li>
<li>Check make up</li>
<li>Camera set up
<ul>
<li>Checks prior to experiment
<ul>
<li>Comfortable seating</li>
<li>Focus</li>
<li>Corners &amp; areas of interest</li>
<li>Automatic &amp; adjust thresholding</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Experiment</p></li>
<li><p>After it</p></li>
</ol>
</section>
<section id="structure-of-an-eye-tracking-experiment-1" class="slide level2">
<h2>Structure of an eye-tracking experiment</h2>
<ol type="1">
<li><p>Set up</p></li>
<li><p>Experiment</p>
<ul>
<li>Calibration &amp; validation</li>
<li>Practice trials &amp; familiarisation (optional)</li>
<li>Experimental phase (recording sequence, block division - optional)
<ul>
<li>(Drift correction) Stimulus –&gt; Trigger –&gt; Follow-up</li>
<li>Goodbye screen</li>
</ul></li>
</ul></li>
<li><p>After it</p></li>
</ol>
</section>
<section id="structure-of-an-eye-tracking-experiment-2" class="slide level2">
<h2>Structure of an eye-tracking experiment</h2>
<ol type="1">
<li><p>Set up</p></li>
<li><p>Experiment</p></li>
<li><p>After it</p>
<ul>
<li>Post-experimental questionnaires &amp; debriefing.</li>
</ul></li>
</ol>
</section>
<section id="participant-set-up" class="slide level2">
<h2>Participant set up</h2>
<p>Briefing</p>
<ul>
<li>Explain the experiment and allow them to ask questions.
<ul>
<li>Add instructions to the experiment &amp; practice sessions.</li>
</ul></li>
<li>Let them know in advance the experiment structure (e.g., calibration &gt; validation &gt; experiment (practice if there is)).</li>
<li>Explain (briefly) how the eye-tracker works.</li>
</ul>
<div class="fragment">
<p>Tempted to ask to not blink?</p>
<ul>
<li>Not blinking leads to dry eyes, and ultimately more blinking.</li>
<li>Instead: Tell them when it is preferred for them to blink (if you decide to mention anything).</li>
</ul>
</div>
</section>
<section id="participant-set-up-1" class="slide level2">
<h2>Participant set up</h2>
<p>Adjust seating</p>
<ul>
<li>Chinrest</li>
<li>Chair height</li>
<li>Sitting straight</li>
<li>Head against foreheadrest, chin over chinrest.</li>
</ul>
</section>
<section id="camera-set-up" class="slide level2">
<h2>Camera set up</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Access to eye image</p>
<ul>
<li>Hidden image: set-up is automatic.
<ul>
<li>Easier to handle but potential for poorer data quality, e.g., data recording issues may go unnoticed.</li>
</ul></li>
<li><span class="fg" style="--col: #e64173">Access</span> to image.
<ul>
<li>Need for technical knowledge.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/host_pc_setup.JPG"></p>
</div>
</div>
</section>
<section id="camera-set-up-1" class="slide level2">
<h2>Camera set up</h2>
<p>Which eye to track? - Dominant eye (by default, right eye)</p>
<p>Detect the eye</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/session_1/eye_undetected.JPG" height="300"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/eye_detected.JPG" height="300"></p>
</div>
</div>
</section>
<section id="camera-set-up-2" class="slide level2">
<h2>Camera set up</h2>
<p>Focus</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/session_1/unfocused_camera.JPG" height="300"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/eye_detected.JPG" height="300"></p>
</div>
</div>
</section>
<section id="camera-set-up-3" class="slide level2">
<h2>Camera set up</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br> <br> <br></p>
<ul>
<li>P-CR tracking:
<ul>
<li>Dark blue pupil</li>
<li>Light blue CR</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/lab_setup/good_captureeye1.png"> <img data-src="images/lab_setup/good_pupil.png"></p>
</div>
</div>
</section>
<section id="camera-set-up-4" class="slide level2">
<h2>Camera set up</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<p>Automatic thresholding + adjustments</p>
<ul>
<li>Reflections = impossible eye movements</li>
</ul>
<p>Pupil tracking</p>
<ul>
<li>Centroid versus Ellipse</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_1/options.jpg" height="550"></p>
</div>
</div>
</section>
<section id="camera-set-up-5" class="slide level2">
<h2>Camera set up</h2>
<ul>
<li>Ask them to move their eyes around the corners of the screen
<ul>
<li>Notice whether we lose track at any place</li>
<li>Especially near your AIs</li>
<li>Ensure pupil is not covered
<ul>
<li>E.g., hair, glasses frame</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="camera-set-up-issues" class="slide level2">
<h2>Camera set up: Issues</h2>
<ul>
<li>No mascara
<ul>
<li>Dark lashes lead to issues -&gt; confounded with the pupil.</li>
</ul></li>
<li>Eyelids
<ul>
<li>Cover the pupil in lower gaze directions</li>
<li>An issue in some cases (e.g., tired participants)</li>
<li>Recording from lower then useful.</li>
</ul></li>
</ul>
</section>
<section id="camera-set-up-issues-1" class="slide level2">
<h2>Camera set up: Issues</h2>
<ul>
<li>Glasses
<ul>
<li>Darker image -&gt; issues in thresholding</li>
<li>Clean glasses</li>
<li>Adjust contrast/brightness</li>
</ul></li>
<li>Glasses
<ul>
<li>Reflect infrared light</li>
<li>Clean glasses</li>
<li>Move camera (angle rather than distance)
<ul>
<li>Place reflections far away from pupil and CR</li>
</ul></li>
<li>Move mirror (only possible in tower-mounted eye-trackers)</li>
</ul></li>
</ul>
</section>
<section id="camera-set-up-issues-2" class="slide level2">
<h2>Camera set up: Issues</h2>
<ul>
<li>Contact lenses
<ul>
<li>Air bubbles between eye and contact lense interact with CR
<ul>
<li>Camera focus</li>
</ul></li>
</ul></li>
<li>Wet eyes
<ul>
<li>Split up the CR
<ul>
<li>Breaks</li>
<li>Stop the experiment</li>
<li>Dim the light in the lab</li>
<li>Tell the participant to go home</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="experiment" class="slide level2">
<h2>Experiment</h2>
<p>Beyond the experiment itself, all eye-tracking experiments share these steps.</p>
<ul>
<li>Calibration</li>
<li>Validation</li>
<li>Drift correction</li>
</ul>
<p>Why?</p>
<ul>
<li>Because of how gaze position is calculated.
<ul>
<li>Estimation from known coordinates = calibration points.</li>
</ul></li>
<li>Accuracy.</li>
</ul>
</section>
<section id="calibration" class="slide level2">
<h2>Calibration</h2>
<p>x,y coordinates of gaze position are estimated from measuring the pupil and the CR on known coordinates (i.e., the calibration points).</p>
<div class="r-stack">
<video id="video_shortcode_videojs_video1" height="500" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="images/lab_setup/calibration_cut.mp4"></video>
</div>
</section>
<section id="calibration-1" class="slide level2">
<h2>Calibration</h2>
<ul>
<li>Beginning of the experiment (blocks?)</li>
<li>Fixate on a series of points
<ul>
<li># points ~ accuracy
<ul>
<li>9HV standard</li>
<li>Smaller AIs need more calibration points</li>
<li>Higher precision = more calibration points.</li>
</ul></li>
</ul></li>
<li>Interplay with kind of eye-tracker:
<ul>
<li>Portable versus fixed.</li>
</ul></li>
</ul>
</section>
<section id="calibration-2" class="slide level2">
<h2>Calibration</h2>
<ul>
<li>Automatic versus manual
<ul>
<li>Participants moving their gaze before they should.</li>
</ul></li>
<li>Adults versus children</li>
<li>Should cover the area where stimuli are presented</li>
</ul>
</section>
<section id="calibration-3" class="slide level2">
<h2>Calibration</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lab_setup/bad_calibration.jpg" height="500"></p>
<figcaption>Bad calibration</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lab_setup/good_calibration.jpg" height="500"></p>
<figcaption>Good calibration</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="calibration-4" class="slide level2">
<h2>Calibration</h2>
<p>Tips:</p>
<ul>
<li>Check that pupil and CR are stable on the four corners of the screen.</li>
<li>Remind participants not to move during the calibration and afterwards.</li>
<li>Explicitly remind them to look at the center of the point and to look until they disappear.</li>
<li>Random order presentation.</li>
<li>Desktop: move camera angle.</li>
<li>Tower-mounted: move mirror.</li>
</ul>
</section>
<section id="validation" class="slide level2">
<h2>Validation</h2>
<p>Done after calibration to ensure accuracy. Same principle as calibration.</p>
<div class="r-stack">
<video id="video_shortcode_videojs_video2" height="500" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="images/lab_setup/validation_cut.mp4"></video>
</div>
</section>
<section id="validation-1" class="slide level2">
<h2>Validation</h2>
<ul>
<li>Further accuracy check.
<ul>
<li>Whole versus subset
<ul>
<li>Error &gt; 0.5, recalibrate.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="validation-2" class="slide level2">
<h2>Validation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lab_setup/bad_validation.jpg" height="500"></p>
<figcaption>Bad validation</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lab_setup/good_validation.jpg" height="500"></p>
<figcaption>Good calibration</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section class="slide level2">

<p>Technically speaking, you could now start recording.</p>
<p>You’d get eye data of your experiment, but…</p>
<ul>
<li>How do we make sure that our calibration points are accurate throughout the experiment?</li>
<li>How can we make sense of the data after the experiment?
<ul>
<li>Areas of Interest</li>
<li>Triggers</li>
</ul></li>
</ul>
</section>
<section id="during-the-experiment" class="slide level2">
<h2>During the experiment</h2>
<p>During trials</p>
<ul>
<li>Lab log
<ul>
<li>Adjustments/recalibrations</li>
<li>Sneezes, sounds</li>
<li>Data cleaning</li>
<li>Blinking</li>
<li>Are they moving their lips while they read?</li>
</ul></li>
</ul>
</section>
<section id="drift-correction" class="slide level2">
<h2>Drift correction</h2>
<p>Every so often, re-check our measurement (accuracy): Drift correction.</p>
<ul>
<li>Commonly, each trial/every other trial/blocks</li>
<li>Ensure accuracy</li>
<li>Where the fixation cross appears depends on your task.
<ul>
<li>At the position where you want high accuracy.</li>
<li>Reading: e.g., beginning of the sentence/paragraph.</li>
</ul></li>
</ul>
</section>
<section id="drift-correction-1" class="slide level2">
<h2>Drift correction</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lab_setup/drift_reading.jpg" height="500"></p>
<figcaption>Drift correction reading</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/lab_setup/drift_vwp.jpg" height="500"></p>
<figcaption>Drift correction VWP</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="areas-of-interest" class="slide level2">
<h2>Areas of Interest</h2>
<p>Areas of display we are interested in measuring and analysing.</p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/ia_reading.png" height="200"></p>
<figcaption>Single sentence reading</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/ia_vwp.png" height="300"></p>
<figcaption>VWP</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="areas-of-interest-1" class="slide level2">
<h2>Areas of Interest</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Reading</p>
<ul>
<li>Word</li>
<li>Sequence of words</li>
<li>Part of a sentence</li>
</ul>
</div><div class="column" style="width:50%;">
<p>VWP</p>
<ul>
<li>Individual images</li>
<li>Blank space!</li>
</ul>
</div>
</div>
</section>
<section id="areas-of-interest-2" class="slide level2">
<h2>Areas of Interest</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Reading</p>
<p>Careful definition of AIs.</p>
<ul>
<li>Manually versus automatic (individual words)</li>
<li>Issues: Segmentation of pre-critical and spillover regions.</li>
<li>Issue: Duplicate data.</li>
<li>Issue: Classification of eye-movements (regressions)</li>
</ul>
</div><div class="column" style="width:50%;">
<p>VWP</p>
<p>Difference between comprehension and production.</p>
<ul>
<li>In the latter case, AIs might not be known in advance.</li>
</ul>
</div>
</div>
</section>
<section id="areas-of-interest-3" class="slide level2">
<h2>Areas of Interest</h2>
<p>Size depends on accuracy and precision of the eye-tracker.</p>
<ul>
<li><p>Low accuracy/precision: Bigger areas of interest.</p></li>
<li><p>Do not put stimuli close to margins.</p>
<ul>
<li>Fixations are generally drawn to the center of a screen.</li>
<li>Lower precision at margins of a monitor.</li>
</ul></li>
<li><p>Overlapping areas of interest.</p></li>
</ul>
<p>Particularly relevant for gaze-contingent paradigms!</p>
</section>
<section id="areas-of-interest-4" class="slide level2">
<h2>Areas of Interest</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Text</p>
<ul>
<li>Span some pixels below and above.</li>
</ul>
</div><div class="column" style="width:50%;">
<p>Pictures</p>
<ul>
<li>Enough separation.</li>
<li>Counterbalance position.</li>
<li>No overlapping.</li>
</ul>
</div>
</div>
</section>
<section id="triggers" class="slide level2">
<h2>Triggers</h2>
<p>Trigger: Information about an event, i.e., when it happens.</p>
<ul>
<li>Synchronization stimulus PC - host PC</li>
</ul>
<p>Most basic trigger:</p>
<ul>
<li>Trial information (e.g., condition, response)</li>
</ul>
<p>But when you also have audio:</p>
<ul>
<li>When the audio begins and ends, gaze, for example.</li>
</ul>
<p>Related to stimulus-synchrony!</p>
</section>
<section id="end-of-experiment" class="slide level2">
<h2>End of experiment</h2>
<p>Any additional data you may want to explore in relation to eye-tracking data.</p>
<ul>
<li>Language proficiency?</li>
<li>Personality traits?</li>
</ul>
<p>Debriefing</p>
<p>See also Rodriguez Ronderos et al.&nbsp;(2018) - Eye Tracking During Visually Situated Language Comprehension: Flexibility and Limitations in Uncovering Visual Context Effects.</p>
</section></section>
<section>
<section id="eye-tracking-in-language-research" class="title-slide slide level1 center">
<h1>Eye-tracking in language research</h1>

</section>
<section id="eye-tracking-in-language-research-1" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Thus far, we have covered why we are measuring eye movements, how we measure them, and what eye movements we can measure.</p>
<ul>
<li>How does this transfer to language research?</li>
</ul>
</section>
<section id="eye-tracking-in-language-research-2" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Eye-tracking is an <em>online</em> measurement of cognitive processes.</p>
<ul>
<li>Different online techniques e.g., EEG, fMRI
<ul>
<li>Spatial versus temporal resolution.</li>
</ul></li>
<li>Offline measurements, e.g., comprehension questions.</li>
</ul>
<p>Ferreira &amp; Yang (2019): <span class="fg" style="--col: #e64173">Linking offline and online measures</span> can provide a better understanding of how speech is processed and represented.</p>
</section>
<section id="eye-tracking-in-language-research-3" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Focus of this course:</p>
<ul>
<li>Visual World Paradigm (i.e., speech comprehension)</li>
<li>Reading Paradigms (i.e., written text comprehension)</li>
</ul>
</section>
<section id="eye-tracking-in-language-research-4" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<ul>
<li>VWP has predominantly been used to investigate speech comprehension as it unfolds, to answer questions such as lexical access, prediction of upcoming elements in the speech signal, or even the interpretation of an utterance away from its propositional content.</li>
</ul>
</section>
<section id="eye-tracking-in-language-research-5" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<p>Communication is multimodal: Both verbal and non-verbal features exist when we produce and comprehend spoken language, and they can facilitate communication. Disfluencies (e.g., um or uh in English) are predominant in spontaneous speech.</p>
<ul>
<li>Do they affect listeners because of beliefs about disfluencies, or because of social reasoning?
<ul>
<li>Research question: Whether and how disfluent speech can be interpreted as deceitful, and whether there is an effect of interlocutors’ linguistic background (i.e., native versus non-native interlocutors).</li>
</ul></li>
</ul>
</section>
<section id="eye-tracking-in-language-research-6" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<ul>
<li>Treasure hunt paradigm</li>
</ul>
<aside class="notes">
<p>don’t forget to update this part :)</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-tracking-in-language-research-7" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
</section>
<section id="eye-tracking-in-language-research-8" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
</section>
<section id="eye-tracking-in-language-research-9" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<p>Conclusions:</p>
<ul>
<li>Participants made their decision shortly after the potential location was said (circa 400 ms).</li>
<li>The presence of a disfluency not only led to more interpretations of deceit, as reflected in the object selected, but also biased eye movements early.</li>
<li>There was no difference between native and non-native speakers.</li>
</ul>
<p>The interpretation of deceit triggered by disfluencies is inflexible, and likely heavily anchored in a stereotype of how deceit sounds (without any form of social reasoning).</p>
</section>
<section id="eye-tracking-in-language-research-10" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<ul>
<li>We investigate naturalistic reading behaviour to make inferences about underlying linguistic processing mechanisms (lexical access, semantic and syntactic integration, prediction…)</li>
<li>Stimuli of different length (single sentence vs.&nbsp;text), different tasks (skim for gist, answer specific questions, etc.)</li>
</ul>
</section>
<section id="eye-tracking-in-language-research-11" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<p>Constructions = grammatical patterns. Their productivity = the number of different words they can be used with. If productivity is high, there are many possible continuations that a sentence with this construction can have (example: “He started to…”). If productivity is low, there are very few possible continuations (example: “He burst into…”).</p>
<ul>
<li>In this low uncertainty situation, will a prediction error occur if readers see a continuation they did not expect?
<ul>
<li>Research question: Does low productivity of syntactic constructions lead to processing difficulty for unexpected lexical items?</li>
</ul></li>
</ul>
</section>
<section id="eye-tracking-in-language-research-12" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<p>High productivity condition:</p>
<ul>
<li>Francisco | <strong>se metió a</strong> | <span class="fg" style="--col: #e64173">hablar</span> | de política | con sus amigos</li>
</ul>
<p>Low productivity condition:</p>
<ul>
<li>Francisco | <strong>rompió a</strong> | <span class="fg" style="--col: #e64173">hablar</span> | de política | con sus amigos</li>
</ul>
</section>
<section id="eye-tracking-in-language-research-13" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<ul>
<li>Single sentence reading</li>
<li>Comprehension questions as attention check</li>
</ul>

<img data-src="images/session_1/reading_trial-structure.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="eye-tracking-in-language-research-14" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<div class="r-stack">
<video id="video_shortcode_videojs_video3" height="300" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="images/session_1/reading_video.mov"></video>
</div>

<aside><div>
<p>Data from Mariia Baltais</p>
</div></aside></section>
<section id="eye-tracking-in-language-research-15" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>se metió a</strong> / <strong>rompió a</strong></p>
</div><div class="column" style="width:50%;">
<p><span class="fg" style="--col: #e64173">hablar</span></p>
</div>
</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/reading_resultsV.png" class="quarto-figure quarto-figure-center" height="350"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_1/reading_resultsINF.png" class="quarto-figure quarto-figure-center" height="350"></p>
</figure>
</div>
</div>
</div>
</div>

<aside><div>
<p>Data from Mariia Baltais</p>
</div></aside></section>
<section id="eye-tracking-in-language-research-16" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<p>Conclusions:</p>
<ul>
<li>Low productivity was not associated with prediction error cost, so processing was not hindered by any unmet expectations.</li>
<li>Low productivity affected reading (integration and interpretation) of the construction/sentence as a whole.</li>
<li>Psycholinguistic evidence for the reality of the productivity phenomenon.</li>
</ul>
</section></section>
<section>
<section id="wrap-up" class="title-slide slide level1 center">
<h1>Wrap up</h1>

</section>
<section id="wrap-up-1" class="slide level2">
<h2>Wrap up</h2>
<ul>
<li>Eye movements are a window to individuals’ minds because attention mediates them.</li>
<li>In the case of language, we take that people process what they are looking at, and for how long they look signifies how costly it is to process.</li>
<li>Eye-trackers work by locating the relative position of the pupil thanks to a camera.</li>
<li>You can investigate both spoken and written language, production and comprehension via the VWP and reading paradigms.</li>
</ul>
</section>
<section id="plan-for-tomorrow" class="slide level2">
<h2>Plan for tomorrow</h2>
<p>Visual World Paradigm</p>
<ul>
<li>Install PsychoPy</li>
<li>Download template script</li>
<li>Recommended reading:
<ul>
<li>Altmann &amp; Kamide (1999)</li>
</ul></li>
</ul>
<aside class="notes">
<p>change to OS</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="plan-for-this-week" class="slide level2">
<h2>Plan for this week</h2>
<ul>
<li>Think about a research question
<ul>
<li>If only questions regarding one paradigm come to mind, that’s alright.</li>
</ul></li>
<li>Decide on an appropriate paradigm</li>
<li>Maybe already materials? (pictures &amp; audios, or sentences…)</li>
</ul>
<p>You can come up with your own idea, or you can take an existing study (see some papers on our GitHub page). Otherwise, tomorrow you can just use the exercise materials we prepared for you if you don’t have time today, so don’t stress it :)</p>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<p>Altmann, G. T. M., &amp; Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of subsequent reference. <em>Cognition, 73</em>(3), 247–264. https://doi.org/10.1016/S0010-0277(99)00059-1</p>
<p>Conklin, K., Pellicer-Sánchez, A., &amp; Carrol, G. (2018). <em>Eye-tracking: A guide for applied linguistics research.</em> Cambridge University Press.</p>
<p>Ferreira, F., &amp; Yang, Z. (2019). The problem of comprehension in psycholinguistics. <em>Discourse Processes, 56</em>(7), 485–495.</p>
<p>Holmqvist, K., Nyström, M., Andersson, R., Dewhurst, R., Jarodzka, H., &amp; Van de Weijer, J. (2011). <em>Eye tracking: A comprehensive guide to methods and measures.</em> OUP Oxford.</p>
<p>Hutton, S. B. (2019). Eye tracking methodology. <em>Eye movement research: An introduction to its scientific foundations and applications</em>, 277–308.</p>
<p>Javal, E. (1879). Essai sur la physiologie de la lecture. <em>Annales d’oculistique, 82</em>, 242–253.</p>
<p>Just, M. A., &amp; Carpenter, P. A. (1980). A theory of reading: from eye fixations to comprehension. <em>Psychological review, 87</em>(4), 329.</p>
</section>
<section id="references-1" class="slide level2 smaller">
<h2>References</h2>
<p>King, J. P., Loy, J. E., &amp; Corley, M. (2018). Contextual effects on online pragmatic inferences of deception. <em>Discourse Processes, 55</em>(2), 123–135.</p>
<p>Magnuson, J. S. (2019). Fixations in the visual world paradigm: where, when, why?. <em>Journal of Cultural Cognitive Science, 3</em>(2), 113–139.</p>
<p>Papoutsaki, A., Sangkloy, P., Laskey, J., Daskalova, N., Huang, J., &amp; Hays, J. (2016). WebGazer : Scalable webcam eye tracking using user interactions. <em>International Joint Conference on Artificial Intelligence.</em></p>
<p>Pickering, M. J., Frisson, S., McElree, B., &amp; Traxler, M. J. (2004). Eye movements and semantic composition. <em>On-line study of sentence comprehension: Eyetracking, ERPs and beyond</em>, 33–50.</p>
<p>Rodriguez Ronderos, C., Münster, K., Guerra, E., Kreysa, H., Rodríguez, A., Kröger, J., Kluth, T., Burigo, M., Abashidze, D., Nunnemann, E., &amp; Knoeferle, P. (2018). Eye Tracking During Visually Situated Language Comprehension: Flexibility and Limitations in Uncovering Visual Context Effects. <em>Journal of Visualized Experiments: JoVE, 141</em>. https://doi.org/10.3791/57694</p>
<p>Trueswell, J. C. (2008). Using eye movements as a developmental measure within psycholinguistics. <em>Language acquisition and language disorders, 44</em>, 73.</p>
<p>Yarbus A. L. (1967). <em>Eye movements and vision.</em> New York: Plenum.</p>
<div class="quarto-auto-generated-content">
<p><img src="images/logo_mils.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Introduction to eye-tracking</p>
</div>
</div>
</section></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session_1_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session_1_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session_1_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session_1_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    <script>videojs(video_shortcode_videojs_video2);</script>
    <script>videojs(video_shortcode_videojs_video3);</script>
    

</body></html>