<!DOCTYPE html>
<html lang="en"><head>
<script src="session_2_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_2_files/libs/quarto-html/tabby.min.js"></script>
<script src="session_2_files/libs/quarto-html/popper.min.js"></script>
<script src="session_2_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session_2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_2_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session_2_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session_2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.538">

  <meta name="author" content="Badaya">
  <title>Eye-tracking - Session 2</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session_2_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session_2_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="session_2_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session_2_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session_2_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session_2_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session_2_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="session_2_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="session_2_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .center-xy {
    margin: 0;
    position: absolute;
    top: 40%;
    left: 40%;
    -ms-transform: translateY(-50%), translateX(-50%);
    transform: translateY(-50%), translateX(-50%);
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Eye-tracking - Session 2</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Badaya 
</div>
</div>
</div>

</section>
<section class="slide level2">

<p>How is everyone doing today? Any questions from yesterday?</p>
</section>
<section id="plan-of-today" class="slide level2">
<h2>Plan of today</h2>
<p>Visual World Paradigm</p>
<ol type="1">
<li>History</li>
<li>Visual World Paradigm</li>
<li>Confounds of the Visual World Paradigm</li>
<li>Uses</li>
<li>Pros &amp; cons</li>
</ol>
</section>
<section>
<section id="the-visual-world-paradigm" class="title-slide slide level1 center">
<h1>The Visual World Paradigm</h1>

</section>
<section id="the-visual-world-paradigm-1" class="slide level2">
<h2>The Visual World Paradigm</h2>
<p>The Visual World Paradigm (VWP) is an eye-tracking paradigm that commonly describes an experiment where <span class="fg" style="--col: #e64173">auditory and visual stimuli</span> are presented to a participant, with the goal of understanding how the latter influences the former around a scene.</p>
</section>
<section id="history" class="slide level2">
<h2>History</h2>
<p>“While on a safari in <span class="fg" style="--col: #e64173">Africa</span> […] I noticed a hungry <span class="fg" style="--col: #e64173">lion</span> slowly moving through the tall grass toward a herd of grazing <span class="fg" style="--col: #e64173">zebra</span>”.</p>
<ul>
<li>Cooper’s (1974) method became later on popularised by Tanenhaus et al.&nbsp;(1995).</li>
</ul>

<img data-src="images/session_2/cooper_vwp.jpg" class="r-stretch quarto-figure-center"><p class="caption">Cooper, 1974</p></section>
<section id="uses" class="slide level2">
<h2>Uses</h2>
<p>Different levels of language comprehension (Huettig et al., 2011)</p>
<ul>
<li>Phonological level (e.g., Allopenna et al., 1998)
<ul>
<li>Beetle versus beaker</li>
</ul></li>
<li>Lexical level (e.g., semantic prediction, Altmann &amp; Kamide, 1999)
<ul>
<li>Eat versus move</li>
</ul></li>
<li>Syntactic level (e.g., Knoeferle et al., 2005)
<ul>
<li>Disambiguation of thematic roles.</li>
</ul></li>
<li>Discourse level (e.g., van Bergen &amp; Bosker, 2018)
<ul>
<li>Actually versus Indeed</li>
</ul></li>
<li>Pragmatic level (e.g., Grodner et al., 2010)
<ul>
<li>Adjective informativeness depends on speaker’s reliability.</li>
</ul></li>
</ul>
</section>
<section id="uses-1" class="slide level2">
<h2>Uses</h2>
<ul>
<li>Dialogue (e.g., Brown-Schmidt &amp; Tanenhaus, 2008)
<ul>
<li>Common ground establishment.</li>
</ul></li>
<li>Paralinguistic cues (e.g., Arnold et al., 2004)
<ul>
<li>New versus given information following a disfluency.</li>
</ul></li>
<li>Linguistic relativity (e.g., Papafragou et al., 2008).
<ul>
<li>Fixation preference following encoding of motion.</li>
</ul></li>
</ul>
<p>And many more (e.g., bilingualism, semantics/syntax interface,…)</p>
</section>
<section id="uses-2" class="slide level2">
<h2>Uses</h2>
<p>Different populations</p>
<ul>
<li>Children</li>
<li>Aphasic patients (e.g., Mirman et al., 2011)</li>
<li>Non-native listeners (e.g., Ito et al., 2018)</li>
</ul>
</section>
<section id="what-are-we-interested-in" class="slide level2">
<h2>What are we interested in?</h2>
<p>(Mostly) Fixations and saccades</p>
<ul>
<li>When &amp; where</li>
<li>Operationalisation
<ul>
<li>Fixation counts, proportion on ROIs,…</li>
</ul></li>
<li>100-200 ms to launch a saccade (Matin et al., 1993)</li>
</ul>
<aside class="notes">
<p>The former much more common than the latter. Because we launch a saccade when we want to fixate on something, we take saccade latency as a measure too. When you consider fixations, you need to acount for the time it takes people to launch a saccade.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-are-we-interested-in-1" class="slide level2">
<h2>What are we interested in?</h2>
<p>(Newer) Pupil size</p>
<ul>
<li>Ease of processing</li>
<li>Challenges for experimental design</li>
</ul>
</section>
<section id="linking-hypothesis" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Linking hypothesis: Link response to a hypothesized process.</p>
<ul>
<li>Eye movements reflect lexical access.
<ul>
<li>(In reading) Time spent looking at a word == how long it takes to process it.</li>
</ul></li>
</ul>
<p>Visual World Parading = <span class="fg" style="--col: #e64173">Linguistic + non-linguistic information.</span></p>
<ul>
<li>What guides what? Whether and how do they interact?</li>
<li><strong>How can we be sure that our results eye-movements were linguistically mediated?</strong></li>
</ul>
</section>
<section id="linking-hypothesis-1" class="slide level2">
<h2>Linking hypothesis</h2>
<p>“Default”: Increases in fixation == increases in activation.</p>
<ul>
<li>Automatised routines; recognising a name triggers these routines, in turn, these routines trigger a saccade and thus fixations on objects (Tanenhaus et al., 2000).</li>
</ul>
</section>
<section id="linking-hypothesis-2" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Not necessarily (see Magnuson, 2019):</p>
<ul>
<li>Language processing guiding vision (e.g., Allopenna et al., 1998).</li>
<li>Vision also affecting language processing (e.g.&nbsp;Huettig &amp; McQueen, 2007).</li>
<li>Listeners getting ahead of speech (e.g., Altmann &amp; Kamide, 1999).</li>
</ul>
<aside class="notes">
<p>A much better discussion of this can be found in Magnuson (2019). I am only mentioning this because these linking hypothesis highlight different aspects we need to consider about the VWP and how we interpret fixations and saccades (e.g., whether they are language-mediated). Magnuson discusses models for those, as well as caveats for each.</p>
<p>In fact, most papers will not discuss what hypothesis they are relying on (and usually, the assumption is the default one, by Tanenhaus et al., 2010)</p>
<p>The first one has its origins in experiments showing that fixations are usualyl mediated by language-dependent factors.</p>
<p>The second one suggests that visual information can partially mediate fixations. This is idea is much more developed in Huettig et al.&nbsp;(2011). The idea is that representations are activated prior to speech, you can think of it as a way of ‘priming’. They also discuss that by necessity, vision and language need to be connected as we need to move our eyes towards something, so we need to link the linguistic information to spatial information (e.g., where is said object). Visual display activates visual features which in turn activate phonological and semantic features associated to those. Similarly, speech activates these. The overlap leads to even more activation. Note that this kind of entails that you are constantly activating labels when you are looking at the world.</p>
<p>The mental world goes back a bit to the first one, but instead of purely describing phonological input, it also considers the possibility that individuals are building a mental world (kind of like a mental representation of what is being said). They then explore scenes with this mental representation. This was developed by Altmann and Kamide, so it kind of makes sense they advocate for something like this: They need to accomodate for prediction.</p>
<p>The final one has components of the other three. Visual and linguistic information interact, what leads what is following Huettig and McQueen e.g., timing and task. One components is deep interaction: when one type of infor alters the other, for example, no showing examples of garden-path when nothing in the display does not support such interpretation (visual information gets ahead of whatever linguistic information may come up, and alters it, as opposed to biasing it).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-3" class="slide level2">
<h2>Linking hypothesis</h2>
<p>But why do we care?</p>
<ul>
<li>Role of instructions and presentation
<ul>
<li>Click on the red [target]</li>
</ul></li>
<li>Role of preview window
<ul>
<li>Activation of phonological and semantic information</li>
</ul></li>
<li>Bottom-up versus top-down effects
<ul>
<li>Passive versus active processes (e.g., Pickering &amp; Gambi’s (2018) prediction-by-production)</li>
</ul></li>
<li>Level of interest
<ul>
<li>Lexical access v Sentence comprehension versus Discourse comprehension</li>
</ul></li>
</ul>
<aside class="notes">
<p>Bottom-up and top-down has to do with mental world</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="basics-of-the-visual-world-paradigm" class="slide level2">
<h2>Basics of the Visual World Paradigm</h2>
<ol type="1">
<li>Elements</li>
<li>Structure</li>
<li>Confounds</li>
<li>Variations</li>
</ol>
</section>
<section id="visual-world-paradigm" class="slide level2">
<h2>Visual World Paradigm</h2>
<p>Working example: Altmann and Kamide (1999)</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Anticipation (~prediction) of lexical items given verb semantics.</p>
<ul>
<li>DV: Fixations to objects on screen</li>
<li>IV: Verb semantics: <span class="fg" style="--col: #FF7F50">constraining</span> versus <span class="fg" style="--col: #40E0D0">unconstraining</span>
<ul>
<li>“The boy will <span class="fg" style="--col: #FF7F50">eat</span> the cake” versus “The boy will <span class="fg" style="--col: #40E0D0">move</span> the cake”</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_2/altmannkamide1999.png"></p>
</div>
</div>
</section>
<section id="elements" class="slide level2">
<h2>Elements</h2>
<p>What are the elements in Altmann and Kamide that you can identify as characteristic of the Visual World Paradigm?</p>
<div class="fragment">
<ul>
<li>Auditory stimuli</li>
<li>Visual stimuli</li>
<li>Task</li>
</ul>
</div>
</section>
<section id="auditory-stimuli" class="slide level2">
<h2>Auditory stimuli</h2>
<p>Example of Altmann and Kamide?</p>
<div class="fragment">
<ul>
<li>The boy will <span class="fg" style="--col: #FF7F50">eat</span> the cake (constraining).</li>
<li>The boy will <span class="fg" style="--col: #40E0D0">move</span> the cake (unconstraining).</li>
</ul>
<p>Control of stimuli:</p>
<ul>
<li>Context matched.</li>
<li>Counterbalancing.</li>
<li>Comparison between levels.</li>
<li>Inter alia.</li>
</ul>
</div>
</section>
<section id="auditory-stimuli-1" class="slide level2">
<h2>Auditory stimuli</h2>
<p>Participants <em>listen</em> to these sentences.</p>
<ul>
<li>Need to <em>record</em> our stimuli.</li>
</ul>
<p>Tips:</p>
<ul>
<li>All recordings in one session.</li>
<li>Talk to the person recording (to avoid monotonous voice).</li>
<li>Sound-isolating recording studio.</li>
<li>Several recordings of the same sentence.</li>
<li>Consider cross-splicing.
<ul>
<li>Editing tools: Audacity, Praat.</li>
</ul></li>
<li>Control speaker’s traits.</li>
</ul>
</section>
<section id="auditory-stimuli-2" class="slide level2">
<h2>Auditory stimuli</h2>
<p>Eye-movements in the VWP are <span class="fg" style="--col: #e64173">time-locked</span>.</p>
<ul>
<li>Time window of analysis around a critical part of speech e.g., critical word.
<ul>
<li>Triggers (coding)</li>
</ul></li>
<li>Different time windows to explore <span class="fg" style="--col: #e64173">different processes</span> within speech comprehension.
<ul>
<li>Integration versus Prediction (analysis)</li>
</ul></li>
</ul>
</section>
<section id="auditory-stimuli-3" class="slide level2">
<h2>Auditory stimuli</h2>
<p>Can you think of other elements of audio that can serve as a time anchor?</p>
<div class="fragment">
<ul>
<li>Prosodic contour.</li>
<li>Case marking.</li>
<li>Speech errors.</li>
</ul>
</div>
</section>
<section id="auditory-stimuli-4" class="slide level2">
<h2>Auditory stimuli</h2>
<p>Can you think of other properties of audio that can be manipulated?</p>
<div class="fragment">
<ul>
<li>Speech rate.</li>
<li>Traits of the speaker.</li>
<li>Prosody.</li>
<li>Noise.</li>
<li>Inter alia.</li>
</ul>
</div>
</section>
<section id="visual-stimuli" class="slide level2">
<h2>Visual stimuli</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Elements in Altmann and Kamide?</p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_2/altmannkamide1999.png"></p>
</div>
</div>
</section>
<section id="visual-stimuli-1" class="slide level2">
<h2>Visual stimuli</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Elements in Altmann and Kamide?</p>
<p>4: Target (cake) and <em>distractors</em> (ball, train, car).</p>
<ul>
<li>And the boy.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_2/altmannkamide1999.png"></p>
</div>
</div>
</section>
<section id="visual-stimuli-2" class="slide level2">
<h2>Visual stimuli</h2>
<p>Eye-movements in the VWP are space-locked: Areas of Interest</p>
<ol type="1">
<li>How many items can there be on display?</li>
<li>How can items be displayed?</li>
<li>How can we manipulate the items?</li>
<li>How can we select images?</li>
</ol>
</section>
<section id="visual-stimuli-3" class="slide level2">
<h2>Visual stimuli</h2>
<ol type="1">
<li>How many items can there be on display?</li>
</ol>
<ul>
<li><span class="fg" style="--col: #e64173">[2, 5]</span></li>
<li>Working memory (see Huettig et al., 2011)</li>
</ul>
</section>
<section id="visual-stimuli-4" class="slide level2">
<h2>Visual stimuli</h2>
<ol start="2" type="1">
<li>How can items be displayed?</li>
</ol>
<p>Properties of the display allow for exploration of different processes in speech comprehension.</p>
<ul>
<li>Semirealistic scenes: <span class="fg" style="--col: #e64173">World knowledge</span>.</li>
<li>Arrays: <span class="fg" style="--col: #e64173">Conceptual and lexical knowledge</span> associated with individual words.</li>
<li>Printed words: <span class="fg" style="--col: #e64173">Phonological information and orthographic</span> processing, comprehension of abstract words.</li>
</ul>
<aside class="notes">
<p>Mention the plausibility bit!!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="visual-stimuli-5" class="slide level2">
<h2>Visual stimuli</h2>
<ol start="2" type="1">
<li>How can items be displayed?</li>
</ol>
<div class="{columns}">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_2/altmannkamide1999.png"></p>
<figcaption>Altmann &amp; Kamide, 1999</figcaption>
</figure>
</div>
</div>
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_2/array_example.jpg"></p>
<figcaption>Huettig &amp; McQueen, 2007</figcaption>
</figure>
</div>
</div>
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_2/ortho_example.jpg"></p>
<figcaption>Huettig &amp; McQueen, 2007</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="visual-stimuli-6" class="slide level2">
<h2>Visual stimuli</h2>
<ol start="3" type="1">
<li>How can we manipulate the items?</li>
</ol>
<p><span class="fg" style="--col: #e64173">Relationship between target and distractor(s)</span>.</p>
<ul>
<li>Semantic distance, phonological distance, etc. - even shape!</li>
<li>Target (critical word) might not even be present.</li>
</ul>
</section>
<section id="visual-stimuli-7" class="slide level2">
<h2>Visual stimuli</h2>
<ol start="3" type="1">
<li>How can we manipulate the items?</li>
</ol>

<img data-src="images/session_2/phonological_manipulation.JPG" class="r-stretch quarto-figure-center"><p class="caption">Allopenna et al., 1996</p></section>
<section id="visual-stimuli-8" class="slide level2">
<h2>Visual stimuli</h2>
<ol start="4" type="1">
<li>How can we select images?</li>
</ol>
<p>Databases <em>or</em> create your own.</p>
<ul>
<li>Later case: Need for validation cf.&nbsp;confounds.
<ul>
<li>Name agreement.</li>
</ul></li>
</ul>
<p>Tips:</p>
<ul>
<li>Control for visual salience.</li>
<li>Control for size (coding).</li>
<li>Familiarisation phase.</li>
<li>If also pupillometry -&gt; stimuli luminance.</li>
</ul>
</section>
<section id="task" class="slide level2">
<h2>Task</h2>
<p>As a function of your research question.</p>
<ul>
<li>Direct action</li>
<li>Look and listen</li>
</ul>
<p>But also;</p>
<ul>
<li>Perform a concurrent task? (impair WM)</li>
<li>Interpretation of speech?</li>
</ul>
<p>Remember: the active viewer (Yarbus).</p>
</section>
<section id="structure" class="slide level2">
<h2>Structure</h2>
<p><strong>Before the experiment begins</strong></p>
<ul>
<li>Calibration and validation.
<ul>
<li>Number of elements for interest areas.</li>
<li>Size of interest areas.</li>
<li>Horizontal and vertical areas.</li>
</ul></li>
<li>Decide sample rate.</li>
</ul>
</section>
<section id="structure-1" class="slide level2">
<h2>Structure</h2>

<img data-src="images/session_2/vwp_trialsequence_example.jpg" class="r-stretch"></section>
<section id="drift-correction" class="slide level2">
<h2>Drift correction</h2>
<ul>
<li>Ensure accuracy.</li>
<li>Where?
<ul>
<li><span class="fg" style="--col: #e64173">Middle</span> (no bias for an image beforehand).</li>
</ul></li>
<li>When?
<ul>
<li>Beginning of every trial/block.</li>
</ul></li>
</ul>
</section>
<section id="preview-window" class="slide level2">
<h2>Preview window</h2>
<ul>
<li>Very specific to the VWP</li>
<li>Presentation of visual stimuli without auditory stimuli, so that participants can inspect the visual scene.
<ul>
<li>Remember the linking hypothesis!</li>
</ul></li>
</ul>
<p><span class="fg" style="--col: #e64173">Pros and cons</span> of preview window (e.g., Huettig &amp; McQueen, 2007).</p>
<ul>
<li>Eye movements towards objects as a function of preview window.</li>
<li>Pre-activation of labels.</li>
<li>Contestable?</li>
</ul>
</section>
<section id="preview-window-1" class="slide level2">
<h2>Preview window</h2>
<p>Length?</p>
<ul>
<li><span class="fg" style="--col: #e64173">Previous research</span> e.g., 2000 ms, 1000 ms from target onset, etc.
<ul>
<li>Level of interest e.g., phonological activation versus semantic activation.</li>
</ul></li>
</ul>
</section>
<section id="audio-presentation" class="slide level2">
<h2>Audio presentation</h2>
<ul>
<li>Send triggers for audio.
<ul>
<li>Give enough time for a measure to occur.</li>
<li>Altmann and Kamide: Trigger -&gt; verb onset.</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="end-of-trial" class="title-slide slide level1 center">
<h1>End of trial</h1>
<ul>
<li>Task?</li>
<li>If no task:
<ul>
<li>Give enough time for processes to fully unfold.</li>
</ul></li>
</ul>
</section>
<section id="exercise-rq-measures-of-interest" class="slide level2">
<h2>Exercise: RQ &amp; measures of interest</h2>
<ul>
<li>Go [here] and type your idea!</li>
</ul>
<p>What would you be interested in? How would the design look like?</p>
</section>
<section id="analysis" class="slide level2">
<h2>Analysis</h2>
<p>DV = saccade latency, proportion of fixations, empirical logits, yes/no fixations…</p>
<p>IV = your experimental variables <strong>and time</strong>.</p>
<ul>
<li>Time window of analysis.
<ul>
<li>Marked by your triggers (e.g., sentence onset, target onset, target offset…)</li>
<li>Research question.</li>
</ul></li>
</ul>
</section>
<section id="analysis-1" class="slide level2">
<h2>Analysis</h2>
<p>Interest: Prediction of a noun following verb semantics, what is more interesting for you? TW from sentence onset, from verb onset, or from noun onset? What reflects prediction?</p>
<div class="fragment">
<p>Verb onset - noun onset (+200 for saccades)</p>
<ul>
<li>Reference resolution, word recognition, …</li>
</ul>
</div>
</section>
<section id="analysis-2" class="slide level2">
<h2>Analysis</h2>
<p>Collapse all samples before and after.</p>
<ul>
<li>Are there more fixations on the cake when is preceded by <span class="fg" style="--col: #FF7F50">eat</span> compared to <span class="fg" style="--col: #40E0D0">move</span>?</li>
</ul>
</section>
<section id="analysis-3" class="slide level2">
<h2>Analysis</h2>
<p>Time binning.</p>
<ul>
<li>Related to sampling frequency.</li>
<li>Group of samples.</li>
</ul>
<p>With 500 Hz, you have 500 samples per second, one every 2 ms.</p>
<ul>
<li>You can make bins of 10 ms (contains 5 samples), 20 ms (contains 10 samples)…</li>
<li>You cannot make bins of 15 ms.</li>
</ul>
<aside class="notes">
<p>You hardly explore second by second, but instead, aggregate samples over X amount of milliseconds. We call this time binning analysis. This is the first kind of data pre-processing, of course you can later see how people then analysis into groups of a 100 (e.g., any significance in the first 100, then the next, and so on).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="analysis-4" class="slide level2">
<h2>Analysis</h2>
<p>More on Friday</p>
<p>Divide in approaches (see Ito &amp; Knoeferle, 2023):</p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><strong>Linear</strong></p>
</div>
<ul>
<li>(G)LMM/ANOVAs by subjects &amp; items/t-tests</li>
</ul>
<p>There is an increase or a decrease over time. Cannot tell <em>when</em> this increase/decrease happens, but (G)LMMs are the most common analysis.</p>
<ul>
<li>but cf.&nbsp;models per time interval.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><strong>Non-linear</strong></p>
</div>
<ul>
<li>Growth Curve Analysis, cluster analysis, Generalised Additive Mixed Models, BOLTS…</li>
</ul>
<p>How this increase/decrease over time occurs. Some can assess when the differences in conditions become statistically significant.</p>
</div>
</div>
</section>
<section id="results" class="slide level2">
<h2>Results</h2>
<p>Commonly, plots.</p>
<ul>
<li>x: time</li>
<li>y: proportion/probability of fixations.</li>
<li>line style: condition</li>
</ul>
<div class="r-stack">
<p><img data-src="images/session_2/altmannkamide_results.png"></p>
</div>
</section>
<section id="results-1" class="slide level2">
<h2>Results</h2>

<img data-src="images/session_1/vwp_data.JPG" class="r-stretch"></section>
<section id="exercise-huettig-mcqueen-2007" class="slide level2">
<h2>Exercise: Huettig &amp; McQueen (2007)</h2>
<p>Any questions on the paper?</p>
<div class="fragment">
<ol type="1">
<li>What is the visual display? Why?</li>
<li>How many samples in 20 ms bins?</li>
<li>What is the time window of analysis? Why?</li>
<li>What is the difference between Exp 1 and Exp 2?</li>
</ol>
</div>
</section>
<section id="exercise-huettig-mcqueen-2007-1" class="slide level2">
<h2>Exercise: Huettig &amp; McQueen (2007)</h2>
<ol type="1">
<li>What is the visual display? Why?</li>
</ol>
<div class="fragment">
<p>4 images representing 3 related and 1 unrelated items to the target.</p>
<ul>
<li>Phonology, shape, semantics.</li>
<li>No target displayed to maximise fixations to distractors.</li>
<li>Distractors chosen to explore whether fixations are driven by phonology, visual or semantic information.</li>
</ul>
</div>
</section>
<section id="exercise-huettig-mcqueen-2007-2" class="slide level2">
<h2>Exercise: Huettig &amp; McQueen (2007)</h2>
<ol start="2" type="1">
<li>How many samples in 20 ms bins?</li>
</ol>
<div class="fragment">
<p>Recording at 250 Hz, 250 samples per second.</p>
<ul>
<li>5 samples.</li>
</ul>
</div>
</section>
<section id="exercise-huettig-mcqueen-2007-3" class="slide level2">
<h2>Exercise: Huettig &amp; McQueen (2007)</h2>
<ol start="3" type="1">
<li>What is the time window of analysis? Why?</li>
</ol>
<div class="fragment">
<p>From noun onset +200 (to account for a saccade) - 1000 ms.</p>
</div>
</section>
<section id="exercise-huettig-mcqueen-2007-4" class="slide level2">
<h2>Exercise: Huettig &amp; McQueen (2007)</h2>
<ol start="4" type="1">
<li>What is the difference between Exp 1 and Exp 2?</li>
</ol>
<div class="fragment">
<p>Preview window.</p>
</div>
</section>
<section id="confounds" class="slide level2">
<h2>Confounds</h2>
<p><br> <br> <br></p>
<div class="r-stack">
<p>Given your knowledge in linguistics &amp; what we’ve discussed, what should we keep in mind when creating a VWP experiment?</p>
</div>
</section>
<section id="confounds-1" class="slide level2">
<h2>Confounds</h2>
<p>Image presentation.</p>
<ul>
<li>Salience</li>
</ul>
<div class="r-stack">
<p><img data-src="images/session_2/train.png"> <img data-src="images/session_2/boat_pic.jpg"></p>
</div>
</section>
<section id="confounds-2" class="slide level2">
<h2>Confounds</h2>
<p><br> <br></p>
<p>Image presentation</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Name agreement
<ul>
<li>Population</li>
<li>Clarity</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="images/session_2/turtle.png"></p>
</div>
</div>
</div>
</section>
<section id="confounds-3" class="slide level2">
<h2>Confounds</h2>
<p>Image presentation</p>
<ul>
<li>Size &amp; quality</li>
<li>Counterbalance position</li>
<li>Luminance (! pupillometry)</li>
</ul>
</section>
<section id="confounds-4" class="slide level2">
<h2>Confounds</h2>
<p>Image relationship</p>
<ul>
<li>Semantic distance</li>
<li>Phonological overlap</li>
<li>Lexical properties e.g., frequency.</li>
</ul>
</section>
<section id="confounds-5" class="slide level2">
<h2>Confounds</h2>
<p>Audio properties</p>
<ul>
<li>Same/different voices
<ul>
<li>Uncanny valley</li>
</ul></li>
<li>Phonetic cues (e.g., co-articulation)
<ul>
<li>Cross-splicing audios</li>
</ul></li>
<li>Volume</li>
<li>Prosody</li>
<li>Accent</li>
</ul>
</section>
<section id="confounds-6" class="slide level2">
<h2>Confounds</h2>
<p>Audio properties</p>
<ul>
<li>Speech rate</li>
</ul>

<img data-src="images/session_2/speechrate_vwp.png" class="r-stretch"></section>
<section id="variations" class="slide level2">
<h2>Variations</h2>
<p>Preferential look paradigm (~ VWP for infants)</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Different tracker</li>
<li>Stimuli presentation
<ul>
<li>What is discourse-old versus difficult.</li>
</ul></li>
<li>Messier data, fewer trials</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/session_2/preferential_kids.png"></p>
</div>
</div>
</section>
<section id="variations-1" class="slide level2">
<h2>Variations</h2>
<p>Language production.</p>
<ul>
<li><p>The link between planning and eye movements is less direct.</p></li>
<li><p>Eye movements ~ labour division of speech production (e.g., Levelt’s model)</p>
<ul>
<li>Conceptual</li>
<li>Formulation</li>
<li>Articulation</li>
</ul></li>
<li><p>How does looking at an object relate to production stages?</p>
<ul>
<li>Do people fixate for longer on objects harder to retrieve?</li>
<li>Do people start producing speech prior to fixating on the object to name?</li>
</ul></li>
</ul>
</section>
<section id="variations-2" class="slide level2">
<h2>Variations</h2>
<p>Language production.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Only images.
<ul>
<li><span class="fg" style="--col: #e64173">Time-locked to voice onset/offset</span>.</li>
<li>Manipulations: Image degradation, word frequency, etc.
<ul>
<li>Relation to the level of interest.</li>
</ul></li>
</ul></li>
<li>Network task</li>
</ul>
</div><div class="column" style="width:50%;">
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_3/example_pistono.JPG"></p>
<figcaption>Pistono &amp; Hartsuiker, 2023</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="variations-3" class="slide level2">
<h2>Variations</h2>
<ul>
<li>Virtual reality e.g., Eichert et al., 2018</li>
<li>Mouse-tracking e.g., King et al., 2019</li>
<li>Web-cam tracking e.g., Slim &amp; Hartsuiker, 2023
<ul>
<li>ROIs size</li>
<li>Display size</li>
</ul></li>
</ul>
</section>
<section id="pros-cons" class="slide level2">
<h2>Pros &amp; cons</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><strong>Pros</strong></p>
</div>
<ul>
<li>Ecological validity.</li>
<li>Relatively easy.</li>
<li>Accessible e.g., no meta-linguistic judgements.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><strong>Cons</strong></p>
</div>
<ul>
<li>Ecological validity.</li>
<li>Confounding variables.</li>
<li>Linking hypothesis.</li>
</ul>
</div>
</div>
</section></section>
<section>
<section id="building-a-vwp-experiment" class="title-slide slide level1 center">
<h1>Building a VWP experiment</h1>

</section>
<section id="section" class="slide level2">
<h2></h2>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<p>Allopenna, P. D., Magnuson, J. S., &amp; Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. <em>Journal of memory and language, 38</em>(4), 419-439.</p>
<p>Altmann, G. T., &amp; Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of subsequent reference. <em>Cognition, 73</em>(3), 247-264.</p>
<p>Arnold, J. E., Tanenhaus, M. K., Altmann, R. J., &amp; Fagnano, M. (2004). The old and thee, uh, new: Disfluency and reference resolution. <em>Psychological science, 15</em>(9), 578-582.</p>
<p>Brown‐Schmidt, S., &amp; Tanenhaus, M. K. (2008). Real‐time investigation of referential domains in unscripted conversation: A targeted language game approach. <em>Cognitive science, 32</em>(4), 643-684.</p>
<p>Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: a new methodology for the real-time investigation of speech perception, memory, and language processing. Cognitive psychology.</p>
<p>Eichert, N., Peeters, D., &amp; Hagoort, P. (2018). Language-driven anticipatory eye movements in virtual reality. <em>Behavior research methods, 50</em>, 1102-1115.</p>
</section>
<section id="references-1" class="slide level2 smaller">
<h2>References</h2>
<p>Grodner, D. J., Klein, N. M., Carbary, K. M., &amp; Tanenhaus, M. K. (2010). “Some,” and possibly all, scalar inferences are not delayed: Evidence for immediate pragmatic enrichment. <em>Cognition, 116</em>(1), 42-55.</p>
<p>Huettig, F., &amp; McQueen, J. M. (2007). The tug of war between phonological, semantic and shape information in language-mediated visual search. <em>Journal of memory and language, 57</em>(4), 460-482.</p>
<p>Huettig, F., Olivers, C. N., &amp; Hartsuiker, R. J. (2011). Looking, language, and memory: Bridging research from the visual world and visual search paradigms. <em>Acta psychologica, 137</em>(2), 138-150.</p>
<p>Huettig, F., Rommers, J., &amp; Meyer, A. S. (2011). Using the visual world paradigm to study language processing: A review and critical evaluation. <em>Acta psychologica, 137</em>(2), 151-171.</p>
<p>Ito, A., &amp; Knoeferle, P. (2023). Analysing data from the psycholinguistic visual-world paradigm: Comparison of different analysis methods. <em>Behavior Research Methods, 55</em>(7), 3461-3493.</p>
<p>Ito, A., Pickering, M. J., &amp; Corley, M. (2018). Investigating the time-course of phonological prediction in native and non-native speakers of English: A visual world eye-tracking study. <em>Journal of Memory and Language, 98</em>, 1-11.</p>
</section>
<section id="references-2" class="slide level2 smaller">
<h2>References</h2>
<p>King, J. P., Loy, J. E., &amp; Corley, M. (2018). Contextual effects on online pragmatic inferences of deception. <em>Discourse Processes, 55</em>(2), 123-135.</p>
<p>Knoeferle, P., Crocker, M. W., Scheepers, C., &amp; Pickering, M. J. (2005). The influence of the immediate visual context on incremental thematic role-assignment: Evidence from eye-movements in depicted events. <em>Cognition, 95</em>(1), 95-127.</p>
<p>Magnuson, J. S. (2019). Fixations in the visual world paradigm: where, when, why?. <em>Journal of Cultural Cognitive Science, 3</em>(2), 113-139.</p>
<p>Matin, E., Shao, K. C., &amp; Boff, K. R. (1993). Saccadic overhead: Information-processing time with and without saccades. <em>Perception &amp; psychophysics, 53</em>, 372-380.</p>
<p>Mirman, D., Yee, E., Blumstein, S. E., &amp; Magnuson, J. S. (2011). Theories of spoken word recognition deficits in aphasia: Evidence from eye-tracking and computational modeling. <em>Brain and language, 117</em>(2), 53-68.</p>
<p>Papafragou, A., Hulbert, J., &amp; Trueswell, J. (2008). Does language guide event perception? Evidence from eye movements. <em>Cognition, 108</em>(1), 155-184.</p>
<p>Pickering, M. J., &amp; Gambi, C. (2018). Predicting while comprehending language: A theory and review. <em>Psychological bulletin, 144</em>(10), 1002.</p>
</section>
<section id="references-3" class="slide level2 smaller">
<h2>References</h2>
<p>Pistono, A., &amp; Hartsuiker, R. J. (2023). Can object identification difficulty be predicted based on disfluencies and eye-movements in connected speech?. <em>Plos one, 18</em>(3), e0281589.</p>
<p>Slim, M. S., &amp; Hartsuiker, R. J. (2023). Moving visual world experiments online? A web-based replication of Dijkgraaf, Hartsuiker, and Duyck (2017) using PCIbex and WebGazer. js. <em>Behavior Research Methods, 55</em>(7), 3786-3804.</p>
<p>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. <em>Science, 268</em>(5217), 1632-1634.</p>
<p>Tanenhaus, M. K., Magnuson, J. S., Dahan, D., &amp; Chambers, C. (2000). Eye movements and lexical access in spoken-language comprehension: Evaluating a linking hypothesis between fixations and linguistic processing. <em>Journal of Psycholinguistic Research,29</em>, 557–580.</p>
<p>Van Bergen, G., &amp; Bosker, H. R. (2018). Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad’indeed’and eigenlijk’actually’. <em>Journal of Memory and Language, 103</em>, 191-209.</p>
<div class="quarto-auto-generated-content">
<p><img src="images/logo_mils.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Session 2 - Visual World Paradigm</p>
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session_2_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session_2_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session_2_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session_2_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>