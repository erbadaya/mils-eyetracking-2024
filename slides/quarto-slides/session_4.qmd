---
title: "Data pre-processing"
author: "Badaya & Baltais"
format: 
   clean-revealjs:
    logo: images/logo_mils.png
    footer: "Data pre-processing"
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          top: 40%;
          left: 40%;
          -ms-transform: translateY(-50%), translateX(-50%);
          transform: translateY(-50%), translateX(-50%);
        }
        </style>
editor: visual
---

# Welcome back!

## Welcome back!

Any questions from yesterday?

## Overview of today

1.  Raw data
2.  Data processing steps
3.  Data Viewer
4.  Visual World Paradigm pipeline
5.  Reading pipeline
6.  Practice time

# Raw data

## Raw data

Project directory

![](images/session_4/projdir1.JPG){fig-align="center"}

## Raw data

Data directory (eye data & behavioural data)

![](images/session_4/projdir2.JPG){fig-align="center"}

:::notes
QUE: is this how the folder really looks? check! (with background images for reading)
:::

## Raw data

The eye-tracker is actually just recording gaze coordinates on the screen (i.e., gaze points) as well as timestamps in each interval (defined by the sampling rate).

It also includes:

-   Timestamps for stimuli on- and offsets (i.e., the triggers/messages we sent).
-   Timestamps for participants' responses.
-   What they responded.
-   Trial information.

## Raw data

ASCIII files

-   Sampling rate
-   x,y coordinates of gaze

![](images/session_4/asciii1.JPG){fig-align="center"}

:::notes
EDF in text format. DV applies algorithms to convert this to a more user-friendly format. Here: some info about the eye-tracker and coordinates for calibration points.
:::

## Raw data

ASCIII files

-   Sampling rate
-   x,y coordinates of gaze

![](images/session_4/asciii2.png){fig-align="center"}

:::notes
And this is experimental data. SFIX R - start of a fixation with a timestamp. First column = timestamp in ms. Can you guess the sampling rate? 500 Hz (every 2 ms). Columns 2 and 3: coordinates for the gaze position (X, Y).
:::

## Software

Licensed software

-   User-friendly, no need to code.
-   A function of your eye-tracker.
    -   Data Viewer - EyeLink (SR Research)
    -   Tobii Pro Studio/Lab - Tobii
    -   BeGaze - SMI

Open software

-   MATLAB, Python, R packages.
    -   gazeR, eyetrackingR

## Data Viewer

EDF files

::: columns
::: {.column width="50%"}
<br> <br>

-   Conversion of ASCIII files.
:::

::: {.column width="50%"}
![](images/session_4/8.png){fig-align="center"}

![](images/session_4/20.png){fig-align="center"}
:::
:::

## Data Viewer

EDF files

::: columns
::: {.column width="50%"}
<br> <br>

-   List of all participants and trials from each participant.
-   List of all events pertaining to each trial.
:::

::: {.column width="50%"}
![](images/session_4/34.png)
:::
:::

# Data processing

## Data processing

All steps:

-   Pre-process data
-   Export data
-   Visualization
-   Data wrangling
-   Analysis

## Data processing

Today: 

-   [Pre-process data]{.fg style="--col: #e64173"}
-   [Export data for analysis]{.fg style="--col: #e64173"}
-   Visualization
-   Data wrangling
-   Analysis

## Data pre-processing

1.  Import data into software
2.  Assess data
    -   Checks
3.  (Automatized) cleaning
4.  Prepare data for analysis & export
    -   Time windows
    -   Areas of Interest

## Import data

Different software = different data files.

-   Data Viewer: .edf files
-   R packages: ASCIII files, reports from DV.

Different software = additional steps.

-   R packages: binning, assigning AIs...

## Assess data

Two aspects:

-   Pre-processing due to eye movements.
    -   Short/long fixations, track loss.
-   Pre-processing due to participants.
    -   Missed trials, participation criteria.

## Assess data

\~ Sanity check (!= visualization)

-   Does everything make sense?
-   Is there anything missing?
    -   Trial-by-trial visual check

:::notes
Espe: have them think of AIs, missing timestamps, missing participants/trials…
:::

## Assess data

Unusable participants

-   Rare?
-   File corrupted?
    -   Lab log
    -   Example: A participant blinked too much during the experiment, too many unusable trials.
    -   If large data loss, it may be easier to exclude the participant.

## Assess data

Unusable trials

-   More common
-   Poor calibration (cf. drift correction)
    -   Umbrella term: "track loss" (could mean anything: participant sneezed, distraction, too fast)
-   Field-specific
-   Lab log!

## Assess data

Common sense + **lab log**

-   Remove participants (lab log? other exclusion criteria?)
-   Items to discard
    -   Misunderstood sentences?
    -   Unforeseen confounds?

![](images/session_4/ex_loglab.JPG){fig-align="center"}

## Cleaning

-   Keep a **log of all changes** (trial & participant exclusion)
    -   Especially if you did not save the viewing session.
    -   Write up e.g., % trials removed.

Try to think ahead (pre-registration!) and report all your steps and motivation.

![](images/session_4/reading_cleaning_logbook.png){fig-align="center"}

## Exporting

Prepare data to be analysed elsewhere (e.g., R, SPSS).

Data Viewer has different reports.

![](images/session_4/9.png)

# Data Viewer

## Interface

![](images/session_4/viewing_dv1.JPG)

## Interface

![](images/session_4/viewing_dv2.jpg){height="500px"}

:::aside
Grouping of trials can be changed (see "Edit").
:::

## Interface

Time window of analysis

![](images/session_4/25.png){fig-align="center"}

:::aside
Here: named as "sentence" (from stimulus presentation to button press).
:::

## Interface

::: columns
::: {.column width="50%"}
<br>

Make sure your AIs are displayed when setting the TW.

-   Preferences \> Data Filters \> Show AIs
:::

::: {.column width="50%"}
![](images/session_4/29.png){fig-align="center"}
:::
:::

## Importing data

::: columns
::: {.column width="50%"}
<br>

.edf (Eyelink Data File) --\> .evs (Eyelink Viewing Session)

-   /results in experiment folder (project directory)
-   One .edf per participant
-   Always copy and save the entire experiment folder with subfolders (not just the results)!
:::

::: {.column width="50%"}
![](images/session_4/33.png)

![](images/session_4/32.png)
:::
:::

:::notes
QUE: check here about the folder and subfolders
:::

## Importing data

**Tip** : After importing, save the session (e.g. "data-uncleaned") and then save it again in a separate copy where you'll perform cleaning.

Note: **You can't go back with DataViewer**

-   Save the viewing session regularly.

![](images/session_4/31.png){fig-align="center"}

# Visual World Paradigm pipeline

## Interest Period: VWP

::: columns
::: {.column width="50%"}
<br>

Make sure your AIs are displayed when setting the TW.

-   Preferences \> Data Filters \> Show AIs
:::

::: {.column width="50%"}
![](images/session_4/29.png){fig-align="center"}
:::
:::

## Interest Period: VWP

::: columns
::: {.column width="50%"}
<br> <br>

Triggers, e.g., target_onset - target_offset

-   Strict matching unchecked
    -   What if there is a time-out?
-   Several TWs
:::

::: {.column width="50%"}
![](images/session_4/27.png)
:::
:::

:::notes
QUE: time-out? several TWs?
What about adding 200ms from the onset or smth?
Is it correct to set TW before visual check?
:::

## Check data: VWP

Fixations etc. across the screen (both in defined AIs and elsewhere).

-   What do we look for?
    -   \~ Subjective decisions, e.g., not looking \~ not processing speech
    -   Individual differences, e.g., different strategies (L1 v L2)
-   Inspect trials with unusual behaviour for that participant (e.g., fewer fixations than average)
-   Check AIs and triggers
    -   Did we code the AIs name?
    -   Did we send triggers?

:::notes
Espe:

The idea here is that in VWP things are harder to clean because we don’t have much to clean, e.g., if a fixation is short, or there are fewer fixations, it can be because of our own manipulation. Most of the time, we deal with these issues when we are exporting data. Note that participants can fixate not only in our AIs, but also elsewhere in the screen and outside of the screen.

For us, deciding how to clean the data is a bit harder, because a participant not looking at an AI does not necessarily not mean that they are not processing speech. There are also individual differences in eye-movements: Eye movements are unconscious, but also strategic: Some people might decide to “not explore” and just perform an action once they hear the word (which is also why how we deliver instructions in VWP is crucial, cause we can also affect their own behaviours), likewise, some populations tend to fixate on ROIs less than others (bilinguals tends to ‘wait’). The only rule that we might thus have is if there is a trial where there are comparatively fewer fixations than on other trials.
:::

## Automatized cleaning: VWP

What are we thinking of?

-   Track loss: When there is no data of eye recorded (e.g., blinks, participant moves, low validity of the sample)
    -   In DV, marked as blinks -\> no direct report.

## Automatized cleaning: VWP

Track loss

-   Trial report: AVERAGE_BLINK_DURATION, BLINK_DURATION, IP_DURATION
-   Proportion of track loss computed as: (AVERAGE_BLINK_DURATION \* BLINK_COUNT) / IP_DURATION
    -   But this will also include blinks! So not ideal.
-   Not common to encounter this step in reports.
    -   Kids vs. adults

:::notes
QUE: kids blink more?
:::

## Automatized cleaning: VWP

What are we thinking of?

::: columns
::: {.column width="50%"}
-   (Short and too long) fixations and blinks.
    -   Merge fixations (\~inspection)
    -   Add blinks and fixations to the previous one.
        -   A blink can interrupt a fixation and make it look like two.
:::

::: {.column width="50%"}
![](images/session_4/dv_filter.JPG){height="500px"}
:::
:::

:::notes
QUE: time-weighted, stop merging? what is recommended?
:::

## Automatized cleaning: VWP

Remaining automatized cleaning done **together with exporting data**.

-   Only samples on AIs, or also including on-screen but not on AIs?
    -   Trade-off.
    -   Affects how *proportions* are calculated.
    -   But counts are still there.

Familiarity with previous literature is **key**.

:::notes
QUE: is this specific for the binning report?
:::

## Automatized cleaning: VWP

What does each mean?

-   Across All Samples: Number of samples in each IA/Total number of samples per bin
-   Across All On-Screen Samples (both defined and undefined): Number of samples in each IA/Sum of samples where fixations landed on IAs and Null interest area[^3].
-   Across All Samples Assigned to Predefined Interest Areas Only: Number of samples in each IA/Sum of samples where fixations landed on IAs.

[^3]: Areas where we don't have IAs defined (e.g., the center of the screen).

## Automatized cleaning: VWP

What does this really mean?

::: columns
::: {.column width="50%"}
Assuming that we have 200 samples in a bin:

-   But 15 are blinks, 30 off-screen events, and 5 are data excluded.

-   There are 50 samples in uncoded areas of interest (i.e., IA 0)

-   There are 10 samples with data for IA 1.
:::

::: {.column width="50%"}
-   In the All Samples report:

    -   The proportion of samples related to IA 1 is 5% (10/200)

-   In the All On-Screen Samples:

    -   The proportion of samples related to IA 1 is 6.7% (10/(200 - 30 - 15 - 5))

-   In All Interest Area Samples:

    -   The proportion of samples related to IA 1 is 10% (10/(200 - 30 - 15 - 5 - 50))
:::
:::

## Automatized cleaning: VWP

-   All On Screen
    -   Overall increased attention.
-   All Interest Area Samples
    -   IA_0 = track loss.
    -   Trade-off.

:::notes
QUE: overall increased attention - but still analyzed per IA so why?
:::

## Automatized cleaning: VWP

IA_0

-   Participants not only look at the images, they also look at areas where we don't have IAs defined (e.g., the center of the screen).
    -   Interesting when exploring some RQs (i.e., do people prefer to not fixate on anything?)
    -   However! You don't know where exactly they were looking at (only that it was not defined!)

## Automatized cleaning: VWP

What do we count as samples for analysis?

-   All Samples in Fixations and Saccades
-   Exclude Samples during Saccades
-   Exclude Samples during Saccades and Exclude Bins that Contain Non-Fixation Samples

No straight-forward answer. Things to consider:

-   Fixations and Saccades:
-   Only fixations: Proportions more 'true' to fixation behaviour **but** also less 'true' to actual behaviour.
-   Only bins with fixations: Loss of data.

## Example data pre-processing: VWP

![Knoeferle & Crocker, 2006](images/session_4/knoerfele1.JPG){fig-align="center"}

## Example data pre-processing: VWP

![Arantzeta et al., 2017](images/session_4/basque.JPG){height="400px"}

## Example data pre-processing: VWP

![Apfelbaum et al., 2021](images/session_4/names.JPG){fig-align="center"}

## Exporting: VWP

Recap: Fixations *over a scene at a particular time*.

::: columns
::: {.column width="50%"}
-   Time window, defined \@ triggers.
    -   Trigger timestamp: zero, beginning of the timeline.
    -   Determines which gaze points are counted as part of the same fixation by DV algorithms.
-   Keep AIs in the report.
:::

::: {.column width="50%"}
![](images/session_4/10.png)
:::
:::

:::notes
QUE: wym by keep AIs? where?
:::

## Exporting: VWP

Common reports: Time binning and Interest Area

-   But time binning allows to measure proportions of fixations and saccades *across time*
    -   We can export info pertaining to a specific AI by doing some data wrangling manually

## Exporting: VWP

Time course (binning) analysis report

-   How much do we want to discretize time?
    -   Time bins, e.g., every 20 or 50 ms.
    -   Smaller bins: finer analysis, detect nuanced differences.
    -   Depends on our sample rate.

![](images/session_4/11.png){fig-align="center"}

## Exporting: VWP

Recap: sampling rate of X Hz = X number of samples per second.

-   Higher sample rate, less space between samples. 
-   1000 Hz is a 1000 sample (one per ms). We can create a bin of 20 ms, that comprises 20 samples.

-   500 Hz would yield 10 samples in a 20 ms bin.

-   Sampling rate also limits the size of our bins!
    -   If we gather a sample every 25 ms (i.e., tracking at 40 Hz, 1000/40), we cannot make 20 ms bins.

## Exporting: VWP

Time binning report

::: columns
::: {.column width="50%"}
-   Choose variables (or just export all)
-   Define the size of your bin
-   On-screen events only (unless you want that information)
-   E.g., samples in fixations and saccades
-   Separate report unchecked
:::

::: {.column width="50%"}
![](images/session_4/12.png)
:::
:::

:::notes
Espe: they might ask you how come the report doesn't include saccades if you can select the data here: the answer is that here you are deciding whether to count what happened during a saccade. It’s a bit difficult to explain, the important bit is that when they write it up they need to report (if they are analysing fixations) if they included samples in saccades or not.
:::

## Output: VWP

Time binning report

Participant ID (RECORDING_SESSION_LABEL), trial number (TRIAL_LABEL/INDEX) (added automatically by Data Viewer) +

BIN_INDEX: Which bin it is. This will help us figure out time (by taking the bin number and multiplying it by the size of your time bin, but you can also use 'BIN_START_TIME').

![](images/session_4/7.png)

## Output: VWP

Time binning report

Non-eye movement related variables that we need, i.e., most (except participant ID & trial number) variables that you coded yourself (can be found at the end of the list in blue).

![](images/session_4/6.png)

## Output: VWP

Time binning report

Variables of interest when looking at fixations: you have duplicated columns per eye. Since you'll most likely do monocular tracking, you'll only need one set.

Information duplicated per Interest Area. If you have two visuals, then two; three visuals, then three, and so on (*IA_x* & Information duplicated per eye). Even if it was monocular recording, it preserves columns for each eye.

-   IA_1_ID, IA_2_ID and so on: The label of the item that was presented in that region (target/distractor).
-   BIN_SAMPLE_COUNT: The number of samples per bin.
-   RIGHT_BLINK_SAMPLE_COUNT: Number of samples in a given bin that were a blink.

## Output: VWP

Time binning report

-   RIGHT_OFF_SCREEN_SAMPLE_COUNT: Number of samples in a given bin that fall outside of the display.
-   RIGHT_IA_1_SAMPLE_COUNT, RIGHT_IA_2_SAMPLE_COUNT and so on[^1]: The number of fixations that fell in that area per bin (out of the number of samples per bin, i.e., if each bin has 10 samples, then it's out of ten).
-   RIGHT_IA_1_SAMPLE_COUNT\_%, RIGHT_IA_2_SAMPLE_COUNT\_% and so on: The proportion of fixations that fell in that area (i.e., a transformation of count).

[^1]: Or AVERAGE\_, because we have done binocular reading, the average equals the count on our eye recorded.

## Output: VWP

.txt → import in Excel and save as .xlsx or .csv

![](images/session_4/5.png)

# Reading pipeline

## Interest Period: Reading

::: columns
::: {.column width="50%"}
<br>

Make sure your AIs are displayed when setting the TW.

-   Preferences \> Data Filters \> Show AIs
:::

::: {.column width="50%"}
![](images/session_4/29.png){fig-align="center"}
:::
:::

## Interest period: Reading

::: columns
::: {.column width="50%"}
<br>

From stimulus presentation to button press (trial end)
:::

::: {.column width="50%"}
![](images/session_4/28.png){fig-align="center"}
:::
:::

## Check data: Reading

More time-consuming than VWP but more conventional

-   Drift (vertical, horizontal)
-   Blinks
-   Unusual behaviours

## Check data: Reading

Trial-by-trial check

Horizontal drift

-   Safest -\> exclude such trials.
-   Especially when you are unsure which AIs certain fixations belong to.
-   **Never** move fixations to the left or right.

![](images/session_4/26.png){fig-align="center"}

## Check data: Reading

Trial-by-trial check

Vertical drift

-   Correct (move up/down) fixations that fall outside AIs
    -   Less work if top and bottom margins were big enough.
-   To move certain fixations up/down: Windows Alt+*arrow*, Mac Option+*arrow*

![](images/session_4/20.png){fig-align="center"}

## Check data: Reading

Trial-by-trial check

Blinks in critical AI

-   Exclude trial? Optional
    -   Individual participants' behaviour: Exclude participant?
    -   Happens randomly across conditions, and excluding those trials leads to overall large data loss: Ignore?

![](images/session_4/23.png){fig-align="center"}

## Check data: Reading

Trial-by-trial check

Other

-   If participants were instructed to look at the corner when finished reading but went back to re-read: Remove those extra fixations

![](images/session_4/22.png){fig-align="center"}

## Check data: Reading

Trial-by-trial check

Other

-   Started reading from the middle (critical AI): Exclude trial

![](images/session_4/21.png){fig-align="center"}

## Check data: Reading

Trial-by-trial check

Keep a detailed **log** of all changes!

![](images/session_4/reading_cleaning_logbook.png){fig-align="center"}

## Automatized cleaning

What are we thinking of?

-   Extremely short and long fixations (outliers)
-   Fixations outside of AIs

Familiarity with previous literature is **key**.

Balanced loss across conditions.

## Automatized cleaning: Reading

Different thresholds --\> report *exactly* what you did.

-   Short outliers
    -   Merge short nearby fixations and remove those that couldn't be merged.
    -   Or just remove all fixations that are, e.g. \< 100 ms (Sereno & Rayner, 2003).
    -   Merging might inflate fixation measures, nor merging might inflate skipping if short critical AI.
-   Long outliers
    -   Remove all fixations that are, e.g., \> 800 ms.
-   Delete fixations outside AIs

## Automatized cleaning: Reading

::: columns
::: {.column width="50%"}
<br> <br> <br>

In-built 4-stage fixation cleaning procedure.

Right-click on the whole folder.
:::

::: {.column width="50%"}
![](images/session_4/24.png) ![](images/session_4/16.png)
:::
:::

## Automatized cleaning: Reading

These are settings from D. Drieghe's lab (non-standard)!

![](images/session_4/16.png){fig-align="center"}

<sup>Stage 1: merge fix shorter than 80 ms and less than 0.5 degrees away from each other. Stage 2: merge remaining fix shorter than 40 ms and less than 1.25 degrees away from each other. Finally (Stage 4), remove all remaining fixations that are shorter than 80 ms or longer than 800 ms. Also, remove all fixations that fall outside the IAs.</sup>

## Example pre-processing: Reading

Howard et al., 2017

-   What technical information do we expect the authors to list and where?

::: columns
::: {.column width="50%"}
![](images/session_4/17.png)
:::

::: {.column width="50%"}
![](images/session_4/15.png)
:::
:::

## Example pre-processing: Reading

Puebla & Felser, 2022

-   What technical information do we expect the authors to list and where?

![](images/session_4/14.png){fig-align="center"}

## Example pre-processing: Reading

Holmqvist et al. (2022). Eye tracking: Empirical foundations for a minimal reporting guideline.[^2]

[^2]: This paper was retracted due to some legal issues.

Jakobi et al. (2024). Reporting eye-tracking data quality: Towards a new standard.

## Exporting: Reading

Recap: Fixations and saccades **in several AIs** (*n-1, n, n+1*)

-   Interest Area report

![](images/session_4/4.png){fig-align="center"}

## Exporting: Reading

Interest Area report

-   Choose variables (or just export all)

![](images/session_4/3.png){fig-align="center"}

## Exporting: Reading

Interest Area report

For analysis, we surely need participant ID (Session_Name\_), condition, item ID, and interest area ID (IA_ID). It could also be helpful to include IA_LABEL (text presented in the IA).

Let's choose some common measures:

::: columns
::: {.column width="50%"}
-   Skipping
-   First fixation duration
-   Gaze duration
-   Regressions-in
-   First-pass regressions-out
-   Regression path duration
-   Total reading time
:::

::: {.column width="50%"}
![](images/session_4/2.png)
:::
:::

## Output: Reading

Interest Area report

.txt → import in Excel and save as .xlsx or .csv

![](images/session_4/1.png){fig-align="center"}

-   Here all IAs are listed; for analysis, we look at each IA separately!

## Reading: Regions skipped on first pass

After we exported our Interest Area report and before starting the analysis:

-   In some trials, the region could have been skipped on first pass (IA_SKIP = 1).

-   This affects subsequent reading measures (first fixation? re-reading? etc.)

-   Therefore, we can report and analyze the skipping rate if we want;

-   For other measures, if IA_SKIP = 1, we will change the values to *NA* (label them as *missing values*) before we analyze them.

-   First-pass measures for sure: e.g., first fixation duration, gaze duration, first-pass regressions-out, regression-path duration.

## Reading: Regions skipped completely

If IA_SKIP = 1, but we don't want to just assign *NA* to all measures indiscriminately:

-   Later measures (e.g., regressions-in, total reading time): we can assign *NA* if the region was skipped completely (i.e., if IA_DWELL_TIME = 0).

    -   For regressions-in it's already the default;

    -   For total reading time the default is 0. Why would we want *NA* instead of 0? It might be better for statistical analysis (certain models don't deal well with zero values).

## Take-home messages

-   Coding your experiment efficiently (e.g., meaningful variable names) saves you some time during pre-processing and data wrangling.
-   Eye-tracking data consists of timestamped eye movement events and messages.
-   Software like Data Viewer make it easy to export what we are interested in.
-   VWP: important to understand how many samples there are per bin and how many seconds a bin entails.
-   Reading: important to decide on cleaning criteria beforehand and report them. Remember that pre-processing (trial-by-trial check) might take time.

# Practice time

## Your turn

Process the data you acquired in the lab!

# Wrap-up

## Plan for tomorrow

-   Data visualization
-   Data analysis

## Homework

-   Have the file ready for analysis
-   Download and install R (if you haven't yet)
-   Think how you'll visualize your data

## References {.smaller}

Apfelbaum, K. S., Klein-Packard, J., & McMurray, B. (2021). The pictures who shall not be named: Empirical support for benefits of preview in the Visual World Paradigm. *Journal of Memory and Language, 121*, 104--279. https://doi.org/10.1016/j.jml.2021.104279

Arantzeta, M., Bastiaanse, R., Burchert, F., Wieling, M., Martinez-Zabaleta, M., & Laka, I. (2017). Eye-tracking the effect of word order in sentence comprehension in aphasia: Evidence from Basque, a free word order ergative language. *Language, Cognition and Neuroscience, 32*(10), 1320--1343. https://doi.org/10.1080/23273798.2017.1344715

Holmqvist, K., Örbom, S. L., Hooge, I. T. C., Niehorster, D. C., Alexander, R. G., Andersson, R., Benjamins, J. S., Blignaut, P., Brouwer, A.-M., Chuang, L. L., Dalrymple, K. A., Drieghe, D., Dunn, M. J., Ettinger, U., Fiedler, S., Foulsham, T., van der Geest, J. N., Hansen, D. W., Hutton, S. B., … Hessels, R. S. (2023). Eye tracking: Empirical foundations for a minimal reporting guideline. *Behavior Research Methods, 55*(1), 364--416. https://doi.org/10.3758/s13428-021-01762-8

## References {.smaller}

Howard, P. L., Liversedge, S. P., & Benson, V. (2017). Processing of co-reference in autism spectrum disorder. *Autism Research, 10*(12), 1968--1980. https://doi.org/10.1002/aur.1845

Jakobi, D. N., Krakowczyk, D. G., & Jäger, L. A. (2024). Reporting Eye-Tracking Data Quality: Towards a New Standard. *ETRA ’24: Proceedings of the 2024 Symposium on Eye Tracking Research and Applications.* https://doi.org/10.1145/3649902.3655658

Knoeferle, P., & Crocker, M. W. (2006). The Coordinated Interplay of Scene, Utterance, and World Knowledge: Evidence From Eye Tracking. *Cognitive Science, 30*(3), 481--529. https://doi.org/10.1207/s15516709cog0000_65

Puebla, C., & Felser, C. (2022). Discourse Prominence and Antecedent Mis-Retrieval during Native and Non-Native Pronoun Resolution. Discours. Revue de Linguistique, Psycholinguistique et Informatique. *A Journal of Linguistics, Psycholinguistics and Computational Linguistics, 29*, Article 29. https://doi.org/10.4000/discours.11720

Sereno, S. C., & Rayner, K. (2003). Measuring word recognition in reading: Eye movements and event-related potentials. *Trends in Cognitive Sciences, 7*(11), 489--493. https://doi.org/10.1016/j.tics.2003.09.010
