<!DOCTYPE html>
<html lang="en"><head>
<script src="session_3_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_3_files/libs/quarto-html/tabby.min.js"></script>
<script src="session_3_files/libs/quarto-html/popper.min.js"></script>
<script src="session_3_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session_3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_3_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session_3_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session_3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session_3_files/libs/quarto-contrib/videojs/video.min.js"></script>
<link href="session_3_files/libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.555">

  <meta name="author" content="Baltais &amp; Badaya">
  <title>Reading</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session_3_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session_3_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="session_3_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session_3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session_3_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session_3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session_3_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .center-xy {
    margin: 0;
    position: absolute;
    top: 40%;
    left: 40%;
    -ms-transform: translateY(-50%), translateX(-50%);
    transform: translateY(-50%), translateX(-50%);
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Reading</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Baltais &amp; Badaya 
</div>
</div>
</div>

</section>
<section id="welcome-back" class="slide level2">
<h2>Welcome back!</h2>
<p>Any questions from yesterday?</p>
</section>
<section id="overview-of-today" class="slide level2">
<h2>Overview of today</h2>
<p>Eye-tracking during reading</p>
<ul>
<li>Theory
<ul>
<li>What’s special about reading</li>
<li>Reading measures</li>
<li>Reading paradigms</li>
<li>Example: single sentence reading</li>
</ul></li>
<li>Practice
<ul>
<li>Build a reading study</li>
</ul></li>
</ul>
</section>
<section>
<section id="whats-special-about-reading" class="title-slide slide level1 center">
<h1>What’s special about reading</h1>

</section>
<section id="reading-research-in-a-nutshell" class="slide level2">
<h2>Reading research in a nutshell</h2>
<p>We have a wealth of <strong>paradigms</strong> to explore how individuals comprehend a piece of written text (from words to multiple sentences, created stimuli or real texts), considering both <strong>low-level processes</strong> for word identification (e.g., lexical access) and <strong>higher-level processes</strong> for sentence and discourse comprehension (e.g., inferences).</p>
</section>
<section id="reading-is-a-skill" class="slide level2">
<h2>Reading is a skill</h2>
<ul>
<li>Reading is relatively new
<ul>
<li>Homo Sapiens <span class="math inline">\(\pm\)</span> 300.000 BC</li>
<li>Sumerian pictographic writing <span class="math inline">\(\pm\)</span> 3.300 BC</li>
<li>Phoenician alphabetic language <span class="math inline">\(\pm\)</span> 2.000 BC</li>
</ul></li>
<li>Exposure, practice, formal instruction
<ul>
<li>We are good at it (<span class="math inline">\(\pm\)</span> 250 WPM; Brysbaert, 2019)</li>
<li>Linked to academic achievement, late L2 acquisition</li>
</ul></li>
</ul>
</section>
<section id="history" class="slide level2">
<h2>History</h2>
<ul>
<li><p>Javal (1879)</p>
<ul>
<li>Reading is not smooth</li>
</ul></li>
<li><p>Mid-70s: Technological advances</p></li>
<li><p>Rayner’s work</p>
<ul>
<li>Clifton et al., 2016: “Eye movements in reading and information processing: Keith Rayner’s 40 year legacy”</li>
</ul></li>
</ul>
</section>
<section id="some-general-facts" class="slide level2">
<h2>Some general facts</h2>
<ul>
<li>Most people are good at reading, but there are individual differences (e.g., reading proficiency, L1 vs.&nbsp;L2…)</li>
<li>There seems to be some immediacy<sup>1</sup> when comprehending a text, but we also get ahead of ourselves</li>
<li>In contrast to spoken language, people can go back when they encounter a difficulty, and indeed they do</li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Cf. linking hypothesis; serial versus parallel processing debate</p></li></ol></aside></section>
<section id="eye-movements-in-reading" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Human Visual System (recap):</p>
<ul>
<li><strong>Fovea</strong>: highest visual acuity</li>
<li><strong>Parafovea</strong>: still partial recognition of objects</li>
<li><strong>Periphery</strong>: blurred image</li>
</ul>
<p><strong>Fixations</strong> vs.&nbsp;<strong>saccades</strong>: movements (saccades) that place objects on the fovea for processing (fixations).</p>
</section>
<section id="eye-movements-in-reading-1" class="slide level2">
<h2>Eye movements in reading</h2>
<p>How does this work in reading?</p>
<ul>
<li>A priori: Our eyes move, so that objects (words?) are placed on the fovea and can be processed.</li>
</ul>
<p>…but this is not exactly so.</p>
</section>
<section id="eye-movements-in-reading-2" class="slide level2">
<h2>Eye movements in reading</h2>

<img data-src="images/session_3/perceptual_span_reading.JPG" width="590" class="r-stretch quarto-figure-center"><p class="caption">From Conklin et al., 2018</p></section>
<section id="eye-movements-in-reading-3" class="slide level2">
<h2>Eye movements in reading</h2>
<ul>
<li>Fovea does not equate the perceptual span (= effective visual field)</li>
<li>Reading is asymmetric = asymmetric extraction of information
<ul>
<li>Bias towards the direction of reading in the language</li>
<li>English: 3-4<sup>1</sup> letter spaces (1 degree of visual angle) to the left, 14-15 ls to the right</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p>Depends on print size, of course.</p></li></ol></aside></section>
<section id="eye-movements-in-reading-4" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Fixations</p>
<ul>
<li>Mean duration (silent reading in English): 200 - 250 ms</li>
<li>Lexical access initiated at about 100 ms (Sereno &amp; Rayner, 2003)</li>
<li>Optimal Viewing Position</li>
<li>Affected by word length, frequency, predictability, etc.</li>
<li>Most reading measures are based on fixation durations</li>
</ul>

<img data-src="images/session_3/mariia_fix.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p><aside class="notes">
<p>The Optimal Viewing Position (OVP) effect shows that word identification is best when the eyes first fixate near the center of words. When words are presented for short durations, recognition performance is maximal when the viewing position is slightly left of the word center.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-in-reading-5" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Saccades</p>
<ul>
<li>Saccadic supression</li>
<li>Mean size (silent reading in English): 7 to 9 letter spaces (<span class="math inline">\(\pm\)</span> 2 words before or after the fixated word)
<ul>
<li>Size: distance travelled (amplitude)</li>
</ul></li>
<li>Not analyzed except for backward saccades (regressions)</li>
</ul>

<img data-src="images/session_3/mariia_sac.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p><aside class="notes">
<p>Saccadic suppression refers to the behavioral observation that healthy humans under normal circumstances do not perceive this motion.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-in-reading-6" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Regressions</p>
<ul>
<li>10-15% of the saccades are regressions to preceding areas
<ul>
<li>Correct ‘overshooting’</li>
<li>Processing difficulty</li>
</ul></li>
</ul>

<img data-src="images/session_3/mariia_sac.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p></section>
<section id="eye-movements-in-reading-7" class="slide level2">
<h2>Eye movements in reading</h2>

<img data-src="images/session_3/skipping.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="eye-movements-in-reading-8" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Skipping</p>
<ul>
<li>70% of the words in a text are fixated</li>
<li>Content words are fixated 85% of the time, function words 35% of the time</li>
<li>Predictors: word length; also predictability, frequency, etc.
<ul>
<li>Parafoveal processing!</li>
</ul></li>
<li>Overshooting or processing ease</li>
</ul>
</section>
<section id="eye-movements-in-reading-9" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Blinking</p>
<ul>
<li>Noise
<ul>
<li>Data pre-processing</li>
</ul></li>
</ul>

<img data-src="images/session_3/mariia_blink.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p></section>
<section id="linking-hypothesis" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Fixations, regressions, skipping -&gt; wide range of measures.</p>
<p>Reading (during a fixation) entails the use of visual, orthographic, phonological, and morphological information to:</p>
<ul>
<li>Identify a word</li>
<li>Activate its lexical representation</li>
<li>Integrate with the preceding context</li>
<li>Predict what’s next?</li>
</ul>
</section>
<section id="linking-hypothesis-1" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Just and Carpenter’s (1980) eye-mind hypothesis (recap):</p>
<ul>
<li><p>Where we look at = what we are processing</p>
<ul>
<li>Extends beyond linguistic processing (visual attention)</li>
</ul></li>
</ul>
<p>Just and Carpenter’s (1980) immediacy hypothesis:</p>
<ul>
<li><p>Readers try to interpret each content word as it is encountered</p>
<ul>
<li>As soon as possible, without delay</li>
</ul></li>
</ul>
<p>And,</p>
<ul>
<li>For how long we look = how difficult it is to process</li>
</ul>
<aside class="notes">
<p>From J&amp;C:</p>
<p>Gaze durations reflect the time to execute comprehension processes. Longer fixations are attributed to longer processing caused by the word’s infrequency and its thematic importance.</p>
<p>The model of reading comprehension proposes that readers interpret a word while they are fixating it, and they continue to fixate it until they have processed it as far as they can.</p>
<p>The link between eye fixation data and the theory rests on two assumptions. One is the eye-mind assumption: <strong>the eye remains fixated on a word as long as the word is being processed</strong>. The eye-mind assumption posits that there is no appreciable lag between what is being fixated and what is being processed.</p>
<p>The immediacy assumption: a reader tries to interpret each content word of a text as it is encountered, even at the expense of making guesses that sometimes turn out to be wrong. The immediacy assumption posits that the interpretations at all levels of processing are not deferred; they occur as soon as possible.</p>
<p>The immediacy assumption posits that an attempt to relate each content word to its referent occurs as soon as possible. Sometimes this can be done when the word is first fixated, but sometimes more information is required. For example, although the semantic interpretation of a relative adjective like large can be computed immediately, the extensive meaning depends on the word it modifies (e.g., large insect vs.&nbsp;large building). The referent of the entire noun phrase can be computed only after both words are processed. The immediacy assumption does not state that the relating is done immediately on each content word, but rather that it occurs as soon as possible.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-2" class="slide level2">
<h2>Linking hypothesis</h2>
<p>The more difficult the text…</p>
<ul>
<li>The ________________ the fixation durations</li>
<li>The ________________ the saccade sizes</li>
<li>The ________________ the regressions</li>
<li>The ________________ the skipping of words</li>
</ul>
</section>
<section id="linking-hypothesis-3" class="slide level2">
<h2>Linking hypothesis</h2>
<p>The more difficult the text…</p>
<ul>
<li>The longer the fixation durations</li>
<li>The smaller the saccade sizes</li>
<li>The more frequent the regressions</li>
<li>The less frequent the skipping of words</li>
</ul>
</section>
<section id="linking-hypothesis-4" class="slide level2">
<h2>Linking hypothesis</h2>
<ul>
<li><p>“Strong” eye-mind hypothesis: readers fixate a word until it’s processed as far as possible</p>
<ul>
<li><p>Morrison, 1984: completion of lexical access on word <em>n</em> -&gt; immediate shift in attention &amp; planning a saccade to word <em>n+1</em></p></li>
<li><p>Boland, 2004: the eyes do not leave a word until it has been <em>structurally</em> integrated</p></li>
</ul></li>
</ul>
<aside class="notes">
<p>Boland, 2004: The eyes do not leave a word until it has been structurally integrated. Therefore, <strong>constraints that control structure-building influence first-pass reading time</strong>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-5" class="slide level2">
<h2>Linking hypothesis</h2>
<ul>
<li><p>Too strong?</p>
<ul>
<li><p>Pickering et al.&nbsp;(2004): some aspects of processing take more time than the eye is “prepared to wait”</p></li>
<li><p>Mitchell et al.&nbsp;(2008): regressions are not only linguistically supervised</p></li>
</ul></li>
</ul>
<aside class="notes">
<p>Pickering et al.&nbsp;(2004): some aspects of lexical, syntactic, and semantic processing do (largely) respect the immediacy and eye-mind assumptions (with some important caveats), but that <strong>many aspects of sentence interpretation are somewhat delayed</strong>. Semantic processing may not all occur “at once” (readers do look at difficult words such as Finland - in “A lot of Americans protested during Finland”, - but they largely do this during later processing rather than during first pass).</p>
<p>Mitchell et al.&nbsp;(2008): the purpose of regressive fixations (and, indeed, re-fixations on the same word) is not to refresh the evidence but merely a delaying tactic used to provide “time out” for as-yet-incomplete parsing operations (to postpone new input).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-6" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Asymmetrical perceptual span:</p>
<ul>
<li><strong>Preview benefit</strong>: some info about <em>n+1</em> while still on <em>n</em> -&gt; shorter reading times <em>n+1</em>
<ul>
<li>Lexical processing can start before a word is fixated</li>
</ul></li>
<li><strong>Parafoveal-on-foveal effects</strong>: <em>n+1</em> characteristics influence reading times for <em>n</em>
<ul>
<li>Cf. Table 4.1 in Conklin et al.&nbsp;(2018: 83)</li>
</ul></li>
</ul>
</section>
<section id="linking-hypothesis-7" class="slide level2">
<h2>Linking hypothesis</h2>
<p>As well as</p>
<ul>
<li><strong>Spillover effects</strong>: <em>n</em> characteristics influence reading times for <em>n+1</em>
<ul>
<li>Processing has not been completed</li>
</ul></li>
</ul>
<p>Therefore,</p>
<ul>
<li><p>Serial vs.&nbsp;parallel processing debate</p>
<ul>
<li>Different models of reading (E-Z Reader vs.&nbsp;SWIFT; OB1-Reader)</li>
</ul></li>
</ul>
<aside class="notes">
<p>Serial: words are processed one at a time in a linear sequence.</p>
<p>Parallel: words are processed simultaneously. The cognitive system processes information from several words at once, allowing for faster and more efficient reading.</p>
<p>The OB1-Reader model (upgrade of E-Z?) incorporates both serial and parallel processing components.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="measures" class="title-slide slide level1 center">
<h1>Measures</h1>

</section>
<section id="measures-1" class="slide level2">
<h2>Measures</h2>
<p>Fixations, regressions, and skipping form multiple reading measures that arguably tap into different cognitive processes.</p>
<ul>
<li><strong>Global</strong> vs.&nbsp;<strong>local</strong> measures
<ul>
<li>Overall reading behaviour versus smaller units of text</li>
</ul></li>
<li>Local measures: <strong>early</strong> (vs.&nbsp;intermediate) vs.&nbsp;<strong>late</strong> (see also Clifton et al., 2007).</li>
</ul>
</section>
<section id="measures-2" class="slide level2">
<h2>Measures</h2>
<p>Reading measures &lt;-&gt; specific cognitive events? <strong>Linking</strong> depends on researchers’ theoretical assumptions (Boland, 2004).</p>
<p>Traditionally,</p>
<ul>
<li><strong>Early measures</strong>: highly automatic word recognition and lexical access processes</li>
<li><strong>Late measures</strong>: more conscious, controlled, strategic processes</li>
</ul>
<p>NB:</p>
<ul>
<li>Different types of linguistic manipulations (e.g., lexical vs.&nbsp;syntactic) can produce effects in the same reading measure (e.g., gaze duration or total reading time).</li>
</ul>
</section>
<section id="early-measures" class="slide level2">
<h2>Early measures</h2>
<p>First-pass measures.</p>
<ul>
<li>First fixation duration, (single fixation duration), gaze duration: Time on an AI.</li>
<li>Skipping rate: % words that receive no fixation during 1st pass.
<ul>
<li>Visual and linguistic factors e.g., parafoveal processing.</li>
</ul></li>
<li>Factors known to affect: Frequency, familiarity, ambiguity, predictability, semantic association.</li>
</ul>
<aside class="notes">
<p>1st fix: duration of the 1st fix on a word provided it isn’t skipped. single fix dur: when there is only one fix gaze duration: sum of all fixations on a word prior to moving to another one (forward or backwards) skipping more likely due to linguistic factors but also processing it in the parafovea</p>
<p>earliest point for us to see an effect of a variable being manipulated</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="early-measures-1" class="slide level2">
<h2>Early measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration: 3</li>
<li>Single fixation duration: NA (there are two fixations in the first pass)</li>
<li>Gaze duration: 3 + 4</li>
<li>Skipping rate: % trials where AI is skipped in first pass.</li>
</ul>
</section>
<section id="intermediate-measures" class="slide level2">
<h2>Intermediate measures</h2>
<ul>
<li>Regression-path duration (or go-past time): time on N, N -1, etc. before N +1,
<ul>
<li>Includes regressions out (1st pass regressions) and regressions in.</li>
</ul></li>
</ul>
<aside class="notes">
<p>hard to tell whether they show difficulty of encountering an item and the subsequent time taken to overcome the difficulty</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="intermediate-measures-1" class="slide level2">
<h2>Intermediate measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>Regression-path duration/go-past time: 3+4+5+6</li>
<li>Regressions-out (on first pass): %</li>
<li>Regressions-in: %</li>
</ul>
</section>
<section id="late-measures" class="slide level2">
<h2>Late measures</h2>
<ul>
<li>Total reading time: <span class="math inline">\(\Sigma\)</span> fix on N.
<ul>
<li>Contextual and discourse-level factors + lexical (i.e., retrieval and integration).</li>
</ul></li>
<li>Re-reading time (2nd pass reading time): Different definitions across the literature.
<ul>
<li>General comprehension difficulties</li>
</ul></li>
</ul>
</section>
<section id="late-measures-1" class="slide level2">
<h2>Late measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>Total reading time: 3+4+6+9</li>
<li>Re-reading time: 6+9 (total reading time - gaze duration)</li>
<li>Fixation count: 4</li>
</ul>
</section>
<section id="measures-3" class="slide level2">
<h2>Measures</h2>
<p>Measures are dependent:</p>
<ul>
<li>1st fixation duration - Gaze duration - Total reading time.</li>
<li>Late measures are cumulative.</li>
<li>Not total transparent mapping!</li>
</ul>
<p>To reiterate…</p>

<img data-src="images/session_3/reiterate.JPG" class="r-stretch quarto-figure-center"><p class="caption">Carroll, 2017</p></section>
<section id="measures-exercise" class="slide level2">
<h2>Measures: Exercise</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/session_3/exercise.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration:</li>
<li>Gaze duration:</li>
<li>Regressions-out:</li>
<li>Regression-path duration:</li>
<li>Re-reading time:</li>
<li>Total reading time:</li>
</ul>
<aside class="notes">
<p>3 3 2 3+4+5+6+7 5+7 3+5+7</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="paradigms-in-reading-research" class="title-slide slide level1 center">
<h1>Paradigms in reading research</h1>

</section>
<section id="paradigms" class="slide level2">
<h2>Paradigms</h2>
<p>Ecological validity.</p>
<ul>
<li>Gaze-contigent paradigms: Eye gaze determines changes to the text display.</li>
<li>Reading of experimental stimuli.</li>
<li>Reading of natural text.</li>
</ul>
</section>
<section id="disappearing-text-paradigm" class="slide level2">
<h2>Disappearing text paradigm</h2>
<p>Liversedge et al., 2004</p>
<ul>
<li>ROI: Disappears after an amount of time of fixation.</li>
<li>Shows: Fovea processing.</li>
<li>IV: Time to disappear ~ time for normal reading.
<ul>
<li>Visual exposure for word recognition.</li>
</ul></li>
</ul>
</section>
<section id="disappearing-text-paradigm-1" class="slide level2">
<h2>Disappearing text paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_3/disappearing_text_animation.mp4"></video>
</section>
<section id="fast-priming-paradigm" class="slide level2">
<h2>Fast priming paradigm</h2>
<p>Sereno &amp; Rayder, 1992</p>
<ul>
<li>ROI: Target word is first a prime, then target word is presented after a fixation on it has occured for some time.</li>
<li>Shows: Prime facilitation</li>
<li>IV: Relationship prime and target.</li>
</ul>
</section>
<section id="fast-priming-paradigm-1" class="slide level2">
<h2>Fast priming paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_3/fast_priming_paradigm.mp4"></video>
</section>
<section id="moving-window-paradigm" class="slide level2">
<h2>Moving window paradigm</h2>
<p>McConkie &amp; Rayder, 1975</p>
<ul>
<li>ROI: Mask around a fixation.</li>
<li>Shows: Parafoveal processing and perceptual span (extraction and use of information).</li>
<li>IV: Size of the window.</li>
</ul>
<aside class="notes">
<p>By varying the size of the window and the type of mask (e.g., X’s, visually similar or dissimilar characters) and comparing the reading times in the window and normal reading conditions, it is possible to define the size of the area from which a reader can efficiently extract and utilize information.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="moving-window-paradigm-1" class="slide level2">
<h2>Moving window paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_3/moving_window.mp4"></video>
</section>
<section id="foveal-mask" class="slide level2">
<h2>Foveal mask</h2>
<p>Rayner &amp; Bertera, 1979</p>
<ul>
<li>ROI: Mask within the fixation</li>
<li>Shows: Parafoveal processing and perceptual span.</li>
<li>IV: Size of the window.</li>
</ul>
</section>
<section id="foveal-mask-1" class="slide level2">
<h2>Foveal mask</h2>
<p><br></p>
<video id="video_shortcode_videojs_video4" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_3/foveal_mask.mp4"></video>
</section>
<section id="boundary-paradigm" class="slide level2">
<h2>Boundary paradigm</h2>
<p>Rayder, 1975</p>
<p>Saccadic suppression.</p>
<ul>
<li>ROI: Masked and then changed after fixations cross an invisible boundary.</li>
<li>Shows: Parafoveal processing and perceptual span (information extracted).
<ul>
<li>Preview window effect.</li>
<li>Parafoveal-on-foveal effect (e.g., Drieghe, 2011).</li>
</ul></li>
<li>IV: What information is extracted e.g., visual similarity.</li>
</ul>
<aside class="notes">
<p>The boundary paradigm (Rayner, 1975) makes use of the saccadic suppression. Saccadic suppression means that during a saccade the intake of visual information is suspended and the reader is practically blind. If a change in the visual environment is made during a saccade or very soon after the eyes have landed (&lt; 6 ms after the end of a saccade, McConkie &amp; Loschky, 2002), the reader does not become consciously aware of it. The target word (“sentence” in the example of Table 7.1) is initially masked with a character string (“somkasoc”), and when the reader’s eyes cross an invisible boundary in the text, the mask is replaced with the actual target word.</p>
<p>If the reader has extracted information from the target word preview prior to its change to the correct form, one should observe increased fixation time on the target word, even though the reader is not consciously aware of this. The size of the slowdown in eye fixation time, i.e.&nbsp;the difference between normal condition inwhich no change was made and a change condition is called the preview effect.</p>
<p>The preview benefit is simply computed as the difference in fixation time between a full preview condition, in which the target word was presented normally, and the preview condition.</p>
<p>Another measure to assess parafoveal processing is the so-called parafoveal-on-foveal effect (Kennedy, 2000). It measures the extent to which parafoveally available information affects fixation time on the previous word (Drieghe, 2011).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="boundary-paradigm-1" class="slide level2">
<h2>Boundary paradigm</h2>
<p><br></p>
<video id="video_shortcode_videojs_video5" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="images/session_3/boundary.mp4"></video>
</section>
<section id="natural-sentence-reading-experimental-stimuli" class="slide level2">
<h2>Natural sentence reading (experimental stimuli)</h2>
<p>Linguistic and non-linguistic factors affecting eye patterns in reading.</p>
<p>Usually, exploration of how movement patterns over a IA differ as a function of the properties of it.</p>
<p>Frazier &amp; Rayner (1982)</p>

<img data-src="images/session_3/fraz.JPG" class="r-stretch"></section>
<section id="natural-text-reading-constructed-texts" class="slide level2">
<h2>Natural text reading (constructed texts)</h2>
<p>Pellicer-Sánchez (2016)</p>

<img data-src="images/session_3/texts_example.JPG" class="r-stretch quarto-figure-center"><p class="caption">From Conklin et al., 2016</p></section>
<section id="natural-text-reading" class="slide level2">
<h2>Natural text reading</h2>
<p>GECO corpus (Cop et al., 2017)</p>
<ul>
<li>The Mysterious Affair At Styles by Agatha Christie (1920)</li>
</ul>

<img data-src="images/session_3/geco.JPG" class="r-stretch"><p>Lack of control over text versus ecological validity.</p>
<ul>
<li>Participants’ characteristics + matching samples
<ul>
<li>Additional questionnaires</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="single-sentence-reading-experiment" class="title-slide slide level1 center">
<h1>Single sentence reading experiment</h1>

</section>
<section id="single-sentence-reading-experiment-1" class="slide level2">
<h2>Single sentence reading experiment</h2>
<p>Elements:</p>
<ul>
<li>A sentence.</li>
</ul>
<p>Trial sequence: Drift &gt; Visual Presentation</p>
</section>
<section id="drift-correction" class="slide level2">
<h2>Drift correction</h2>
<p>Where the first word appears</p>

<img data-src="images/lab_setup/drift_reading.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="visual-presentation" class="slide level2">
<h2>Visual Presentation</h2>
<p>Visual presentation.</p>
<ul>
<li>Time window of interest is from start to end (so no triggers in between).
<ul>
<li>cf.&nbsp;gaze-contingent paradigms.</li>
</ul></li>
</ul>
</section>
<section id="visual-presentation-1" class="slide level2">
<h2>Visual Presentation</h2>
<p>Visual presentation: Areas of Interest</p>

<img data-src="images/session_2/ia_reading.png" class="r-stretch"></section>
<section id="areas-of-interest" class="slide level2">
<h2>Areas of Interest</h2>
<ul>
<li>Target region but also
<ul>
<li>Pre-critical region</li>
<li>Spillover region</li>
</ul></li>
</ul>
<p>Research question.</p>
<ul>
<li>Single words v phrases v clauses …</li>
</ul>
</section>
<section id="areas-of-interest-1" class="slide level2">
<h2>Areas of Interest</h2>
<ul>
<li>Never beginning/end of the line.
<ul>
<li>1st and last fixations on a line are 5-7 ls from the edges.</li>
<li>1st word likely to be skipped.</li>
<li>1st fix longer, last fix shorter.</li>
</ul></li>
<li>AI never clause-final.
<ul>
<li>Wrap-up effect.</li>
</ul></li>
</ul>
</section>
<section id="task" class="slide level2">
<h2>Task</h2>
<ul>
<li>Move to the next trial</li>
<li>Comprehension (every now and then/every trial)</li>
</ul>
</section>
<section id="confounds" class="slide level2">
<h2>Confounds</h2>
<ul>
<li>Characteristics of the visual presentation (non-linguistic).</li>
<li>Characteristics of the materials (linguistic)</li>
<li>Characteristics of the human visual system</li>
</ul>
</section>
<section id="non-linguistic-confounds" class="slide level2">
<h2>Non-linguistic confounds</h2>
<p>Text layout:</p>
<ul>
<li>Font style and size
<ul>
<li>B&amp;W for dyslexic participants is not good.</li>
<li>Horizontal space.</li>
<li>14-18 pt</li>
<li>Monospace font (e.g., Courier)</li>
</ul></li>
</ul>
</section>
<section id="non-linguistic-confounds-1" class="slide level2">
<h2>Non-linguistic confounds</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Line spacing
<ul>
<li>Less accuracy for vertical movements</li>
<li>Double/triple spacing.</li>
</ul></li>
<li>Line breaks</li>
<li>Margins
<ul>
<li>Accuracy is low at screen borders.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br></p>
<p><img data-src="images/session_3/example_spacing.JPG"></p>
</div>
</div>
</section>
<section id="linguistic-confounds" class="slide level2">
<h2>Linguistic confounds</h2>
<p>Same we have discussed in previous weeks (see Clifton et al., 2016):</p>
<ul>
<li>The Big Three (Frequency, Length, Predictability)</li>
<li>Lexical ambiguity</li>
<li>Orthographic neighbourhood size (differ by 1 letter)</li>
<li>Prevalence</li>
<li>Inter alia</li>
</ul>
</section>
<section id="hvs-confounds" class="slide level2">
<h2>HVS confounds</h2>
<p>Foveal versus parafoveal processing.</p>
<ul>
<li><p>Sequential nature of reading.</p></li>
<li><p>How information from one area can influence eye movements on the other.</p>
<ul>
<li>Predictability, length, etc.</li>
<li>Parafoveal-on-foveal effects (see Hyönä, 2011).</li>
<li>Serial versus parallel processing.
<ul>
<li>Preview benefit, spillover effects.</li>
</ul></li>
</ul></li>
<li><p>Spillover: Effects of processing N are seen on N + 1, + 2.</p></li>
<li><p>Preview: Effects of perceiving N on N - 1.</p>
<ul>
<li>Match contexts!</li>
</ul></li>
</ul>
</section>
<section id="exercise" class="slide level2">
<h2>Exercise</h2>
<p>Howard et al.&nbsp;(2017)</p>
<ol type="1">
<li>How did they define the Areas of Interest?</li>
<li>What factors should be considered for the antecedent? (i.e., hammer/plunger in the example)</li>
<li>What information do you need from procedure and apparatus to replicate this study?</li>
</ol>
</section>
<section id="exercise-1" class="slide level2">
<h2>Exercise</h2>
<ol type="1">
<li>How did they define the Areas of Interest?</li>
</ol>
<p>7 AIs, two regions of interest.</p>

<img data-src="images/session_3/ias_paper.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="exercise-2" class="slide level2">
<h2>Exercise</h2>
<ol start="2" type="1">
<li>What factors should be considered for the antecedent?</li>
</ol>
<ul>
<li>Typicality (IV)</li>
<li>No matching!</li>
</ul>

<img data-src="images/session_3/matching_exercise.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="exercise-3" class="slide level2">
<h2>Exercise</h2>
<ol start="3" type="1">
<li>What information do you need from procedure and apparatus to replicate this study?</li>
</ol>
<p><img data-src="images/session_3/3a.JPG" class="quarto-figure quarto-figure-left" width="500"> <img data-src="images/session_3/3b.JPG" class="quarto-figure quarto-figure-right" width="500"></p>
</section>
<section id="analysis" class="slide level2">
<h2>Analysis</h2>
<p>More on W9 &amp; W11 but:</p>
<ul>
<li>Multiple dependent variables (stages of processing).</li>
<li>Type I error</li>
<li>Tables!</li>
</ul>
</section>
<section id="uses" class="slide level2">
<h2>Uses</h2>
<ul>
<li>Word recognition
<ul>
<li>Frequency effect, Rayner &amp; Duffy (1986)</li>
</ul></li>
<li>Syntactic processing
<ul>
<li>Structural ambiguities, Frazier &amp; Rayner (1982)</li>
</ul></li>
<li>Semantic integration
<ul>
<li>Plausibility effect, Staub et al.&nbsp;(2007)</li>
</ul></li>
<li>Predictive processing (Frisson et al., 2017)</li>
<li>Parafoveal processing (Juhasz et al., 2009)</li>
<li>Text comprehension (Dirix et al., 2019)</li>
<li>Multiword units (Carrol et al., 2016)</li>
</ul>
</section>
<section id="uses-1" class="slide level2">
<h2>Uses</h2>
<ul>
<li>L2 speakers
<ul>
<li>Gender agreement, Keating (2009)</li>
</ul></li>
<li>Children (Blythe et al., 2011)</li>
<li>Older adults (Solan et al., 1995)</li>
<li>Clinical populations
<ul>
<li>Children with dyslexia, Hyönä et al.&nbsp;(1995)</li>
<li>Adults with ASD, Howard et al.&nbsp;(2017)</li>
</ul></li>
</ul>
</section>
<section id="pros-cons" class="slide level2">
<h2>Pros &amp; cons</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Pros</strong></p>
<ul>
<li>Naturalistic reading.</li>
<li>Non-invasive method.</li>
<li>Different stages of processing.</li>
<li>Combination with offline measures.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Cons</strong></p>
<ul>
<li>Literacy required.</li>
<li>Dependent variables.</li>
<li>Resources.</li>
</ul>
</div>
</div>
</section>
<section class="slide level2">

<p>Let’s take a break!</p>
</section>
<section id="building-an-eye-tracking-experiment" class="slide level2">
<h2>Building an eye-tracking experiment</h2>
<p>We are going to code a reading eye-tracking experiment on OS.</p>
<ul>
<li>Many of the things we learnt yesterday can be used today!
<ul>
<li>Especially, we will use the same eye-tracking plugins</li>
</ul></li>
</ul>
</section>
<section id="building-a-reading-experiment" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Example: high/low frequency words (Rayner &amp; Duffy,1986))</p>
<ul>
<li>DV: Fixations on an area of interest</li>
<li>IV: Word frequency</li>
</ul>
<p>5 areas of interest</p>
</section>
<section id="building-a-reading-experiment-1" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The constraints of design:</p>
<ul>
<li>We want to counterbalance particants across lists</li>
<li>We want participants to progress in the experiment by pressing the spacebar</li>
<li>We want to show participants the instructions at the beginning of the experiment</li>
</ul>
</section>
<section id="building-a-reading-experiment-2" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The constraints of the eye tracker:</p>
<ul>
<li>We want five areas of interest</li>
<li>We want a drift correction at the beginning of every trial
<ul>
<li>Drift should appear where the sentence starts</li>
</ul></li>
</ul>
</section>
<section id="building-a-reading-experiment-3" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The loop</p>
<ul>
<li>Each row: a trial</li>
<li>What does the first row represent?</li>
</ul>

<img data-src="images/session_3/OS/datasource.jpg" class="r-stretch"></section>
<section id="building-a-reading-experiment-4" class="slide level2">
<h2>Building a reading experiment</h2>
<p>The trial sequence</p>

<img data-src="images/session_3/OS/trialsequence.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-5" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Send backdrop to .edf (before trial sequence)</p>

<img data-src="images/session_3/OS/examplebackdrop.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-6" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Send backdrop to .edf (before trial sequence)</p>

<img data-src="images/session_3/OS/psychoend.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-7" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Send backdrop to .edf (before trial sequence)</p>

<img data-src="images/session_3/OS/importmodules.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-8" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Drift correction</p>

<img data-src="images/session_3/OS/drift.JPG" class="r-stretch"></section>
<section id="building-a-reading-experiment-9" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Present stimuli and sending areas of interest done via inline python script</p>

<img data-src="images/session_3/OS/prepare.JPG" class="r-stretch quarto-figure-center"><p class="caption">The prepare tab</p></section>
<section id="building-a-reading-experiment-10" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Present stimuli: text drawn on an area of interest basis</p>
<ul>
<li>Get where one area ends so the next one starts there
<ul>
<li>“Draw” rectangles</li>
</ul></li>
</ul>

<img data-src="images/session_3/OS/presentstimuli1.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-11" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Calculate the edges of the areas of interest</p>

<img data-src="images/session_3/OS/sendIAs.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-12" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Present the text</p>

<img data-src="images/session_3/OS/showtext.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-13" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Record the areas of interest</p>

<img data-src="images/session_3/OS/recordIAs.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-14" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Take a screenshot of the screen + send it to the tracker</p>

<img data-src="images/session_3/OS/sendIAs.JPG" class="r-stretch quarto-figure-center"><p class="caption">The run tab</p></section>
<section id="building-a-reading-experiment-15" class="slide level2">
<h2>Building a reading experiment</h2>
<p>Your turn now!</p>
<ul>
<li>Try:
<ul>
<li>This example.</li>
<li>A mock experiment</li>
<li>Your own research question</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<p>Blythe, H. I., Häikiö, T., Bertam, R., Liversedge, S. P., &amp; Hyönä, J. (2011). Reading disappearing text: Why do children refixate words? <em>Vision Research, 51</em>(1), 84–92. https://doi.org/10.1016/j.visres.2010.10.003</p>
<p>Boland, J. E. (2004). Linking eye movements to sentence comprehension in reading and listening. <em>The on-line study of sentence comprehension: Eyetracking, ERP, and beyond</em>, 51-76.</p>
<p>Brysbaert, M. (2019). How many words do we read per minute? A review and meta-analysis of reading rate. <em>Journal of Memory and Language, 109</em>, 104047. https://doi.org/10.1016/j.jml.2019.104047</p>
<p>Carrol, G., Conklin, K., &amp; Gyllstad, H. (2016). FOUND IN TRANSLATION: The Influence of the L1 on the Reading of Idioms in a L2. <em>Studies in Second Language Acquisition, 38</em>(3), 403–443. https://doi.org/10.1017/S0272263115000492</p>
<p>Carroll, T. (2017). Eye Behavior While Reading Words of Sanskrit and Urdu Origin in Hindi. Brigham Young University.</p>
<p>Clifton, C., Staub, A., &amp; Rayner, K. (2007). Eye movements in reading words and sentences. <em>Eye movements</em>, 341-371.</p>
</section>
<section id="references-1" class="slide level2 smaller">
<h2>References</h2>
<p>Clifton, C., Ferreira, F., Henderson, J. M., Inhoff, A. W., Liversedge, S. P., Reichle, E. D., &amp; Schotter, E. R. (2016). Eye movements in reading and information processing: Keith Rayner’s 40year legacy. <em>Journal of Memory and Language, 86</em>, 1–19. https://doi.org/10.1016/j.jml.2015.07.004</p>
<p>Conklin, K., Pellicer-Sánchez, A., &amp; Carrol, G. (2018). Eye-Tracking: A Guide for Applied Linguistics Research. <em>Cambridge University Press</em>. https://doi.org/10.1017/9781108233279</p>
<p>Cop, U., Dirix, N., Drieghe, D., &amp; Duyck, W. (2017). Presenting GECO: An eyetracking corpus of monolingual and bilingual sentence reading. <em>Behavior Research Methods, 49</em>(2), 602–615. https://doi.org/10.3758/s13428-016-0734-0</p>
<p>Dirix, N., Vander Beken, H., De Bruyne, E., Brysbaert, M., &amp; Duyck, W. (2019). Reading Text When Studying in a Second Language: An Eye-Tracking Study. <em>Reading Research Quarterly, 55</em>(3), 371–397. https://doi.org/10.1002/rrq.277</p>
<p>Drieghe, D. (2011). Parafoveal-on-foveal effects on eye movements during reading. In S. P. Liversedge, I. Gilchrist, &amp; S. Everling (Eds.), <em>The Oxford handbook of eye movements</em> (pp.&nbsp;840–855). Oxford: Oxford University Press.</p>
</section>
<section id="references-2" class="slide level2 smaller">
<h2>References</h2>
<p>Frazier, L., &amp; Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. <em>Cognitive Psychology, 14</em>(2), 178–210. https://doi.org/10.1016/0010-0285(82)90008-1</p>
<p>Frisson, S., Harvey, D. R., &amp; Staub, A. (2017). No prediction error cost in reading: Evidence from eye movements. <em>Journal of Memory and Language, 95</em>, 200–214. https://doi.org/10.1016/j.jml.2017.04.007</p>
<p>Hyönä, J. (2011). Foveal and parafoveal processing during reading. In S. P. Liversedge, I. Gilchrist, &amp; S. Everling (Eds.), <em>The Oxford handbook of eye movements</em> (pp.&nbsp;820–838). Oxford: Oxford University Press.</p>
<p>Hyönä, J., Olson, R., Defries, J., Fulker, D., Pennington, B., &amp; Smith, S. (1995). Eye Fixation Patterns Among Dyslexic and Normal Readers: Effects of Word Length and Word Frequency. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 21</em>, 1430–1440. https://doi.org/10.1037/0278-7393.21.6.1430</p>
<p>Howard, P. L., Liversedge, S. P., &amp; Benson, V. (2017). Processing of co-reference in autism spectrum disorder. <em>Autism Research, 10</em>(12), 1968–1980. https://doi.org/10.1002/aur.1845</p>
<p>Juhasz, B. J., Pollatsek, A., Hyönä, J., Drieghe, D., &amp; Rayner, K. (2009). Parafoveal processing within and between words. <em>Quarterly Journal of Experimental Psychology, 62</em>(7), 1356–1376. https://doi.org/10.1080/17470210802400010</p>
</section>
<section id="references-3" class="slide level2 smaller">
<h2>References</h2>
<p>Just, M. A., &amp; Carpenter, P. A. (1980). A theory of reading: from eye fixations to comprehension. <em>Psychological review, 87</em>(4), 329.</p>
<p>Keating, G. D. (2009). Sensitivity to Violations of Gender Agreement in Native and Nonnative Spanish: An Eye-Movement Investigation. <em>Language Learning, 59</em>(3), 503–535. https://doi.org/10.1111/j.1467-9922.2009.00516.x</p>
<p>Liversedge, S. P., Rayner, K., White, S. J., Vergilino-Perez, D., Findlay, J. M., &amp; Kentridge, R. W. (2004). Eye movements when reading disappearing text: is there a gap effect in reading?. <em>Vision research, 44</em>(10), 1013-1024.</p>
<p>McConkie, G. W., &amp; Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. <em>Perception &amp; Psychophysics, 17</em>, 578–586.</p>
<p>Mitchell, D. C., Shen, X., Green, M. J., &amp; Hodgson, T. L. (2008). Accounting for regressive eye-movements in models of sentence processing: A reappraisal of the Selective Reanalysis hypothesis. <em>Journal of Memory and Language, 59</em>(3), 266-293.</p>
<p>Morrison, R. E. (1984). Manipulation of stimulus onset delay in reading: Evidence for parallel programming of saccades. Journal of Experimental Psychology: Human Perception and Performance, 10(5), 667–682. https://doi.org/10.1037/0096-1523.10.5.667</p>
<p>Pellicer-Sánchez, A. (2016). INCIDENTAL L2 VOCABULARY ACQUISITION FROM AND WHILE READING: An Eye-Tracking Study. <em>Studies in Second Language Acquisition, 38</em>(1), 97–130. https://doi.org/10.1017/S0272263115000224</p>
</section>
<section id="references-4" class="slide level2 smaller">
<h2>References</h2>
<p>Pickering, M. J., Frisson, S., McElree, B., &amp; Traxler, M. J. (2004). Eye Movements and Semantic Composition. In M. Carreiras &amp; C. Clifton Jr.&nbsp;(Eds.), <em>The On-line Study of Sentence Comprehension</em>. Psychology Press.</p>
<p>Rayner, K. (1975). The perceptual span and peripheral cues in reading. <em>Cognitive Psychology, 7</em>, 65–81.</p>
<p>Rayner, K., &amp; Bertera, J. H. (1979). Reading without a fovea. <em>Science, 206</em>, 468–469.</p>
<p>Rayner, K., &amp; Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. <em>Memory &amp; Cognition, 14</em>(3), 191–201. https://doi.org/10.3758/BF03197692</p>
<p>Sereno, S. C., &amp; Rayner, K. (1992). Fast priming during eye fixations in reading. <em>Journal of Experimental Psychology: Human Perception and Performance, 18</em>(1), 173.</p>
<p>Sereno, S. C., &amp; Rayner, K. (2003). Measuring word recognition in reading: Eye movements and event-related potentials. <em>Trends in Cognitive Sciences, 7</em>(11), 489–493. https://doi.org/10.1016/j.tics.2003.09.010</p>
<p>Solan, H. A., Feldman, J., &amp; Tujak, L. (1995). Developing Visual and Reading Efficiency in Older Adults. <em>Optometry and Vision Science, 72</em>(2), 139.</p>
</section>
<section id="references-5" class="slide level2 smaller">
<h2>References</h2>
<p>Staub, A., Rayner, K., Pollatsek, A., Hyönä, J., &amp; Majewski, H. (2007). The time course of plausibility effects on eye movements in reading: Evidence from noun-noun compounds. <em>Journal of Experimental Psychology. Learning, Memory, and Cognition, 33</em>(6), 1162–1169. https://doi.org/10.1037/0278-7393.33.6.1162</p>
<div class="quarto-auto-generated-content">
<p><img src="images/logo_mils.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Reading</p>
</div>
</div>
</section></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session_3_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session_3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session_3_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session_3_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    <script>videojs(video_shortcode_videojs_video2);</script>
    <script>videojs(video_shortcode_videojs_video3);</script>
    <script>videojs(video_shortcode_videojs_video4);</script>
    <script>videojs(video_shortcode_videojs_video5);</script>
    

</body></html>