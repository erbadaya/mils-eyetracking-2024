---
title: "Visual World Paradigm"
author: "Badaya"
format: 
   clean-revealjs:
    chalkboard: true
    logo: images/logo_mils.png
    footer: "Visual World Paradigm"
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          top: 40%;
          left: 40%;
          -ms-transform: translateY(-50%), translateX(-50%);
          transform: translateY(-50%), translateX(-50%);
        }
        </style>
editor: visual
---

How is everyone doing today? Any questions from yesterday?

## Plan of today

Visual World Paradigm

1.  History
2.  Visual World Paradigm
3.  Confounds of the Visual World Paradigm
4.  Uses
5.  Pros & cons

# The Visual World Paradigm

## The Visual World Paradigm

The Visual World Paradigm (VWP) is an eye-tracking paradigm that commonly describes an experiment where [auditory and visual stimuli]{.fg style="--col: #e64173"} are presented to a participant, with the goal of understanding how the latter influences the former around a scene.

## History

"While on a safari in [Africa]{.fg style="--col: #e64173"} \[...\] I noticed a hungry [lion]{.fg style="--col: #e64173"} slowly moving through the tall grass toward a herd of grazing [zebra]{.fg style="--col: #e64173"}".

-   Cooper's (1974) method became later on popularised by Tanenhaus et al. (1995).

![Cooper, 1974](images/session_2/cooper_vwp.jpg)

## Uses

Different levels of language comprehension (Huettig et al., 2011)

-   Phonological level (e.g., Allopenna et al., 1998)
    -   Beetle versus beaker
-   Lexical level (e.g., semantic prediction, Altmann & Kamide, 1999)
    -   Eat versus move
-   Syntactic level (e.g., Knoeferle et al., 2005)
    -   Disambiguation of thematic roles.
-   Discourse level (e.g., van Bergen & Bosker, 2018)
    -   Actually versus Indeed
-   Pragmatic level (e.g., Grodner et al., 2010)
    -   Adjective informativeness depends on speaker's reliability.

## Uses

-   Dialogue (e.g., Brown-Schmidt & Tanenhaus, 2008)
    -   Common ground establishment.
-   Paralinguistic cues (e.g., Arnold et al., 2004)
    -   New versus given information following a disfluency.
-   Linguistic relativity (e.g., Papafragou et al., 2008).
    -   Fixation preference following encoding of motion.

And many more (e.g., bilingualism, semantics/syntax interface,...)

## Uses

Different populations

-   Children
-   Aphasic patients (e.g., Mirman et al., 2011)
-   Non-native listeners (e.g., Ito et al., 2018)

## What are we interested in?

(Mostly) Fixations and saccades

-   When & where
-   Operationalisation
    -   Fixation counts, proportion on ROIs,... (day 5)
-   100-200 ms to launch a saccade (Matin et al., 1993)

::: notes
The former much more common than the latter. Because we launch a saccade when we want to fixate on something, we take saccade latency as a measure too. When you consider fixations, you need to acount for the time it takes people to launch a saccade.
:::


## What are we interested in?

(Newer) Pupil size

-   Ease of processing (thus particularly relevant for auditory stimuli)
-   Challenges for experimental design
    - See Winn et al. (2018)

## Linking hypothesis

Linking hypothesis: Link response to a hypothesized process.

-   Eye movements reflect lexical access.
    -   (In reading) Time spent looking at a word == how long it takes to process it.

Visual World Parading = [Linguistic + non-linguistic information.]{.fg style="--col: #e64173"}

-   What guides what? Whether and how do they interact?
-   **How can we be sure that our results eye-movements were linguistically mediated?**

## Linking hypothesis

"Default": Increases in fixation == increases in activation.

-   Automatised routines; recognising a name triggers these routines, in turn, these routines trigger a saccade and thus fixations on objects (Tanenhaus et al., 2000).
    - Easier to follow verbal instructions if you look at the objects you have to interact with. 

## Linking hypothesis

Not necessarily (see Magnuson, 2019):

-   Language processing guiding vision (e.g., Allopenna et al., 1998).
-   Vision also affecting language processing (e.g. Huettig & McQueen, 2007).
-   Listeners getting ahead of speech (e.g., Altmann & Kamide, 1999).

::: notes
A much better discussion of this can be found in Magnuson (2019). I am only mentioning this because these linking hypothesis highlight different aspects we need to consider about the VWP and how we interpret fixations and saccades (e.g., whether they are language-mediated). Magnuson discusses models for those, as well as caveats for each.

In fact, most papers will not discuss what hypothesis they are relying on (and usually, the assumption is the default one, by Tanenhaus et al., 2010)

The first one has its origins in experiments showing that fixations are usualyl mediated by language-dependent factors. For example, in Allopenna et al., they found that there were effects driven by neighbourhood size: frequency of words, rhymes, etc.

The second one suggests that visual information can partially mediate fixations. This is idea is much more developed in Huettig et al. (2011). The idea is that representations are activated prior to speech, you can think of it as a way of 'priming'. They also discuss that by necessity, vision and language need to be connected as we need to move our eyes towards something, so we need to link the linguistic information to spatial information (e.g., where is said object). Visual display activates visual features which in turn activate phonological and semantic features associated to those. Similarly, speech activates these. The overlap leads to even more activation. Note that this kind of entails that you are constantly activating labels when you are looking at the world.

The mental world goes back a bit to the first one, but instead of purely describing phonological input, it also considers the possibility that individuals are building a mental world (kind of like a mental representation of what is being said). They then explore scenes with this mental representation. This was developed by Altmann and Kamide, so it kind of makes sense they advocate for something like this: They need to accomodate for prediction.

The final one has components of the other three. Visual and linguistic information interact, what leads what is following Huettig and McQueen e.g., timing and task. One components is deep interaction: when one type of infor alters the other, for example, no showing examples of garden-path when nothing in the display does not support such interpretation (visual information gets ahead of whatever linguistic information may come up, and alters it, as opposed to biasing it).
:::

## Linking hypothesis

But why do we care?

-   Role of instructions and presentation
    -   Click on the red \[target\] (see Spivey et al., 2004)
-   Role of preview window
    -   Activation of phonological and semantic information
-   Bottom-up versus top-down effects
    -   Passive versus active processes (e.g., Pickering & Gambi's (2018) prediction-by-production)
-   Level of interest
    -   Lexical access v Sentence comprehension versus Discourse comprehension

::: notes

Indeed, Spivey et al. (2004) added a linguistic twist to a classic visual search paradigm, where time to respond varied with set size and target presence (there either was or was not a target item with a unique conjunction of color and orientation in the display). A spoken instruction (e.g., Is there a green vertical?) either preceded the visual display (auditory first) or was offset such that the first dimension (e.g., green) was aligned just after visual display onset (A/V concurrent). Participants were much less affected by set size (number of distractors) in the A/V concurrent condition. Spivey et al. proposed that in the concurrent condition, the timing of the spoken input allowed participants to serially filter the display, first identifying items matching in color and then in orientation; essentially, it allowed participants to conduct two single-feature searches. In the auditory first condition, having all of the details (both features) in advance relatively impaired performance because it promoted a conjunction (dual-feature) search (see also Chiu and Spivey 2014; Reali et al. 2006). Thus, the nature of the parallel contingency is flexible and can flux with simple parameters like relative timing.

Bottom-up and top-down has to do with mental world
:::

## Basics of the Visual World Paradigm

1.  Elements
2.  Structure
3.  Confounds
4.  Variations

## Visual World Paradigm

Working example: Altmann and Kamide (1999)

::: columns
::: {.column width="50%"}
Anticipation (\~prediction) of lexical items given verb semantics.

-   DV: Fixations to objects on screen
-   IV: Verb semantics: [constraining]{.fg style="--col: #FF7F50"} versus [unconstraining]{.fg style="--col: #40E0D0"}
    -   "The boy will [eat]{.fg style="--col: #FF7F50"} the cake" versus "The boy will [move]{.fg style="--col: #40E0D0"} the cake"
:::

::: {.column width="50%"}
![](images/session_2/altmannkamide1999.png)
:::
:::

## Elements

What are the elements in Altmann and Kamide that you can identify as characteristic of the Visual World Paradigm?

. . .

-   Auditory stimuli
-   Visual stimuli
-   Task

## Auditory stimuli

Example of Altmann and Kamide?

. . .

-   The boy will [eat]{.fg style="--col: #FF7F50"} the cake (constraining).
-   The boy will [move]{.fg style="--col: #40E0D0"} the cake (unconstraining).

Control of stimuli:

-   Context matched.
-   Counterbalancing.
-   Comparison between levels.
-   Inter alia.

## Auditory stimuli

Participants *listen* to these sentences.

-   Need to *record* our stimuli.

Tips:

-   All recordings in one session.
-   Talk to the person recording (to avoid monotonous voice).
-   Sound-isolating recording studio.
-   Several recordings of the same sentence.
-   Consider cross-splicing.
    -   Editing tools: Audacity, Praat.
-   Control speaker's traits.

## Auditory stimuli

Eye-movements in the VWP are [time-locked]{.fg style="--col: #e64173"}.

-   Time window of analysis around a critical part of speech e.g., critical word.
    -   Triggers (coding)
-   Different time windows to explore [different processes]{.fg style="--col: #e64173"} within speech comprehension.
    -   Integration versus Prediction (analysis)

## Auditory stimuli

Can you think of other elements of audio that can serve as a time anchor?

. . .

-   Prosodic contour.
-   Case marking.
-   Speech errors.

## Auditory stimuli

Can you think of other properties of audio that can be manipulated?

. . .

-   Speech rate.
-   Traits of the speaker.
-   Prosody.
-   Noise.
-   Inter alia.

## Visual stimuli

::: columns
::: {.column width="50%"}
Elements in Altmann and Kamide?
:::

::: {.column width="50%"}
![](images/session_2/altmannkamide1999.png)
:::
:::

## Visual stimuli

::: columns
::: {.column width="50%"}
Elements in Altmann and Kamide?

4: Target (cake) and *distractors* (ball, train, car).

-   And the boy.
:::

::: {.column width="50%"}
![](images/session_2/altmannkamide1999.png)
:::
:::

## Visual stimuli

Eye-movements in the VWP are space-locked: Areas of Interest

1.  How many items can there be on display?
2.  How can items be displayed?
3.  How can we manipulate the items?
4.  How can we select images?

## Visual stimuli

1.  How many items can there be on display?

-   [\[2, 5\]]{.fg style="--col: #e64173"}
-   Working memory (see Huettig et al., 2011)

## Visual stimuli

2.  How can items be displayed?

Properties of the display allow for exploration of different processes in speech comprehension.

-   Semirealistic scenes: [World knowledge]{.fg style="--col: #e64173"}.
-   Arrays: [Conceptual and lexical knowledge]{.fg style="--col: #e64173"} associated with individual words.
-   Printed words: [Phonological information and orthographic]{.fg style="--col: #e64173"} processing, comprehension of abstract words.

::: notes
Mention the plausibility bit!!
:::

## Visual stimuli

2.  How can items be displayed?

::: {columns}
::: {.column width="30%"}
![Altmann & Kamide, 1999](images/session_2/altmannkamide1999.png)
:::

::: {.column width="30%"}
![Huettig & McQueen, 2007](images/session_2/array_example.jpg)
:::

::: {.column width="30%"}
![Huettig & McQueen, 2007](images/session_2/ortho_example.jpg)
:::
:::

## Visual stimuli

2.  How can items be displayed?

In some cases:

-   Items - blank screen
    -   Short term visual memory

## Visual stimuli

3.  How can we manipulate the items?

[Relationship between target and distractor(s)]{.fg style="--col: #e64173"}.

-   Semantic distance, phonological distance, etc. - even shape!
-   Target (critical word) might not even be present.

## Visual stimuli

3.  How can we manipulate the items?

![Allopenna et al., 1996](images/session_2/phonological_manipulation.JPG)

## Visual stimuli

4.  How can we select images?

Databases *or* create your own.

-   Later case: Need for validation cf. confounds.
    -   Name agreement.

Tips:

-   Control for visual salience.
-   Control for size (coding).
-   Familiarisation phase.
-   If also pupillometry -\> stimuli luminance.

## Task

As a function of your research question.

-   Direct action
-   Look and listen

But also;

-   Perform a concurrent task? (impair WM)
-   Interpretation of speech?

Remember: the active viewer (Yarbus).

## Structure

**Before the experiment begins**

-   Calibration and validation.
    -   Number of elements for interest areas.
    -   Size of interest areas.
    -   Horizontal and vertical areas.
-   Decide sample rate.

## Structure

![](images/session_2/vwp_trialsequence_example.jpg)

## Drift correction

-   Ensure accuracy.
-   Where?
    -   [Middle]{.fg style="--col: #e64173"} (no bias for an image beforehand).
-   When?
    -   Beginning of every trial/block.

## Preview window

-   Very specific to the VWP
-   Presentation of visual stimuli without auditory stimuli, so that participants can inspect the visual scene.
    -   Remember the linking hypothesis!

[Pros and cons]{.fg style="--col: #e64173"} of preview window (e.g., Huettig & McQueen, 2007).

-   Eye movements towards objects as a function of preview window.
-   Pre-activation of labels.
-   Contestable?

## Preview window

Length?

-   [Previous research]{.fg style="--col: #e64173"} e.g., 2000 ms, 1000 ms from target onset, etc.
    -   Level of interest e.g., phonological activation versus semantic activation.

## Audio presentation

-   Send triggers for audio.
    -   Give enough time for a measure to occur.
    -   Altmann and Kamide: Trigger -\> verb onset.

## End of trial

-   Task?
-   If no task:
    -   Give enough time for processes to fully unfold.

## Exercise: RQ & measures of interest

-   Go [here](https://docs.google.com/document/d/1TdjmVutw2PpQvvS09bjW5HF8EeGFbMUE5y4Dof7BzjU/edit?usp=sharing) and type your idea!

What would you be interested in? How would the design look like?

## Analysis

DV = saccade latency, proportion of fixations, empirical logits, yes/no fixations...

IV = your experimental variables **and time**.

-   Time window of analysis.
    -   Marked by your triggers (e.g., sentence onset, target onset, target offset...)
    -   Research question.

## Analysis

Interest: Prediction of a noun following verb semantics, what is more interesting for you? TW from sentence onset, from verb onset, or from noun onset? What reflects prediction?

. . .

Verb onset - noun onset (+200 for saccades)

-   Reference resolution, word recognition, ...

## Analysis

Collapse all samples before and after.

-   Are there more fixations on the cake when is preceded by [eat]{.fg style="--col: #FF7F50"} compared to [move]{.fg style="--col: #40E0D0"}?

## Analysis

Time binning.

-   Related to sampling frequency.
-   Group of samples.

With 500 Hz, you have 500 samples per second, one every 2 ms.

-   You can make bins of 10 ms (contains 5 samples), 20 ms (contains 10 samples)...
-   You cannot make bins of 15 ms.

::: notes
You hardly explore second by second, but instead, aggregate samples over X amount of milliseconds. We call this time binning analysis. This is the first kind of data pre-processing, of course you can later see how people then analysis into groups of a 100 (e.g., any significance in the first 100, then the next, and so on).
:::

## Analysis

More on Friday

Divide in approaches (see Ito & Knoeferle, 2023):

::: columns
::: {.column width="50%"}
::: r-stack
**Linear**
:::

-   (G)LMM/ANOVAs by subjects & items/t-tests

There is an increase or a decrease over time. Cannot tell *when* this increase/decrease happens, but (G)LMMs are the most common analysis.

-   but cf. models per time interval.
:::

::: {.column width="50%"}
::: r-stack
**Non-linear**
:::

-   Growth Curve Analysis, cluster analysis, Generalised Additive Mixed Models, BOLTS...

How this increase/decrease over time occurs. Some can assess when the differences in conditions become statistically significant.
:::
:::

## Results

Commonly, plots.

-   x: time
-   y: proportion/probability of fixations.
-   line style: condition

::: r-stack
![](images/session_2/altmannkamide_results.png)
:::

## Results

![](images/session_1/vwp_data.JPG)

## Exercise: Huettig & McQueen (2007)

1.  What is the visual display? Why?
2.  How many samples in 20 ms bins?
3.  What is the time window of analysis? Why?
4.  What is the difference between Exp 1 and Exp 2?

## Exercise: Huettig & McQueen (2007)

1.  What is the visual display? Why?

. . .

4 images representing 3 related and 1 unrelated items to the target.

-   Phonology, shape, semantics.
-   No target displayed to maximise fixations to distractors.
-   Distractors chosen to explore whether fixations are driven by phonology, visual or semantic information.

## Exercise: Huettig & McQueen (2007)

2.  How many samples in 20 ms bins?

. . .

Recording at 250 Hz, 250 samples per second.

-   5 samples.

## Exercise: Huettig & McQueen (2007)

3.  What is the time window of analysis? Why?

. . .

From noun onset +200 (to account for a saccade) - 1000 ms.

## Exercise: Huettig & McQueen (2007)

4.  What is the difference between Exp 1 and Exp 2?

. . .

Preview window.

## Confounds

<br> <br> <br>

::: r-stack
Given your knowledge in linguistics & what we've discussed, what should we keep in mind when creating a VWP experiment?
:::

## Confounds

Image presentation.

-   Salience

::: r-stack
![](images/session_2/train.png) ![](images/session_2/boat_pic.jpg)
:::

## Confounds

<br> <br>

Image presentation

::: columns
::: {.column width="50%"}
-   Name agreement
    -   Population
    -   Clarity
:::

::: {.column width="50%"}
::: r-stack
![](images/session_2/turtle.png)
:::
:::
:::

## Confounds

Image presentation

-   Size & quality
-   Counterbalance position
-   Luminance (! pupillometry)

## Confounds

Image relationship

-   Semantic distance
-   Phonological overlap
-   Lexical properties e.g., frequency.

## Confounds

Audio properties

-   Same/different voices
    -   Uncanny valley
-   Phonetic cues (e.g., co-articulation)
    -   Cross-splicing audios
-   Volume
-   Prosody
-   Accent

## Confounds

Audio properties

-   Speech rate

![](images/session_2/speechrate_vwp.png)

## Pros & cons

::: columns
::: {.column width="50%"}
::: r-stack
**Pros**
:::

-   Ecological validity.
-   Relatively easy.
-   Accessible e.g., no meta-linguistic judgements.
:::

::: {.column width="50%"}
::: r-stack
**Cons**
:::

-   Ecological validity.
-   Confounding variables.
-   Linking hypothesis.
:::
:::

## Variations

Preferential look paradigm/Head-turn preference (\~ VWP for infants)

::: columns
::: {.column width="50%"}
-   Different tracker
-   Stimuli presentation
    -   What is discourse-old versus difficult.
-   Messier data, fewer trials

Example of a lab protocol: Stone & Bosworth (2017)

:::

::: {.column width="50%"}
![](images/session_2/preferential_kids.png)
:::
:::

## Variations

**Do children's  and adults' eye movements differ?**

Scene viewing (Helo et al., 2014): 

  - Shorter fixations, bigger saccades as kids age (2, 4-6, 6-8, 8-10)
  - Main effect of time (changes when viewing a scene)
  - Different attentional modes?

Biases for information?

- Social information, semantic information (see Linka et al., 2023)

## Variations

Language production.

-   No auditory stimuli
    -   Participants are asked to describe what they see
    -   Instructions

## Variations

-   The link between planning and eye movements is less direct.
-   Eye movements \~ labour division of speech production (e.g., Levelt's model)
    -   Conceptual
    -   Formulation
    -   Articulation
-   How does looking at an object relate to production stages?
    -   Do people fixate for longer on objects harder to retrieve?
    -   Do people start producing speech prior to fixating on the object to name?

## Variations

Language production.

- A direct fixation is not necessary to identify an object or when producing a label.
  - Speakers tend to look at the next object they will name (about 1 s before articulation).


## Variations

Language production.

::: columns
::: {.column width="50%"}
-   Only images.
    -   [Time-locked to voice onset/offset]{.fg style="--col: #e64173"}.
    -   Manipulations: Image degradation, word frequency, etc.
        -   Relation to the level of interest.
-   Network task
:::

::: {.column width="50%"}
<br>

![Pistono & Hartsuiker, 2023](images/session_2/example_pistono.JPG)
:::
:::

## Variations

Mouse-tracking e.g., King et al., 2019

- Same logic as eye-tracking: track x,y coordinates every X ms (bounded by refresh rate)
- Motor movements taken to reflect mental trajectories (Spivey et al., 2005).

## Variations

- Can be run online (less costly, e.g., large-scale experiments; wider audience e.g., clinical groups).
- Validity has been proven several times (see Loy (2017), chapter 7).
- Computer requirements.
- Participant instructions (e.g., emphasize they need to move the mouse, but not too much!).
- Eye movements are temporally sensitive, mouse movements are spatially sensitive.
- Motor movements are delayed in comparison to eye movements (Farmer et al., 2007).

## Variations

-   Web-cam tracking e.g., Slim & Hartsuiker, 2023
    -   ROIs size
    -   Display size

------------------------------------------------------------------------

Let's take a break!

# Building an eye-tracking experiment

## Building an eye-tracking experiment

General commercial software:

-   E-Prime
-   Presentation
-   PsyScope

Advantages: User-friendly.

Disadvantages: License.

## Building an eye-tracking experiment

Commercial software associated with eye-tracking hardware

-   SMI: Experiment Center
-   Tobii: Tobii Studio
-   EyeLink: Experiment Builder

Advantages: User-friendly, created with eye-tracking experiments in mind.

Disadvantages: License (but if you own an eye-tracker, you commonly have one).

## Building an eye-tracking experiment

Programming environments

-   MATLAB (Psychophysics Toolbox)
-   Python (e.g., PsychoPy, OpenSesame)

Advantages: Free software, nicely documented, versatile, user-friendly interfaces.

Disadvantages: More complex functionalities require coding ("more intimidating").

## Building an eye-tracking experiment

Focus of this course: license-free software

-   But: Packages designed for EyeLink+
    -   Shouldn't disencourage you

In this course: OpenSesame

-   Best trade-off
    -   Adaptation to other trackers
    -   Coding/plugins **But**:
    -   Less ideal for reading
        -   PsychoPy (check MultiplEYE project too!)

## OpenSesame

We need to install *pylink* and *pygaze*.

-   Open OpenSesame & type in the terminal

``` r

pip install pylink
pip install python-pygaze # for those using Windows, this is not necessary
```

## OpenSesame

![](images/session_2/OS_layout.PNG)

## OpenSesame

::: columns
::: {.column width="50%"}
Combination of drag-in elements plus the possibility of coding

-   Interaction with the eye-tracker via PyGaze
:::

::: {.column width="50%"}
![](images/session_2/OS_plugins.PNG) ![](images/session_2/pygaze_plugins.PNG)
:::
:::

## OpenSesame

![](images/session_2/OS_beginning.PNG)

## OpenSesame

::: columns
::: {.column width="50%"}
Loop:

-   Data source
    -   Trial information e.g., items, conditions
    -   Order of presentation of items
:::

::: {.columns width="50%"}
![](images/session_2/OS_loop.PNG)
:::
:::

## OpenSesame

Sequence

-   Trial structure
-   Where the other plugins go inside

![](images/session_2/OS_trialsequence.PNG)

## OpenSesame

Presentation plugins

::: columns
::: {.column width="30%"}
![](images/session_2/canvas.PNG)
:::

::: {.column width="30%"}
![](images/session_2/audio.PNG)
:::

::: {.column width="30%"}
![](images/session_2/response.PNG)
:::
:::

## OpenSesame

Eye-tracking components

What is the structure of an experiment?

. . .

-   Calibration & validation
-   Drift correction
-   Recording
-   Send information

## OpenSesame

Calibration and validation

::: columns
::: {.column width="50%"}
-   PyGaze Start recording
-   (Often times) At the beginning of the experiment
    -   Not within a loop!
:::

::: {.column width="50%"}
![](images/session_2/pygaze_cal.PNG)
:::
:::

## OpenSesame

Drift correction

::: columns
::: {.column width="50%"}
-   PyGaze Drift correction
-   (Often times) Inside your trial sequence
    -   Never after start recording
:::

::: {.column width="50%"}
![](images/session_2/pygaze_drift.PNG)
:::
:::

## OpenSesame

Start recording

::: columns
::: {.column width="50%"}
-   PyGaze Start recording
-   Inside your trial sequence
:::

::: {.column width="50%"}
![](images/session_2/pygaze_record.PNG)
:::
:::

## OpenSesame

Log variables

::: columns
::: {.column width="50%"}
-   PyGaze Log
-   Inside your trial sequence
    -   Never after stop recording
:::

::: {.column width="50%"}
![](images/session_2/pygaze_log.PNG)
:::
:::

## OpenSesame

Stop recording

::: columns
::: {.column width="50%"}
-   PyGaze Stop recording
-   Inside your trial sequence
:::

::: {.column width="50%"}
![](images/session_2/pygaze_stop.PNG)
:::
:::

## OpenSesame

![](images/session_2/os_initiallook.PNG)

# Building a VWP experiment

## Building a VWP experiment

Within the trial sequence, what plugins do you think we should have? (both for stimuli presentation and eye tracking)

. . .

![](images/session_2/original_idea.PNG)

## Building a VWP experiment

![](images/session_2/actual.PNG)

## Building a VWP experiment

Download the OpenSesame script

-   Explore the "data source"
-   How to send triggers
-   How to define Areas of Interest (cf. manual on GitHub)

## Building a VWP experiment

Example: Lexical access (loose adaptation of Allopenna et al., 1996)

-   DV: Fixations on referred item on the screen.
-   IV: Relationship target and other items on the screen (cohort competitor, rhyme competitor, distractor)

4-images visual array

## Building a VWP experiment

::: columns
::: {.column width="50%"}
The stimuli:

-   18 images (some pictures are recycled)
-   32 audios (16 carrier sentences, 16 nouns, to ease sending triggers)

You don't need to send the stimuli outside of the script, everything is in the file!
:::

::: {.column width="50%"}
![](images/session_2/OS/items.JPG)
:::
:::

## Building a VWP experiment

The constraints of design:

-   We want to counterbalance the position of the target.
-   We want to have a preview window and then the audio.
-   We want participants to click on an item only after the noun is said.
-   We want to show participants the instructions at the beginning of the experiment.

## Building a VWP experiment

The constraints of the eye tracker:

-   We want three triggers (audio onset - noun onset - trial offset).
-   We want four areas of interest (target versus distractor).
-   We want a drift correction at the beginning of every trial.

## Building a VWP experiment

The loop

-   Each row: a trial
-   What does the first row represent?

![](images/session_2/OS/datasource.jpg)

## Building a VWP experiment

The trial sequence

![](images/session_2/OS/trialsequence.JPG)

## Building a VWP experiment

Send Areas of Interest

-   inline python script (but NB label is position, not object on position)

![](images/session_2/OS/sendIAs.JPG)

## Building a VWP experiment

Dynamic visual stimuli

-   NB pseudo-randomised

![](images/session_2/OS/dynamicposition.JPG)

## Building a VWP experiment

Auditory stimuli

::: columns
::: {.column width="20%"}
-   Cross splicing
:::

::: {.column width="40%"}
![](images/session_2/OS/carrier.JPG)
:::

::: {.column width="40%"}
![](images/session_2/OS/noun.JPG)
:::
:::

## Building a VWP

Send triggers

-   inline python script (NB order of sending! *before* the event)

![](images/session_2/OS/trigger_code.JPG)

## Building a VWP experiment

Your turn now!

-   Try:
    -   This example.
    -   A mock experiment
    -   Your own research question

# Wrap-up

## Plan for tomorrow

-   Eye-tracking-while-reading with Mariia
    -   Recommended reading: Rayner & Duffy (1986)
-   Coding an experiment in OpenSesame

## Homework

-   Play around with OpenSesame
-   Try to come up with a RQ within your own field where VWP would be useful

## References {.smaller}

Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. *Journal of memory and language, 38*(4), 419-439.

Altmann, G. T., & Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of subsequent reference. *Cognition, 73*(3), 247-264.

Arnold, J. E., Tanenhaus, M. K., Altmann, R. J., & Fagnano, M. (2004). The old and thee, uh, new: Disfluency and reference resolution. *Psychological science, 15*(9), 578-582.

Brown‐Schmidt, S., & Tanenhaus, M. K. (2008). Real‐time investigation of referential domains in unscripted conversation: A targeted language game approach. *Cognitive science, 32*(4), 643-684.

Cooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: a new methodology for the real-time investigation of speech perception, memory, and language processing. Cognitive psychology.

Eichert, N., Peeters, D., & Hagoort, P. (2018). Language-driven anticipatory eye movements in virtual reality. *Behavior research methods, 50*, 1102-1115.

## References {.smaller}

Grodner, D. J., Klein, N. M., Carbary, K. M., & Tanenhaus, M. K. (2010). "Some," and possibly all, scalar inferences are not delayed: Evidence for immediate pragmatic enrichment. *Cognition, 116*(1), 42-55.

Helo, A., Pannasch, S., Sirri, L., & Rämä, P. (2014). The maturation of eye movement behavior: Scene viewing characteristics in children and adults. *Vision research, 103*, 83-91.

Huettig, F., & McQueen, J. M. (2007). The tug of war between phonological, semantic and shape information in language-mediated visual search. *Journal of memory and language, 57*(4), 460-482.

Huettig, F., Olivers, C. N., & Hartsuiker, R. J. (2011). Looking, language, and memory: Bridging research from the visual world and visual search paradigms. *Acta psychologica, 137*(2), 138-150.

Huettig, F., Rommers, J., & Meyer, A. S. (2011). Using the visual world paradigm to study language processing: A review and critical evaluation. *Acta psychologica, 137*(2), 151-171.

Ito, A., & Knoeferle, P. (2023). Analysing data from the psycholinguistic visual-world paradigm: Comparison of different analysis methods. *Behavior Research Methods, 55*(7), 3461-3493.

## References {.smaller}

Ito, A., Pickering, M. J., & Corley, M. (2018). Investigating the time-course of phonological prediction in native and non-native speakers of English: A visual world eye-tracking study. *Journal of Memory and Language, 98*, 1-11.


King, J. P., Loy, J. E., & Corley, M. (2018). Contextual effects on online pragmatic inferences of deception. *Discourse Processes, 55*(2), 123-135.

Knoeferle, P., Crocker, M. W., Scheepers, C., & Pickering, M. J. (2005). The influence of the immediate visual context on incremental thematic role-assignment: Evidence from eye-movements in depicted events. *Cognition, 95*(1), 95-127.

Linka, M., Sensoy, Ö., Karimpur, H., Schwarzer, G., & de Haas, B. (2023). Free viewing biases for complex scenes in preschoolers and adults. *Scientific reports, 13*(1), 11803.

Magnuson, J. S. (2019). Fixations in the visual world paradigm: where, when, why?. *Journal of Cultural Cognitive Science, 3*(2), 113-139.

Matin, E., Shao, K. C., & Boff, K. R. (1993). Saccadic overhead: Information-processing time with and without saccades. *Perception & psychophysics, 53*, 372-380.

Mirman, D., Yee, E., Blumstein, S. E., & Magnuson, J. S. (2011). Theories of spoken word recognition deficits in aphasia: Evidence from eye-tracking and computational modeling. *Brain and language, 117*(2), 53-68.

Papafragou, A., Hulbert, J., & Trueswell, J. (2008). Does language guide event perception? Evidence from eye movements. *Cognition, 108*(1), 155-184.

## References {.smaller}

Pickering, M. J., & Gambi, C. (2018). Predicting while comprehending language: A theory and review. *Psychological bulletin, 144*(10), 1002.

Pistono, A., & Hartsuiker, R. J. (2023). Can object identification difficulty be predicted based on disfluencies and eye-movements in connected speech?. *Plos one, 18*(3), e0281589.

Rayner, K., & Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. *Memory & Cognition, 14*(3), 191--201. https://doi.org/10.3758/BF03197692

Slim, M. S., & Hartsuiker, R. J. (2023). Moving visual world experiments online? A web-based replication of Dijkgraaf, Hartsuiker, and Duyck (2017) using PCIbex and WebGazer. js. *Behavior Research Methods, 55*(7), 3786-3804.

Spivey, M. J., Richardson, D. C., & Fitneva, S. A. (2004). Thinking outside the brain: Spatial indices to visual and linguistic information. In J. M. Henderson & F. Ferreira (Eds.), *The interface of language, vision, and action: Eye movements and the visual world* (pp. 161–189). New York, NY, US: Psychology Press.

Stone, A., & Bosworth, R. G. (2019). Exploring infant sensitivity to visual language using eye tracking and the preferential looking paradigm. *JoVE (Journal of Visualized Experiments)*, (147), e59581.

Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., & Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. *Science, 268*(5217), 1632-1634.

## References {.smaller}

Tanenhaus, M. K., Magnuson, J. S., Dahan, D., & Chambers, C. (2000). Eye movements and lexical access in spoken-language comprehension: Evaluating a linking hypothesis between fixations and linguistic processing. *Journal of Psycholinguistic Research,29*, 557--580.

Winn, M. B., Wendt, D., Koelewijn, T., & Kuchinsky, S. E. (2018). Best practices and advice for using pupillometry to measure listening effort: An introduction for those who want to get started. *Trends in hearing, 22*, 2331216518800869.

Van Bergen, G., & Bosker, H. R. (2018). Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad'indeed'and eigenlijk'actually'. *Journal of Memory and Language, 103*, 191-209.
